{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f826cf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "from torch_geometric.data import Dataset, download_url, Data\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from torch_geometric import nn as gnn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import crop\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from torch.nn import Sequential as Seq\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation')\n",
    "from onegraph_rest_free_graphds import OneGraphDS\n",
    "from gan_based_models import Discriminator\n",
    "from second_stage_vig_based_models import FullModel\n",
    "from vig_based_functions import act_layer, get_multi_shot_set\n",
    "from vig_graph_modules import GrapherSetEdges, FFN\n",
    "from perceptual_loss import VGGPerceptualLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43e70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/paulraae/MS_Thesis_Data/tester_eye_data/test_202_with_tiles_processed.pk1', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbe554d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'image', 'features', 'factor', 'cluster', 'centroids', 'umap_reducer', 'tiles', 'tile_features', 'tile_cluster', 'tile_centroids', 'tile_images', 'tile_factor', 'all_tiles', 'tile_knn_edge_index'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7142536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([775, 56, 56, 3])\n",
      "torch.Size([264, 56, 56, 3])\n",
      "torch.Size([775])\n",
      "torch.Size([264])\n",
      "torch.Size([775, 12])\n",
      "torch.Size([264, 12])\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"whatmatters_split_eye_tile_dataset.pickle\"\n",
    "with open(SAVE_PATH, 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "train_tiles = data[\"train_tiles\"]\n",
    "test_tiles = data[\"test_tiles\"]\n",
    "train_cluster = data[\"train_cluster\"]\n",
    "test_cluster = data[\"test_cluster\"]\n",
    "cluster_dists = data[\"train_cluster_dists\"]\n",
    "te_cluster_dists = data[\"test_cluster_dists\"]\n",
    "\n",
    "print(train_tiles.shape)\n",
    "print(test_tiles.shape)\n",
    "print(train_cluster.shape)\n",
    "print(test_cluster.shape)\n",
    "print(cluster_dists.shape)\n",
    "print(te_cluster_dists.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f208ccf",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dd4c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([202, 224, 224, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "#B,H,W,C = train_tiles.shape\n",
    "#B2,H,W,C = test_tiles.shape\n",
    "train_tiles = torch.Tensor(data[\"image\"])\n",
    "#x = train_tiles.reshape(B,-1)\n",
    "train_tiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544cd2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KMeans</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.cluster.KMeans.html\">?<span>Documentation for KMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_clusters',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_clusters&nbsp;</td>\n",
       "            <td class=\"value\">5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('init',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">init&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;k-means++&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_init',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_init&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">300</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy_x',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy_x&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('algorithm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">algorithm&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lloyd&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "KMeans(n_clusters=5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "x = train_tiles.reshape(train_tiles.shape[0], -1)\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73da190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tiles_clusters = torch.Tensor(kmeans.labels_).to(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c96ce6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 42 54 42 26]\n",
      "torch.Size([202, 224, 224, 3])\n",
      "torch.Size([202])\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(train_tiles_clusters, return_counts=True)\n",
    "\n",
    "print(counts)\n",
    "print(train_tiles.shape)\n",
    "print(train_tiles_clusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544cd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.Tensor(kmeans.cluster_centers_).reshape(12,56,56,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "caea83cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 61  38  30  53  40  51  50 174  42  49  83 104]\n",
      "775\n",
      "[21 13 11 18 14 17 17 59 14 17 28 35]\n",
      "264\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "train_counts=np.array(np.floor(counts*0.75), dtype=np.int64)\n",
    "print(train_counts)\n",
    "print(train_counts.sum())\n",
    "print(counts - train_counts)\n",
    "print((counts - train_counts).sum())\n",
    "print((train_counts+(counts-train_counts)) == counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2e4d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "test_ind = []\n",
    "train_ind = []\n",
    "unique, counts = np.unique(train_tiles_clusters, return_counts=True)\n",
    "\n",
    "for cluster, count in zip(np.unique(train_tiles_clusters), train_counts):\n",
    "    main_array = np.where(train_tiles_clusters == cluster)[0]\n",
    "    train_values = rng.choice(main_array, size=count, replace=False)\n",
    "    test_values = np.setdiff1d(main_array, train_values)\n",
    "    \n",
    "    train_ind.extend(train_values)\n",
    "    test_ind.extend(test_values)\n",
    "\n",
    "train_ind = np.array(train_ind)\n",
    "test_ind = np.array(test_ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28f2a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(775,)\n",
      "(264,)\n"
     ]
    }
   ],
   "source": [
    "print(train_ind.shape)\n",
    "print(test_ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8cd78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([775, 56, 56, 3])\n",
      "torch.Size([264, 56, 56, 3])\n",
      "torch.Size([775])\n",
      "torch.Size([264])\n"
     ]
    }
   ],
   "source": [
    "test_tiles = train_tiles[test_ind]\n",
    "train_tiles = train_tiles[train_ind]\n",
    "\n",
    "train_cluster = train_tiles_clusters[train_ind]\n",
    "test_cluster = train_tiles_clusters[test_ind]\n",
    "\n",
    "print(train_tiles.shape)\n",
    "print(test_tiles.shape)\n",
    "print(train_cluster.shape)\n",
    "print(test_cluster.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e2fc549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n+=1\n",
    "#print(n)\n",
    "#print(\"Cluster:\",train_cluster[n].item())\n",
    "cluster_dists = []\n",
    "with torch.no_grad():\n",
    "    for tiles in train_tiles:\n",
    "        aux = []\n",
    "        for centroid in centroids:\n",
    "            aux.append(torch.dist(tiles, centroid, p=2))\n",
    "        aux = torch.Tensor(aux)\n",
    "        cluster_dists.append(aux)\n",
    "    cluster_dists = torch.stack(cluster_dists)\n",
    "\n",
    "cluster_dists = (cluster_dists - torch.min(cluster_dists))/(torch.max(cluster_dists) - torch.min(cluster_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e18e8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n+=1\n",
    "#print(n)\n",
    "#print(\"Cluster:\",train_cluster[n].item())\n",
    "te_cluster_dists = []\n",
    "with torch.no_grad():\n",
    "    for tiles in test_tiles:\n",
    "        aux = []\n",
    "        for centroid in centroids:\n",
    "            aux.append(torch.dist(tiles, centroid, p=2))\n",
    "        aux = torch.Tensor(aux)\n",
    "        te_cluster_dists.append(aux)\n",
    "    te_cluster_dists = torch.stack(te_cluster_dists)\n",
    "\n",
    "te_cluster_dists = (te_cluster_dists - torch.min(te_cluster_dists))/(torch.max(te_cluster_dists) - torch.min(te_cluster_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a94cec4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([775, 12])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "76292617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([775, 56, 56, 3])\n",
      "torch.Size([264, 56, 56, 3])\n",
      "torch.Size([775])\n",
      "torch.Size([264])\n",
      "torch.Size([775, 12])\n",
      "torch.Size([264, 12])\n"
     ]
    }
   ],
   "source": [
    "print(train_tiles.shape)\n",
    "print(test_tiles.shape)\n",
    "print(train_cluster.shape)\n",
    "print(test_cluster.shape)\n",
    "print(cluster_dists.shape)\n",
    "print(te_cluster_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_split_eye_tile_dataset.pickle\"\n",
    "data = {\"train_tiles\":train_tiles, \"test_tiles\":test_tiles, \"train_cluster\":train_cluster, \"test_cluster\":test_cluster, \"train_cluster_dists\":cluster_dists, \"test_cluster_dists\":te_cluster_dists}\n",
    "with open('whatmatters_split_eye_tile_dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18da714",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad446a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscDownBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "      super().__init__()\n",
    "      self.convs = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(True),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      return self.convs(x)\n",
    "    \n",
    "class DiscStrideBlock(nn.Module):\n",
    "  def __init__(self, in_dim):\n",
    "    super().__init__()\n",
    "    self.convs = nn.Sequential(\n",
    "      nn.Conv2d(in_dim, in_dim, 3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(in_dim),\n",
    "      nn.ReLU(True),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.convs(x)\n",
    "\n",
    "class DiscBlock(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim):\n",
    "    super().__init__()\n",
    "    self.down = DiscDownBlock(in_dim, out_dim)\n",
    "    self.stride = DiscStrideBlock(out_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.down(x)\n",
    "    return self.stride(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_cats):\n",
    "      super().__init__()\n",
    "      self.block1 = DiscBlock(in_dim, out_dim//4)\n",
    "      self.block2 = DiscBlock(out_dim//4, out_dim//2)\n",
    "      self.block3 = DiscBlock(out_dim//2, out_dim)\n",
    "\n",
    "      self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(out_dim*7*7, 8192),\n",
    "        nn.BatchNorm1d(8192),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(8192, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(4096, num_cats),\n",
    "        nn.Sigmoid(),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      x1 = self.block1(x)\n",
    "      x2 = self.block2(x1)\n",
    "      x3 = self.block3(x2)\n",
    "      x = self.classifier(x3)\n",
    "      return [x1,x2,x3], x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32b4d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorBlock(nn.Module):\n",
    "  def __init__(self, out_dim, img_size):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(out_dim*(img_size//2)*(img_size//2), 8192),\n",
    "      nn.BatchNorm1d(8192),\n",
    "      nn.ReLU(True),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear(x)\n",
    "    \n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_cats ,num_patches = 3, stride = 14, img_size=56, device=\"cuda:7\"):\n",
    "      super().__init__()\n",
    "      self.block1 = DetectorBlock(out_dim//4, img_size)\n",
    "      self.block2 = DetectorBlock(out_dim//2, img_size//2)\n",
    "      self.block3 = DetectorBlock(out_dim, img_size//4)\n",
    "      \n",
    "      self.regressor = nn.Sequential(\n",
    "        nn.Linear(8192*3, 8192),\n",
    "        nn.BatchNorm1d(8192),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(8192, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(4096, num_patches*2),\n",
    "        #nn.Sigmoid()\n",
    "      )\n",
    "      self.classifier = Discriminator(in_dim, out_dim, num_cats)\n",
    "      self.stride = stride\n",
    "      self.device = device\n",
    "\n",
    "    def forward(self, x, x_in):\n",
    "      B,C,H,W = x[0].shape\n",
    "      x1 = self.block1(x[0])\n",
    "      x2 = self.block2(x[1])\n",
    "      x3 = self.block3(x[2])\n",
    "      \n",
    "      x = torch.cat([x1,x2,x3], dim=1)\n",
    "      \n",
    "      x1 = self.regressor(x)\n",
    "      x1 = (x1 - torch.min(x1))/(torch.max(x1) - torch.min(x1))\n",
    "      \n",
    "      base = torch.ones(B,56,56,3)\n",
    "      #base = (base - torch.min(base))/(torch.max(base) - torch.min(base))\n",
    "      i=-1\n",
    "      for b,xx in zip(base,x1):\n",
    "        i+=1\n",
    "        for o in xx.reshape(-1,2):\n",
    "          x_min = int(o[0] * 56)\n",
    "          y_min = int(o[1] * 56)\n",
    "          x_max = x_min+self.stride\n",
    "          y_max = y_min+self.stride\n",
    "          b[x_min:x_max, y_min:y_max] = x_in[i].reshape(56,56,3)[x_min:x_max, y_min:y_max]\n",
    "      \n",
    "      base = base.to(self.device)\n",
    "\n",
    "      x2, x = self.classifier(base.reshape(B,3,56,56))\n",
    "      return x1, base, x, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bec5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 100\n",
    "DEVICE = \"cuda:7\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc4ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.00005\n",
    "model = Discriminator(3, 128, 12)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0918ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_d_model1.pickle\"\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "6a0d16e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.09773704130202532\n",
      "Test Loss: 0.22279034554958344\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.11587470583617687\n",
      "Test Loss: 0.18520136177539825\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.06125547410920262\n",
      "Test Loss: 0.1601082682609558\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.030876642325893044\n",
      "Test Loss: 0.1523774266242981\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.017175075132399797\n",
      "Test Loss: 0.1686883419752121\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.011832939577288926\n",
      "Test Loss: 0.20152780413627625\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.007993872684892267\n",
      "Test Loss: 0.22658026218414307\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.007469496224075556\n",
      "Test Loss: 0.24440152943134308\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.004760098294354975\n",
      "Test Loss: 0.24934493005275726\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.004006414412287995\n",
      "Test Loss: 0.25686317682266235\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.003463669476332143\n",
      "Test Loss: 0.25693047046661377\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.0037787009205203503\n",
      "Test Loss: 0.2524816691875458\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.0026194366510026157\n",
      "Test Loss: 0.2580968737602234\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.003622300719143823\n",
      "Test Loss: 0.2501392662525177\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.0030053920636419207\n",
      "Test Loss: 0.26073041558265686\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.003315257577924058\n",
      "Test Loss: 0.2509983777999878\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.002307287766598165\n",
      "Test Loss: 0.258778840303421\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.0027735300245694816\n",
      "Test Loss: 0.2548934817314148\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.0024871611094567925\n",
      "Test Loss: 0.2582141160964966\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.0021985446655889973\n",
      "Test Loss: 0.2554820775985718\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.0025280041445512325\n",
      "Test Loss: 0.2567472457885742\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.0024118803994497284\n",
      "Test Loss: 0.2570541501045227\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.0022367197525454685\n",
      "Test Loss: 0.2573663592338562\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.0026650220970623195\n",
      "Test Loss: 0.2536487877368927\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.002810299483826384\n",
      "Test Loss: 0.2552585005760193\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.0025728575128596276\n",
      "Test Loss: 0.25279903411865234\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.0020924548734910786\n",
      "Test Loss: 0.25494447350502014\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.001601687486981973\n",
      "Test Loss: 0.2542802095413208\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.0022219332749955356\n",
      "Test Loss: 0.2562732994556427\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.001446084163035266\n",
      "Test Loss: 0.2594597041606903\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.0024441361165372655\n",
      "Test Loss: 0.25411781668663025\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.0016562900418648496\n",
      "Test Loss: 0.252824991941452\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.0016950561112025753\n",
      "Test Loss: 0.2554253935813904\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.0017711962864268571\n",
      "Test Loss: 0.2544857859611511\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.0017610033391974866\n",
      "Test Loss: 0.2520441710948944\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.0019257668900536373\n",
      "Test Loss: 0.25306710600852966\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.0016081672656582668\n",
      "Test Loss: 0.2520219385623932\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.001996053251787089\n",
      "Test Loss: 0.2556072771549225\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.001607300408068113\n",
      "Test Loss: 0.25531113147735596\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.003747403941815719\n",
      "Test Loss: 0.25959885120391846\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.0019857360748574138\n",
      "Test Loss: 0.251221239566803\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.0018680716748349369\n",
      "Test Loss: 0.2595498561859131\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.0019049214461119846\n",
      "Test Loss: 0.2554352283477783\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.0024156513245543465\n",
      "Test Loss: 0.25602349638938904\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.0020753475691890344\n",
      "Test Loss: 0.2566806972026825\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.0016521477518836036\n",
      "Test Loss: 0.2570834159851074\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.0017919831007020548\n",
      "Test Loss: 0.2528614103794098\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.0017496201617177576\n",
      "Test Loss: 0.25352251529693604\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.001369307588902302\n",
      "Test Loss: 0.25369319319725037\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.0016758922138251364\n",
      "Test Loss: 0.2546120882034302\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.0015707073034718633\n",
      "Test Loss: 0.25479185581207275\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.0016612626786809415\n",
      "Test Loss: 0.2545221745967865\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.0013149508304195479\n",
      "Test Loss: 0.25447747111320496\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.0015047736815176904\n",
      "Test Loss: 0.25761300325393677\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.001242901271325536\n",
      "Test Loss: 0.2532038390636444\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.0016711051284801215\n",
      "Test Loss: 0.25355133414268494\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.0012449809146346524\n",
      "Test Loss: 0.25879523158073425\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.0021065919136162847\n",
      "Test Loss: 0.2559613287448883\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.001336923276539892\n",
      "Test Loss: 0.2551604211330414\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.0012782341509591788\n",
      "Test Loss: 0.2564002573490143\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.001680011540884152\n",
      "Test Loss: 0.253982275724411\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.0013615730567835271\n",
      "Test Loss: 0.25422435998916626\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.0017141574790002778\n",
      "Test Loss: 0.25860723853111267\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.001524340405012481\n",
      "Test Loss: 0.2604474425315857\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.0013642935373354703\n",
      "Test Loss: 0.25304171442985535\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.0015055006806505844\n",
      "Test Loss: 0.25856173038482666\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.001497870936873369\n",
      "Test Loss: 0.2534879148006439\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.0015870707429712638\n",
      "Test Loss: 0.2563976049423218\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.001277958624996245\n",
      "Test Loss: 0.255506694316864\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.0013568290160037577\n",
      "Test Loss: 0.2535519599914551\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.0011660319069051184\n",
      "Test Loss: 0.2564401626586914\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.0013219352986197919\n",
      "Test Loss: 0.2578149139881134\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.0013209578901296481\n",
      "Test Loss: 0.2559947371482849\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.0013412984990281984\n",
      "Test Loss: 0.2534824013710022\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.001842005949583836\n",
      "Test Loss: 0.2564668655395508\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.0013164159609004855\n",
      "Test Loss: 0.25176289677619934\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.001613206390175037\n",
      "Test Loss: 0.2578624486923218\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.0013121903102728538\n",
      "Test Loss: 0.25565335154533386\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.001228249748237431\n",
      "Test Loss: 0.2534703016281128\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.0011908335727639496\n",
      "Test Loss: 0.2555201053619385\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.0014147111141937785\n",
      "Test Loss: 0.25582486391067505\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.0010231763881165534\n",
      "Test Loss: 0.2553408443927765\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.001187034635222517\n",
      "Test Loss: 0.2608770430088043\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.0014170370995998383\n",
      "Test Loss: 0.257847398519516\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.001402240333845839\n",
      "Test Loss: 0.25120729207992554\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.0012850849743699655\n",
      "Test Loss: 0.2551933526992798\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.0013427106605377048\n",
      "Test Loss: 0.2552684247493744\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.0015754473133711144\n",
      "Test Loss: 0.2565000653266907\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.001111687124648597\n",
      "Test Loss: 0.25486335158348083\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.0010915241000475362\n",
      "Test Loss: 0.25572115182876587\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.001042836571286898\n",
      "Test Loss: 0.2560044825077057\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.0009881160513032228\n",
      "Test Loss: 0.25721460580825806\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.0016112738303490914\n",
      "Test Loss: 0.2533033788204193\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.0014998673577792943\n",
      "Test Loss: 0.25778886675834656\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.0013797210267512128\n",
      "Test Loss: 0.25232672691345215\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.0013385039928834885\n",
      "Test Loss: 0.25764304399490356\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.0011241262691328302\n",
      "Test Loss: 0.2557356655597687\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.0009246469344361685\n",
      "Test Loss: 0.255011647939682\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.0011304324871161953\n",
      "Test Loss: 0.2578895390033722\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.0009370953193865716\n",
      "Test Loss: 0.2554839253425598\n",
      "\n",
      "Epoch: 100\n",
      "Train Loss: 0.0009053080575540662\n",
      "Test Loss: 0.25568887591362\n",
      "\n",
      "Epoch: 101\n",
      "Train Loss: 0.000858856707054656\n",
      "Test Loss: 0.25353845953941345\n",
      "\n",
      "Epoch: 102\n",
      "Train Loss: 0.0008840797454467975\n",
      "Test Loss: 0.2531062364578247\n",
      "\n",
      "Epoch: 103\n",
      "Train Loss: 0.0011305940643069334\n",
      "Test Loss: 0.2525438368320465\n",
      "\n",
      "Epoch: 104\n",
      "Train Loss: 0.0010064247908303514\n",
      "Test Loss: 0.2533877193927765\n",
      "\n",
      "Epoch: 105\n",
      "Train Loss: 0.0012744986233883537\n",
      "Test Loss: 0.25361841917037964\n",
      "\n",
      "Epoch: 106\n",
      "Train Loss: 0.0010603165937936865\n",
      "Test Loss: 0.25519004464149475\n",
      "\n",
      "Epoch: 107\n",
      "Train Loss: 0.0012738672085106373\n",
      "Test Loss: 0.2532695531845093\n",
      "\n",
      "Epoch: 108\n",
      "Train Loss: 0.0009511609096080065\n",
      "Test Loss: 0.25652551651000977\n",
      "\n",
      "Epoch: 109\n",
      "Train Loss: 0.0009933488472597674\n",
      "Test Loss: 0.25638508796691895\n",
      "\n",
      "Epoch: 110\n",
      "Train Loss: 0.0010134987460332923\n",
      "Test Loss: 0.25567832589149475\n",
      "\n",
      "Epoch: 111\n",
      "Train Loss: 0.0009416855828021653\n",
      "Test Loss: 0.25502654910087585\n",
      "\n",
      "Epoch: 112\n",
      "Train Loss: 0.0007826616638340056\n",
      "Test Loss: 0.2541526257991791\n",
      "\n",
      "Epoch: 113\n",
      "Train Loss: 0.0007878596370574087\n",
      "Test Loss: 0.25453898310661316\n",
      "\n",
      "Epoch: 114\n",
      "Train Loss: 0.0010148349829250947\n",
      "Test Loss: 0.2589644491672516\n",
      "\n",
      "Epoch: 115\n",
      "Train Loss: 0.000985672239039559\n",
      "Test Loss: 0.25389835238456726\n",
      "\n",
      "Epoch: 116\n",
      "Train Loss: 0.0011094567817053758\n",
      "Test Loss: 0.25541383028030396\n",
      "\n",
      "Epoch: 117\n",
      "Train Loss: 0.0008241739124059677\n",
      "Test Loss: 0.2542174756526947\n",
      "\n",
      "Epoch: 118\n",
      "Train Loss: 0.0009596967938705347\n",
      "Test Loss: 0.2525929808616638\n",
      "\n",
      "Epoch: 119\n",
      "Train Loss: 0.0007234117801999673\n",
      "Test Loss: 0.2559400796890259\n",
      "\n",
      "Epoch: 120\n",
      "Train Loss: 0.0009855650350800715\n",
      "Test Loss: 0.2572285532951355\n",
      "\n",
      "Epoch: 121\n",
      "Train Loss: 0.0011559632694115862\n",
      "Test Loss: 0.253832072019577\n",
      "\n",
      "Epoch: 122\n",
      "Train Loss: 0.0012627239775611088\n",
      "Test Loss: 0.25412917137145996\n",
      "\n",
      "Epoch: 123\n",
      "Train Loss: 0.0011535306402947754\n",
      "Test Loss: 0.25327304005622864\n",
      "\n",
      "Epoch: 124\n",
      "Train Loss: 0.0010072405420942232\n",
      "Test Loss: 0.2534818649291992\n",
      "\n",
      "Epoch: 125\n",
      "Train Loss: 0.0011379632633179426\n",
      "Test Loss: 0.2539539039134979\n",
      "\n",
      "Epoch: 126\n",
      "Train Loss: 0.0012289119331398979\n",
      "Test Loss: 0.25714218616485596\n",
      "\n",
      "Epoch: 127\n",
      "Train Loss: 0.001403702663083095\n",
      "Test Loss: 0.2522817552089691\n",
      "\n",
      "Epoch: 128\n",
      "Train Loss: 0.001323263655649498\n",
      "Test Loss: 0.25359684228897095\n",
      "\n",
      "Epoch: 129\n",
      "Train Loss: 0.001306256846874021\n",
      "Test Loss: 0.25411343574523926\n",
      "\n",
      "Epoch: 130\n",
      "Train Loss: 0.001436331876902841\n",
      "Test Loss: 0.2504168748855591\n",
      "\n",
      "Epoch: 131\n",
      "Train Loss: 0.00117546807450708\n",
      "Test Loss: 0.2553989589214325\n",
      "\n",
      "Epoch: 132\n",
      "Train Loss: 0.0012010267964797094\n",
      "Test Loss: 0.25173044204711914\n",
      "\n",
      "Epoch: 133\n",
      "Train Loss: 0.001076710133929737\n",
      "Test Loss: 0.25649023056030273\n",
      "\n",
      "Epoch: 134\n",
      "Train Loss: 0.0009080447925953194\n",
      "Test Loss: 0.254886269569397\n",
      "\n",
      "Epoch: 135\n",
      "Train Loss: 0.000981761040748097\n",
      "Test Loss: 0.25361713767051697\n",
      "\n",
      "Epoch: 136\n",
      "Train Loss: 0.0008416169875999913\n",
      "Test Loss: 0.2538633346557617\n",
      "\n",
      "Epoch: 137\n",
      "Train Loss: 0.000825849871034734\n",
      "Test Loss: 0.2553083300590515\n",
      "\n",
      "Epoch: 138\n",
      "Train Loss: 0.0009679773793322966\n",
      "Test Loss: 0.25828394293785095\n",
      "\n",
      "Epoch: 139\n",
      "Train Loss: 0.0011214137339266017\n",
      "Test Loss: 0.25637954473495483\n",
      "\n",
      "Epoch: 140\n",
      "Train Loss: 0.001505931228166446\n",
      "Test Loss: 0.25331878662109375\n",
      "\n",
      "Epoch: 141\n",
      "Train Loss: 0.0012582560011651367\n",
      "Test Loss: 0.2567800283432007\n",
      "\n",
      "Epoch: 142\n",
      "Train Loss: 0.0012247395934537053\n",
      "Test Loss: 0.2516166865825653\n",
      "\n",
      "Epoch: 143\n",
      "Train Loss: 0.0009538892554701306\n",
      "Test Loss: 0.2539308965206146\n",
      "\n",
      "Epoch: 144\n",
      "Train Loss: 0.000809130520792678\n",
      "Test Loss: 0.254251629114151\n",
      "\n",
      "Epoch: 145\n",
      "Train Loss: 0.00097866149735637\n",
      "Test Loss: 0.25542253255844116\n",
      "\n",
      "Epoch: 146\n",
      "Train Loss: 0.0010577377179288305\n",
      "Test Loss: 0.25348418951034546\n",
      "\n",
      "Epoch: 147\n",
      "Train Loss: 0.0008483218407491222\n",
      "Test Loss: 0.2533266544342041\n",
      "\n",
      "Epoch: 148\n",
      "Train Loss: 0.0009138623281614855\n",
      "Test Loss: 0.2544705271720886\n",
      "\n",
      "Epoch: 149\n",
      "Train Loss: 0.0008492798297083937\n",
      "Test Loss: 0.2531595230102539\n",
      "\n",
      "Epoch: 150\n",
      "Train Loss: 0.0008744968581595458\n",
      "Test Loss: 0.25682905316352844\n",
      "\n",
      "Epoch: 151\n",
      "Train Loss: 0.0010134320909855887\n",
      "Test Loss: 0.2570638656616211\n",
      "\n",
      "Epoch: 152\n",
      "Train Loss: 0.0009007234330056235\n",
      "Test Loss: 0.2574658691883087\n",
      "\n",
      "Epoch: 153\n",
      "Train Loss: 0.0010332505844417028\n",
      "Test Loss: 0.25455623865127563\n",
      "\n",
      "Epoch: 154\n",
      "Train Loss: 0.0008407937712036073\n",
      "Test Loss: 0.2555008828639984\n",
      "\n",
      "Epoch: 155\n",
      "Train Loss: 0.0008481274780933745\n",
      "Test Loss: 0.2568850517272949\n",
      "\n",
      "Epoch: 156\n",
      "Train Loss: 0.0010290060308761895\n",
      "Test Loss: 0.2535119354724884\n",
      "\n",
      "Epoch: 157\n",
      "Train Loss: 0.0008937294842326082\n",
      "Test Loss: 0.25318512320518494\n",
      "\n",
      "Epoch: 158\n",
      "Train Loss: 0.0007985575139173307\n",
      "Test Loss: 0.2557523250579834\n",
      "\n",
      "Epoch: 159\n",
      "Train Loss: 0.001045312157657463\n",
      "Test Loss: 0.25522682070732117\n",
      "\n",
      "Epoch: 160\n",
      "Train Loss: 0.000818536318547558\n",
      "Test Loss: 0.2545567452907562\n",
      "\n",
      "Epoch: 161\n",
      "Train Loss: 0.0007901896024122834\n",
      "Test Loss: 0.25590524077415466\n",
      "\n",
      "Epoch: 162\n",
      "Train Loss: 0.0010860418769880198\n",
      "Test Loss: 0.2544906437397003\n",
      "\n",
      "Epoch: 163\n",
      "Train Loss: 0.0008034787024371326\n",
      "Test Loss: 0.2563793659210205\n",
      "\n",
      "Epoch: 164\n",
      "Train Loss: 0.0009081067983061075\n",
      "Test Loss: 0.25347885489463806\n",
      "\n",
      "Epoch: 165\n",
      "Train Loss: 0.0008388014466618188\n",
      "Test Loss: 0.25561994314193726\n",
      "\n",
      "Epoch: 166\n",
      "Train Loss: 0.0010106579939019866\n",
      "Test Loss: 0.25456488132476807\n",
      "\n",
      "Epoch: 167\n",
      "Train Loss: 0.0011148069752380252\n",
      "Test Loss: 0.25354650616645813\n",
      "\n",
      "Epoch: 168\n",
      "Train Loss: 0.0009419747948413715\n",
      "Test Loss: 0.2530229091644287\n",
      "\n",
      "Epoch: 169\n",
      "Train Loss: 0.0007963422758621164\n",
      "Test Loss: 0.25566041469573975\n",
      "\n",
      "Epoch: 170\n",
      "Train Loss: 0.0009081098978640512\n",
      "Test Loss: 0.25392088294029236\n",
      "\n",
      "Epoch: 171\n",
      "Train Loss: 0.0007523457024944946\n",
      "Test Loss: 0.2588633894920349\n",
      "\n",
      "Epoch: 172\n",
      "Train Loss: 0.0008953340729931369\n",
      "Test Loss: 0.2538536489009857\n",
      "\n",
      "Epoch: 173\n",
      "Train Loss: 0.0008225826168200001\n",
      "Test Loss: 0.2573525011539459\n",
      "\n",
      "Epoch: 174\n",
      "Train Loss: 0.0009088903898373246\n",
      "Test Loss: 0.25589630007743835\n",
      "\n",
      "Epoch: 175\n",
      "Train Loss: 0.0007403361159958877\n",
      "Test Loss: 0.2572292685508728\n",
      "\n",
      "Epoch: 176\n",
      "Train Loss: 0.0005659298622049391\n",
      "Test Loss: 0.25376495718955994\n",
      "\n",
      "Epoch: 177\n",
      "Train Loss: 0.0006291745557973627\n",
      "Test Loss: 0.2570152282714844\n",
      "\n",
      "Epoch: 178\n",
      "Train Loss: 0.0007791935495333746\n",
      "Test Loss: 0.25528794527053833\n",
      "\n",
      "Epoch: 179\n",
      "Train Loss: 0.0006235039436432999\n",
      "Test Loss: 0.25475800037384033\n",
      "\n",
      "Epoch: 180\n",
      "Train Loss: 0.000775156106101349\n",
      "Test Loss: 0.2555483281612396\n",
      "\n",
      "Epoch: 181\n",
      "Train Loss: 0.0006525901662826072\n",
      "Test Loss: 0.25621575117111206\n",
      "\n",
      "Epoch: 182\n",
      "Train Loss: 0.0007913189110695384\n",
      "Test Loss: 0.25296178460121155\n",
      "\n",
      "Epoch: 183\n",
      "Train Loss: 0.0006613908262806945\n",
      "Test Loss: 0.25337865948677063\n",
      "\n",
      "Epoch: 184\n",
      "Train Loss: 0.0008703867642907426\n",
      "Test Loss: 0.2536468207836151\n",
      "\n",
      "Epoch: 185\n",
      "Train Loss: 0.0007764655820210464\n",
      "Test Loss: 0.25522157549858093\n",
      "\n",
      "Epoch: 186\n",
      "Train Loss: 0.0007492236618418247\n",
      "Test Loss: 0.2516266107559204\n",
      "\n",
      "Epoch: 187\n",
      "Train Loss: 0.0007154524864745326\n",
      "Test Loss: 0.255052387714386\n",
      "\n",
      "Epoch: 188\n",
      "Train Loss: 0.0008865211711963639\n",
      "Test Loss: 0.2543366849422455\n",
      "\n",
      "Epoch: 189\n",
      "Train Loss: 0.0007057893963064998\n",
      "Test Loss: 0.25546714663505554\n",
      "\n",
      "Epoch: 190\n",
      "Train Loss: 0.0007192616612883285\n",
      "Test Loss: 0.2524532675743103\n",
      "\n",
      "Epoch: 191\n",
      "Train Loss: 0.0006484512268798426\n",
      "Test Loss: 0.2565549314022064\n",
      "\n",
      "Epoch: 192\n",
      "Train Loss: 0.0007784276021993719\n",
      "Test Loss: 0.25442424416542053\n",
      "\n",
      "Epoch: 193\n",
      "Train Loss: 0.0010827475562109612\n",
      "Test Loss: 0.25335654616355896\n",
      "\n",
      "Epoch: 194\n",
      "Train Loss: 0.0007978468711371534\n",
      "Test Loss: 0.25442105531692505\n",
      "\n",
      "Epoch: 195\n",
      "Train Loss: 0.000797266919107642\n",
      "Test Loss: 0.2530575692653656\n",
      "\n",
      "Epoch: 196\n",
      "Train Loss: 0.000718612500349991\n",
      "Test Loss: 0.2532881200313568\n",
      "\n",
      "Epoch: 197\n",
      "Train Loss: 0.0008054288846324198\n",
      "Test Loss: 0.25582873821258545\n",
      "\n",
      "Epoch: 198\n",
      "Train Loss: 0.0007976105625857599\n",
      "Test Loss: 0.2565118372440338\n",
      "\n",
      "Epoch: 199\n",
      "Train Loss: 0.0007659101611352526\n",
      "Test Loss: 0.2543990910053253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch:\",epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_index = np.array([x for x in range(train_tiles.shape[0])])\n",
    "    np.random.shuffle(train_index)\n",
    "\n",
    "    train_index = train_index.T[:700].reshape(-1,BATCH_SIZE)\n",
    "    i=-1\n",
    "    \n",
    "    model.train()\n",
    "    for tr_ind in train_index:\n",
    "        i+=1\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = train_tiles[tr_ind]\n",
    "        y = cluster_dists[tr_ind]\n",
    "        \n",
    "        x = x.reshape(-1,3,56,56)\n",
    "        y = y.to(DEVICE)\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        conv, pred = model(x)\n",
    "        del x\n",
    "        pl = loss(pred,y)\n",
    "        del y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pl.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += pl.item()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        x = test_tiles\n",
    "        y = te_cluster_dists\n",
    "        \n",
    "        x = x.reshape(-1,3,56,56).to(DEVICE)\n",
    "        y = y.to(torch.long).to(DEVICE)\n",
    "        conv, pred = model(x)\n",
    "        tl = loss(pred,y)\n",
    "        del x\n",
    "        del y\n",
    "    \n",
    "    \n",
    "    print(\"Train Loss:\", running_loss)\n",
    "    print(\"Test Loss:\", tl.item())\n",
    "    print()\n",
    "    del pl, tl\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f751d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "826b655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_model = Detector(3, 128, 12, 12, 14, 56, DEVICE)\n",
    "o_model = o_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "330ccff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "O_LR = 0.0001\n",
    "o_optimizer = torch.optim.Adam(o_model.parameters(), lr = O_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44e2f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.44655927022298175\n",
      "Test Loss: 0.2751169800758362\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.372843677798907\n",
      "Test Loss: 0.2941916882991791\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.3410715212424596\n",
      "Test Loss: 0.2913052439689636\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.32080231606960297\n",
      "Test Loss: 0.30430954694747925\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.3049542158842087\n",
      "Test Loss: 0.2656610608100891\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.2911191259821256\n",
      "Test Loss: 0.2457675337791443\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.2815807561079661\n",
      "Test Loss: 0.2387140393257141\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.2712120960156123\n",
      "Test Loss: 0.23537881672382355\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.2630285918712616\n",
      "Test Loss: 0.23358279466629028\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.25643699367841083\n",
      "Test Loss: 0.2320142388343811\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.25126846631368\n",
      "Test Loss: 0.22713172435760498\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.24361479034026465\n",
      "Test Loss: 0.22239196300506592\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.2375803366303444\n",
      "Test Loss: 0.21811732649803162\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.2382522945602735\n",
      "Test Loss: 0.2202693521976471\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.2329220175743103\n",
      "Test Loss: 0.21464309096336365\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.23044641564289728\n",
      "Test Loss: 0.20958571135997772\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.22485040128231049\n",
      "Test Loss: 0.20609521865844727\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.22299258907636008\n",
      "Test Loss: 0.20430888235569\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.22337069114049277\n",
      "Test Loss: 0.20377123355865479\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.21989827851454416\n",
      "Test Loss: 0.20174641907215118\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.21520567933718363\n",
      "Test Loss: 0.19863592088222504\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.21746960282325745\n",
      "Test Loss: 0.19684220850467682\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.2133980467915535\n",
      "Test Loss: 0.1977277547121048\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.21413432558377585\n",
      "Test Loss: 0.1946355104446411\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.2128307099143664\n",
      "Test Loss: 0.19674082100391388\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.20868456612030664\n",
      "Test Loss: 0.1924372911453247\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.20693845550219217\n",
      "Test Loss: 0.1907261461019516\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.2058659369746844\n",
      "Test Loss: 0.19242273271083832\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.20528520892063776\n",
      "Test Loss: 0.1897229701280594\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.20783704270919165\n",
      "Test Loss: 0.19242101907730103\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.20484634985526404\n",
      "Test Loss: 0.1895420253276825\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.20411858210961023\n",
      "Test Loss: 0.1887233406305313\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.20219679921865463\n",
      "Test Loss: 0.19614636898040771\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.2002944971124331\n",
      "Test Loss: 0.18732275068759918\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.19640491406122842\n",
      "Test Loss: 0.184847891330719\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.19977714866399765\n",
      "Test Loss: 0.18743614852428436\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.19961952914794287\n",
      "Test Loss: 0.18444643914699554\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.19924971461296082\n",
      "Test Loss: 0.1859290599822998\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.19892116139332452\n",
      "Test Loss: 0.18366190791130066\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.19573717564344406\n",
      "Test Loss: 0.18118125200271606\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.1971071089307467\n",
      "Test Loss: 0.18369507789611816\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.1966542974114418\n",
      "Test Loss: 0.18282072246074677\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.19576801111300787\n",
      "Test Loss: 0.1816723495721817\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.1938513865073522\n",
      "Test Loss: 0.17969344556331635\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.19307945171991983\n",
      "Test Loss: 0.17825964093208313\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.19346928844849268\n",
      "Test Loss: 0.17925935983657837\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.19255512952804565\n",
      "Test Loss: 0.1784663200378418\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.19362224886814752\n",
      "Test Loss: 0.17836984992027283\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.18985321621100107\n",
      "Test Loss: 0.1794564127922058\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.19000724206368128\n",
      "Test Loss: 0.1761983036994934\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.18976187209288278\n",
      "Test Loss: 0.1783217340707779\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.18936812380949655\n",
      "Test Loss: 0.17589609324932098\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.18777290731668472\n",
      "Test Loss: 0.18046356737613678\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.1880501632889112\n",
      "Test Loss: 0.17812713980674744\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.18670699000358582\n",
      "Test Loss: 0.17434194684028625\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.18660951902469\n",
      "Test Loss: 0.17604230344295502\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.18586787084738413\n",
      "Test Loss: 0.17337952554225922\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.1844309518734614\n",
      "Test Loss: 0.17266511917114258\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.18554047246774039\n",
      "Test Loss: 0.1732609122991562\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.1851221943895022\n",
      "Test Loss: 0.1750210076570511\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.18486559142669043\n",
      "Test Loss: 0.17579153180122375\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.18313906590143839\n",
      "Test Loss: 0.1735701560974121\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.18589892983436584\n",
      "Test Loss: 0.18154370784759521\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.18418555210034052\n",
      "Test Loss: 0.17587809264659882\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.1841340884566307\n",
      "Test Loss: 0.17453040182590485\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.1860355536142985\n",
      "Test Loss: 0.17446422576904297\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.18213994055986404\n",
      "Test Loss: 0.17506781220436096\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.18373637149731317\n",
      "Test Loss: 0.17402054369449615\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.18391871203978857\n",
      "Test Loss: 0.17386165261268616\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.18518650035063425\n",
      "Test Loss: 0.17153209447860718\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.1842083881298701\n",
      "Test Loss: 0.17286397516727448\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.18052837749322256\n",
      "Test Loss: 0.17163920402526855\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.18104708443085352\n",
      "Test Loss: 0.1693861037492752\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.181098202864329\n",
      "Test Loss: 0.1702585369348526\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.1794156034787496\n",
      "Test Loss: 0.17001648247241974\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.18277101467053095\n",
      "Test Loss: 0.16995126008987427\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.17972678939501444\n",
      "Test Loss: 0.16929969191551208\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.17876457422971725\n",
      "Test Loss: 0.1675671637058258\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.17918960253397623\n",
      "Test Loss: 0.1688876748085022\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.17843108624219894\n",
      "Test Loss: 0.1752125322818756\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.17805450409650803\n",
      "Test Loss: 0.16763007640838623\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.17702793578306833\n",
      "Test Loss: 0.16852784156799316\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.17714960128068924\n",
      "Test Loss: 0.1667466163635254\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.17869464804728827\n",
      "Test Loss: 0.16850247979164124\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.17550734182198843\n",
      "Test Loss: 0.16722290217876434\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.17629894614219666\n",
      "Test Loss: 0.1692235916852951\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.17645636945962906\n",
      "Test Loss: 0.16637732088565826\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.176801435649395\n",
      "Test Loss: 0.17617537081241608\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.17704928169647852\n",
      "Test Loss: 0.1668088734149933\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.17772111793359122\n",
      "Test Loss: 0.16591502726078033\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.1748726045091947\n",
      "Test Loss: 0.1675128936767578\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.176555335521698\n",
      "Test Loss: 0.16661468148231506\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.17631089190642038\n",
      "Test Loss: 0.16788145899772644\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.17457114160060883\n",
      "Test Loss: 0.16754330694675446\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.17607802400986353\n",
      "Test Loss: 0.16675597429275513\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.17522648721933365\n",
      "Test Loss: 0.16587898135185242\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.1730428288380305\n",
      "Test Loss: 0.1684715896844864\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.17448449631532034\n",
      "Test Loss: 0.1711142361164093\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.1759138678510984\n",
      "Test Loss: 0.16582025587558746\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.17291216055552164\n",
      "Test Loss: 0.1682833582162857\n",
      "\n",
      "Epoch: 100\n",
      "Train Loss: 0.17467334121465683\n",
      "Test Loss: 0.16391444206237793\n",
      "\n",
      "Epoch: 101\n",
      "Train Loss: 0.17431744933128357\n",
      "Test Loss: 0.16422949731349945\n",
      "\n",
      "Epoch: 102\n",
      "Train Loss: 0.1737235188484192\n",
      "Test Loss: 0.16546723246574402\n",
      "\n",
      "Epoch: 103\n",
      "Train Loss: 0.17476878066857657\n",
      "Test Loss: 0.1639164686203003\n",
      "\n",
      "Epoch: 104\n",
      "Train Loss: 0.1745064233740171\n",
      "Test Loss: 0.16980355978012085\n",
      "\n",
      "Epoch: 105\n",
      "Train Loss: 0.17396187533934912\n",
      "Test Loss: 0.16305962204933167\n",
      "\n",
      "Epoch: 106\n",
      "Train Loss: 0.17231829961140951\n",
      "Test Loss: 0.166387140750885\n",
      "\n",
      "Epoch: 107\n",
      "Train Loss: 0.17290102938810983\n",
      "Test Loss: 0.17003536224365234\n",
      "\n",
      "Epoch: 108\n",
      "Train Loss: 0.17058742543061575\n",
      "Test Loss: 0.16353994607925415\n",
      "\n",
      "Epoch: 109\n",
      "Train Loss: 0.17283627639214197\n",
      "Test Loss: 0.16799932718276978\n",
      "\n",
      "Epoch: 110\n",
      "Train Loss: 0.1735751157005628\n",
      "Test Loss: 0.16714569926261902\n",
      "\n",
      "Epoch: 111\n",
      "Train Loss: 0.17345780382553735\n",
      "Test Loss: 0.1621856838464737\n",
      "\n",
      "Epoch: 112\n",
      "Train Loss: 0.1722206249833107\n",
      "Test Loss: 0.16421929001808167\n",
      "\n",
      "Epoch: 113\n",
      "Train Loss: 0.17203924556573233\n",
      "Test Loss: 0.16401897370815277\n",
      "\n",
      "Epoch: 114\n",
      "Train Loss: 0.1709809203942617\n",
      "Test Loss: 0.1635555773973465\n",
      "\n",
      "Epoch: 115\n",
      "Train Loss: 0.17142338305711746\n",
      "Test Loss: 0.16353511810302734\n",
      "\n",
      "Epoch: 116\n",
      "Train Loss: 0.1720732326308886\n",
      "Test Loss: 0.16372919082641602\n",
      "\n",
      "Epoch: 117\n",
      "Train Loss: 0.1717585250735283\n",
      "Test Loss: 0.16492289304733276\n",
      "\n",
      "Epoch: 118\n",
      "Train Loss: 0.171518936753273\n",
      "Test Loss: 0.16519998013973236\n",
      "\n",
      "Epoch: 119\n",
      "Train Loss: 0.17116394887367883\n",
      "Test Loss: 0.1621466875076294\n",
      "\n",
      "Epoch: 120\n",
      "Train Loss: 0.1706633890668551\n",
      "Test Loss: 0.1645737886428833\n",
      "\n",
      "Epoch: 121\n",
      "Train Loss: 0.17021816223859787\n",
      "Test Loss: 0.1629897952079773\n",
      "\n",
      "Epoch: 122\n",
      "Train Loss: 0.16998523225386938\n",
      "Test Loss: 0.16157101094722748\n",
      "\n",
      "Epoch: 123\n",
      "Train Loss: 0.17005174855391184\n",
      "Test Loss: 0.1646125316619873\n",
      "\n",
      "Epoch: 124\n",
      "Train Loss: 0.16796640306711197\n",
      "Test Loss: 0.17778533697128296\n",
      "\n",
      "Epoch: 125\n",
      "Train Loss: 0.1707208752632141\n",
      "Test Loss: 0.1614670306444168\n",
      "\n",
      "Epoch: 126\n",
      "Train Loss: 0.1698229635755221\n",
      "Test Loss: 0.16255122423171997\n",
      "\n",
      "Epoch: 127\n",
      "Train Loss: 0.16756979376077652\n",
      "Test Loss: 0.1623402237892151\n",
      "\n",
      "Epoch: 128\n",
      "Train Loss: 0.1687153254946073\n",
      "Test Loss: 0.16200043261051178\n",
      "\n",
      "Epoch: 129\n",
      "Train Loss: 0.17051473259925842\n",
      "Test Loss: 0.16177555918693542\n",
      "\n",
      "Epoch: 130\n",
      "Train Loss: 0.16900064051151276\n",
      "Test Loss: 0.17901989817619324\n",
      "\n",
      "Epoch: 131\n",
      "Train Loss: 0.17008961737155914\n",
      "Test Loss: 0.16132135689258575\n",
      "\n",
      "Epoch: 132\n",
      "Train Loss: 0.16987665245930353\n",
      "Test Loss: 0.16927523910999298\n",
      "\n",
      "Epoch: 133\n",
      "Train Loss: 0.16846857716639838\n",
      "Test Loss: 0.16023245453834534\n",
      "\n",
      "Epoch: 134\n",
      "Train Loss: 0.16920542965332666\n",
      "Test Loss: 0.16368499398231506\n",
      "\n",
      "Epoch: 135\n",
      "Train Loss: 0.16875781367222467\n",
      "Test Loss: 0.16324132680892944\n",
      "\n",
      "Epoch: 136\n",
      "Train Loss: 0.1682309384147326\n",
      "Test Loss: 0.16121986508369446\n",
      "\n",
      "Epoch: 137\n",
      "Train Loss: 0.16942717134952545\n",
      "Test Loss: 0.16020309925079346\n",
      "\n",
      "Epoch: 138\n",
      "Train Loss: 0.16612215836842856\n",
      "Test Loss: 0.1607651561498642\n",
      "\n",
      "Epoch: 139\n",
      "Train Loss: 0.16597755750020346\n",
      "Test Loss: 0.1594606637954712\n",
      "\n",
      "Epoch: 140\n",
      "Train Loss: 0.1684852788845698\n",
      "Test Loss: 0.15978413820266724\n",
      "\n",
      "Epoch: 141\n",
      "Train Loss: 0.16740834961334863\n",
      "Test Loss: 0.16179493069648743\n",
      "\n",
      "Epoch: 142\n",
      "Train Loss: 0.16773849725723267\n",
      "Test Loss: 0.15975633263587952\n",
      "\n",
      "Epoch: 143\n",
      "Train Loss: 0.16648563742637634\n",
      "Test Loss: 0.16189926862716675\n",
      "\n",
      "Epoch: 144\n",
      "Train Loss: 0.16659831752379736\n",
      "Test Loss: 0.15879251062870026\n",
      "\n",
      "Epoch: 145\n",
      "Train Loss: 0.1666885366042455\n",
      "Test Loss: 0.15973471105098724\n",
      "\n",
      "Epoch: 146\n",
      "Train Loss: 0.16602021952470145\n",
      "Test Loss: 0.15977323055267334\n",
      "\n",
      "Epoch: 147\n",
      "Train Loss: 0.16807771225770315\n",
      "Test Loss: 0.15903431177139282\n",
      "\n",
      "Epoch: 148\n",
      "Train Loss: 0.16526455928881964\n",
      "Test Loss: 0.16000597178936005\n",
      "\n",
      "Epoch: 149\n",
      "Train Loss: 0.16650418688853583\n",
      "Test Loss: 0.1602356880903244\n",
      "\n",
      "Epoch: 150\n",
      "Train Loss: 0.1667119637131691\n",
      "Test Loss: 0.1587766706943512\n",
      "\n",
      "Epoch: 151\n",
      "Train Loss: 0.16438730557759604\n",
      "Test Loss: 0.1583755910396576\n",
      "\n",
      "Epoch: 152\n",
      "Train Loss: 0.16377719988425574\n",
      "Test Loss: 0.16604703664779663\n",
      "\n",
      "Epoch: 153\n",
      "Train Loss: 0.16504847755034766\n",
      "Test Loss: 0.15762414038181305\n",
      "\n",
      "Epoch: 154\n",
      "Train Loss: 0.16461139917373657\n",
      "Test Loss: 0.15859995782375336\n",
      "\n",
      "Epoch: 155\n",
      "Train Loss: 0.16586483269929886\n",
      "Test Loss: 0.1582147479057312\n",
      "\n",
      "Epoch: 156\n",
      "Train Loss: 0.16450282682975134\n",
      "Test Loss: 0.16218902170658112\n",
      "\n",
      "Epoch: 157\n",
      "Train Loss: 0.16623585919539133\n",
      "Test Loss: 0.15783879160881042\n",
      "\n",
      "Epoch: 158\n",
      "Train Loss: 0.16649245967467627\n",
      "Test Loss: 0.15808811783790588\n",
      "\n",
      "Epoch: 159\n",
      "Train Loss: 0.1666567400097847\n",
      "Test Loss: 0.15828296542167664\n",
      "\n",
      "Epoch: 160\n",
      "Train Loss: 0.1654780978957812\n",
      "Test Loss: 0.16330662369728088\n",
      "\n",
      "Epoch: 161\n",
      "Train Loss: 0.16196550925572714\n",
      "Test Loss: 0.15743084251880646\n",
      "\n",
      "Epoch: 162\n",
      "Train Loss: 0.16596216708421707\n",
      "Test Loss: 0.158554345369339\n",
      "\n",
      "Epoch: 163\n",
      "Train Loss: 0.16614778339862823\n",
      "Test Loss: 0.16799846291542053\n",
      "\n",
      "Epoch: 164\n",
      "Train Loss: 0.16481032222509384\n",
      "Test Loss: 0.15859563648700714\n",
      "\n",
      "Epoch: 165\n",
      "Train Loss: 0.1651731232802073\n",
      "Test Loss: 0.15763086080551147\n",
      "\n",
      "Epoch: 166\n",
      "Train Loss: 0.16318815698226294\n",
      "Test Loss: 0.1570245325565338\n",
      "\n",
      "Epoch: 167\n",
      "Train Loss: 0.16459334641695023\n",
      "Test Loss: 0.15807953476905823\n",
      "\n",
      "Epoch: 168\n",
      "Train Loss: 0.16175056745608649\n",
      "Test Loss: 0.1564672291278839\n",
      "\n",
      "Epoch: 169\n",
      "Train Loss: 0.16472382098436356\n",
      "Test Loss: 0.15575094521045685\n",
      "\n",
      "Epoch: 170\n",
      "Train Loss: 0.16310065984725952\n",
      "Test Loss: 0.15746454894542694\n",
      "\n",
      "Epoch: 171\n",
      "Train Loss: 0.1613782743612925\n",
      "Test Loss: 0.15684625506401062\n",
      "\n",
      "Epoch: 172\n",
      "Train Loss: 0.16307445615530014\n",
      "Test Loss: 0.16023819148540497\n",
      "\n",
      "Epoch: 173\n",
      "Train Loss: 0.16356858611106873\n",
      "Test Loss: 0.15984484553337097\n",
      "\n",
      "Epoch: 174\n",
      "Train Loss: 0.1648450270295143\n",
      "Test Loss: 0.15717938542366028\n",
      "\n",
      "Epoch: 175\n",
      "Train Loss: 0.16285705069700876\n",
      "Test Loss: 0.15672418475151062\n",
      "\n",
      "Epoch: 176\n",
      "Train Loss: 0.1630075623591741\n",
      "Test Loss: 0.1601387858390808\n",
      "\n",
      "Epoch: 177\n",
      "Train Loss: 0.16194972644249597\n",
      "Test Loss: 0.1578577160835266\n",
      "\n",
      "Epoch: 178\n",
      "Train Loss: 0.16134228308995566\n",
      "Test Loss: 0.16003496944904327\n",
      "\n",
      "Epoch: 179\n",
      "Train Loss: 0.16435099641482034\n",
      "Test Loss: 0.15779411792755127\n",
      "\n",
      "Epoch: 180\n",
      "Train Loss: 0.1628048320611318\n",
      "Test Loss: 0.15789157152175903\n",
      "\n",
      "Epoch: 181\n",
      "Train Loss: 0.16136678556601206\n",
      "Test Loss: 0.15862515568733215\n",
      "\n",
      "Epoch: 182\n",
      "Train Loss: 0.1634890933831533\n",
      "Test Loss: 0.1567891538143158\n",
      "\n",
      "Epoch: 183\n",
      "Train Loss: 0.16151913007100424\n",
      "Test Loss: 0.1556413471698761\n",
      "\n",
      "Epoch: 184\n",
      "Train Loss: 0.16377836962540945\n",
      "Test Loss: 0.1555623710155487\n",
      "\n",
      "Epoch: 185\n",
      "Train Loss: 0.1616384113828341\n",
      "Test Loss: 0.1565820872783661\n",
      "\n",
      "Epoch: 186\n",
      "Train Loss: 0.1628801922003428\n",
      "Test Loss: 0.1606549173593521\n",
      "\n",
      "Epoch: 187\n",
      "Train Loss: 0.16071044902006784\n",
      "Test Loss: 0.15468940138816833\n",
      "\n",
      "Epoch: 188\n",
      "Train Loss: 0.1629758800069491\n",
      "Test Loss: 0.1578141301870346\n",
      "\n",
      "Epoch: 189\n",
      "Train Loss: 0.16229528188705444\n",
      "Test Loss: 0.15615586936473846\n",
      "\n",
      "Epoch: 190\n",
      "Train Loss: 0.1608476589123408\n",
      "Test Loss: 0.16219064593315125\n",
      "\n",
      "Epoch: 191\n",
      "Train Loss: 0.16388275722662607\n",
      "Test Loss: 0.15715913474559784\n",
      "\n",
      "Epoch: 192\n",
      "Train Loss: 0.16016007463137308\n",
      "Test Loss: 0.15556733310222626\n",
      "\n",
      "Epoch: 193\n",
      "Train Loss: 0.16115354746580124\n",
      "Test Loss: 0.15470683574676514\n",
      "\n",
      "Epoch: 194\n",
      "Train Loss: 0.16316373646259308\n",
      "Test Loss: 0.15793833136558533\n",
      "\n",
      "Epoch: 195\n",
      "Train Loss: 0.16169877101977667\n",
      "Test Loss: 0.15530359745025635\n",
      "\n",
      "Epoch: 196\n",
      "Train Loss: 0.1612309068441391\n",
      "Test Loss: 0.15804478526115417\n",
      "\n",
      "Epoch: 197\n",
      "Train Loss: 0.16225542376438776\n",
      "Test Loss: 0.15853795409202576\n",
      "\n",
      "Epoch: 198\n",
      "Train Loss: 0.16133616119623184\n",
      "Test Loss: 0.16091443598270416\n",
      "\n",
      "Epoch: 199\n",
      "Train Loss: 0.16038720806439719\n",
      "Test Loss: 0.15598617494106293\n",
      "\n",
      "Epoch: 200\n",
      "Train Loss: 0.15901674330234528\n",
      "Test Loss: 0.16216638684272766\n",
      "\n",
      "Epoch: 201\n",
      "Train Loss: 0.16179298857847849\n",
      "Test Loss: 0.1562575101852417\n",
      "\n",
      "Epoch: 202\n",
      "Train Loss: 0.1624146377046903\n",
      "Test Loss: 0.15435519814491272\n",
      "\n",
      "Epoch: 203\n",
      "Train Loss: 0.16102677832047144\n",
      "Test Loss: 0.15690405666828156\n",
      "\n",
      "Epoch: 204\n",
      "Train Loss: 0.1616392433643341\n",
      "Test Loss: 0.15714624524116516\n",
      "\n",
      "Epoch: 205\n",
      "Train Loss: 0.1593590279420217\n",
      "Test Loss: 0.15648910403251648\n",
      "\n",
      "Epoch: 206\n",
      "Train Loss: 0.16132845729589462\n",
      "Test Loss: 0.15932628512382507\n",
      "\n",
      "Epoch: 207\n",
      "Train Loss: 0.15821473300457\n",
      "Test Loss: 0.15504580736160278\n",
      "\n",
      "Epoch: 208\n",
      "Train Loss: 0.1610859657327334\n",
      "Test Loss: 0.1540963351726532\n",
      "\n",
      "Epoch: 209\n",
      "Train Loss: 0.1573567564288775\n",
      "Test Loss: 0.157808318734169\n",
      "\n",
      "Epoch: 210\n",
      "Train Loss: 0.1571656713883082\n",
      "Test Loss: 0.157169908285141\n",
      "\n",
      "Epoch: 211\n",
      "Train Loss: 0.15994148204723993\n",
      "Test Loss: 0.1563507616519928\n",
      "\n",
      "Epoch: 212\n",
      "Train Loss: 0.16136832535266876\n",
      "Test Loss: 0.15603628754615784\n",
      "\n",
      "Epoch: 213\n",
      "Train Loss: 0.1589992418885231\n",
      "Test Loss: 0.15330998599529266\n",
      "\n",
      "Epoch: 214\n",
      "Train Loss: 0.15945027271906534\n",
      "Test Loss: 0.15250207483768463\n",
      "\n",
      "Epoch: 215\n",
      "Train Loss: 0.159674604733785\n",
      "Test Loss: 0.15393227338790894\n",
      "\n",
      "Epoch: 216\n",
      "Train Loss: 0.15804561227560043\n",
      "Test Loss: 0.15516486763954163\n",
      "\n",
      "Epoch: 217\n",
      "Train Loss: 0.15918742616971335\n",
      "Test Loss: 0.15662746131420135\n",
      "\n",
      "Epoch: 218\n",
      "Train Loss: 0.15971197932958603\n",
      "Test Loss: 0.16122758388519287\n",
      "\n",
      "Epoch: 219\n",
      "Train Loss: 0.15972292174895605\n",
      "Test Loss: 0.15619495511054993\n",
      "\n",
      "Epoch: 220\n",
      "Train Loss: 0.1587881421049436\n",
      "Test Loss: 0.15639427304267883\n",
      "\n",
      "Epoch: 221\n",
      "Train Loss: 0.15626578281323114\n",
      "Test Loss: 0.15657249093055725\n",
      "\n",
      "Epoch: 222\n",
      "Train Loss: 0.15922284374634424\n",
      "Test Loss: 0.15320494771003723\n",
      "\n",
      "Epoch: 223\n",
      "Train Loss: 0.1578267440199852\n",
      "Test Loss: 0.15791980922222137\n",
      "\n",
      "Epoch: 224\n",
      "Train Loss: 0.1606504867474238\n",
      "Test Loss: 0.15571583807468414\n",
      "\n",
      "Epoch: 225\n",
      "Train Loss: 0.1573872168858846\n",
      "Test Loss: 0.166304811835289\n",
      "\n",
      "Epoch: 226\n",
      "Train Loss: 0.15890967597564062\n",
      "Test Loss: 0.15665088593959808\n",
      "\n",
      "Epoch: 227\n",
      "Train Loss: 0.15927647550900778\n",
      "Test Loss: 0.15435779094696045\n",
      "\n",
      "Epoch: 228\n",
      "Train Loss: 0.1579963117837906\n",
      "Test Loss: 0.15205730497837067\n",
      "\n",
      "Epoch: 229\n",
      "Train Loss: 0.1574429968992869\n",
      "Test Loss: 0.15390849113464355\n",
      "\n",
      "Epoch: 230\n",
      "Train Loss: 0.15717069307963052\n",
      "Test Loss: 0.1548493206501007\n",
      "\n",
      "Epoch: 231\n",
      "Train Loss: 0.16155599802732468\n",
      "Test Loss: 0.1554776132106781\n",
      "\n",
      "Epoch: 232\n",
      "Train Loss: 0.15708317855993906\n",
      "Test Loss: 0.1555328220129013\n",
      "\n",
      "Epoch: 233\n",
      "Train Loss: 0.15734077741702399\n",
      "Test Loss: 0.15224692225456238\n",
      "\n",
      "Epoch: 234\n",
      "Train Loss: 0.15784865617752075\n",
      "Test Loss: 0.15444251894950867\n",
      "\n",
      "Epoch: 235\n",
      "Train Loss: 0.15818114827076593\n",
      "Test Loss: 0.15279056131839752\n",
      "\n",
      "Epoch: 236\n",
      "Train Loss: 0.15805218244592348\n",
      "Test Loss: 0.15904085338115692\n",
      "\n",
      "Epoch: 237\n",
      "Train Loss: 0.1572310874859492\n",
      "Test Loss: 0.1552315056324005\n",
      "\n",
      "Epoch: 238\n",
      "Train Loss: 0.15850109358628592\n",
      "Test Loss: 0.15374164283275604\n",
      "\n",
      "Epoch: 239\n",
      "Train Loss: 0.15923204272985458\n",
      "Test Loss: 0.15453079342842102\n",
      "\n",
      "Epoch: 240\n",
      "Train Loss: 0.15559069563945135\n",
      "Test Loss: 0.15221700072288513\n",
      "\n",
      "Epoch: 241\n",
      "Train Loss: 0.15726187825202942\n",
      "Test Loss: 0.15477614104747772\n",
      "\n",
      "Epoch: 242\n",
      "Train Loss: 0.1562026763955752\n",
      "Test Loss: 0.1578095555305481\n",
      "\n",
      "Epoch: 243\n",
      "Train Loss: 0.15507457156976065\n",
      "Test Loss: 0.16018357872962952\n",
      "\n",
      "Epoch: 244\n",
      "Train Loss: 0.1577066977818807\n",
      "Test Loss: 0.15451455116271973\n",
      "\n",
      "Epoch: 245\n",
      "Train Loss: 0.1559134100874265\n",
      "Test Loss: 0.15650103986263275\n",
      "\n",
      "Epoch: 246\n",
      "Train Loss: 0.15711595863103867\n",
      "Test Loss: 0.15406739711761475\n",
      "\n",
      "Epoch: 247\n",
      "Train Loss: 0.1561033179362615\n",
      "Test Loss: 0.1522963047027588\n",
      "\n",
      "Epoch: 248\n",
      "Train Loss: 0.15707469979921976\n",
      "Test Loss: 0.15472985804080963\n",
      "\n",
      "Epoch: 249\n",
      "Train Loss: 0.1580570563673973\n",
      "Test Loss: 0.15783058106899261\n",
      "\n",
      "Epoch: 250\n",
      "Train Loss: 0.15858374536037445\n",
      "Test Loss: 0.15844815969467163\n",
      "\n",
      "Epoch: 251\n",
      "Train Loss: 0.15660318483908972\n",
      "Test Loss: 0.15247371792793274\n",
      "\n",
      "Epoch: 252\n",
      "Train Loss: 0.15473627547423044\n",
      "Test Loss: 0.15100543200969696\n",
      "\n",
      "Epoch: 253\n",
      "Train Loss: 0.15603997061649957\n",
      "Test Loss: 0.15619120001792908\n",
      "\n",
      "Epoch: 254\n",
      "Train Loss: 0.15894672522942224\n",
      "Test Loss: 0.15226495265960693\n",
      "\n",
      "Epoch: 255\n",
      "Train Loss: 0.15627730637788773\n",
      "Test Loss: 0.15652406215667725\n",
      "\n",
      "Epoch: 256\n",
      "Train Loss: 0.15604733427365622\n",
      "Test Loss: 0.1598440557718277\n",
      "\n",
      "Epoch: 257\n",
      "Train Loss: 0.15491707374652228\n",
      "Test Loss: 0.15423768758773804\n",
      "\n",
      "Epoch: 258\n",
      "Train Loss: 0.15702325850725174\n",
      "Test Loss: 0.15765005350112915\n",
      "\n",
      "Epoch: 259\n",
      "Train Loss: 0.15508205443620682\n",
      "Test Loss: 0.1538330763578415\n",
      "\n",
      "Epoch: 260\n",
      "Train Loss: 0.15612735599279404\n",
      "Test Loss: 0.15065667033195496\n",
      "\n",
      "Epoch: 261\n",
      "Train Loss: 0.15411435812711716\n",
      "Test Loss: 0.15203121304512024\n",
      "\n",
      "Epoch: 262\n",
      "Train Loss: 0.1551028937101364\n",
      "Test Loss: 0.15369680523872375\n",
      "\n",
      "Epoch: 263\n",
      "Train Loss: 0.1564008022348086\n",
      "Test Loss: 0.15153655409812927\n",
      "\n",
      "Epoch: 264\n",
      "Train Loss: 0.15614814311265945\n",
      "Test Loss: 0.15116630494594574\n",
      "\n",
      "Epoch: 265\n",
      "Train Loss: 0.15310204525788626\n",
      "Test Loss: 0.15265873074531555\n",
      "\n",
      "Epoch: 266\n",
      "Train Loss: 0.15450685719648996\n",
      "Test Loss: 0.15073184669017792\n",
      "\n",
      "Epoch: 267\n",
      "Train Loss: 0.1544603556394577\n",
      "Test Loss: 0.1556508094072342\n",
      "\n",
      "Epoch: 268\n",
      "Train Loss: 0.15396152436733246\n",
      "Test Loss: 0.1535303294658661\n",
      "\n",
      "Epoch: 269\n",
      "Train Loss: 0.15613218396902084\n",
      "Test Loss: 0.1531081199645996\n",
      "\n",
      "Epoch: 270\n",
      "Train Loss: 0.1568771700064341\n",
      "Test Loss: 0.15565034747123718\n",
      "\n",
      "Epoch: 271\n",
      "Train Loss: 0.154861219227314\n",
      "Test Loss: 0.1604236215353012\n",
      "\n",
      "Epoch: 272\n",
      "Train Loss: 0.15458129594723383\n",
      "Test Loss: 0.15272077918052673\n",
      "\n",
      "Epoch: 273\n",
      "Train Loss: 0.15421591947476068\n",
      "Test Loss: 0.1561327874660492\n",
      "\n",
      "Epoch: 274\n",
      "Train Loss: 0.15594127774238586\n",
      "Test Loss: 0.1512681245803833\n",
      "\n",
      "Epoch: 275\n",
      "Train Loss: 0.154953271150589\n",
      "Test Loss: 0.15833114087581635\n",
      "\n",
      "Epoch: 276\n",
      "Train Loss: 0.15511792401472727\n",
      "Test Loss: 0.15164566040039062\n",
      "\n",
      "Epoch: 277\n",
      "Train Loss: 0.15584072470664978\n",
      "Test Loss: 0.1521454155445099\n",
      "\n",
      "Epoch: 278\n",
      "Train Loss: 0.15687437603871027\n",
      "Test Loss: 0.14963938295841217\n",
      "\n",
      "Epoch: 279\n",
      "Train Loss: 0.15484454234441122\n",
      "Test Loss: 0.1563762128353119\n",
      "\n",
      "Epoch: 280\n",
      "Train Loss: 0.1524663344025612\n",
      "Test Loss: 0.1501033753156662\n",
      "\n",
      "Epoch: 281\n",
      "Train Loss: 0.15451164295276007\n",
      "Test Loss: 0.15549276769161224\n",
      "\n",
      "Epoch: 282\n",
      "Train Loss: 0.15427050987879434\n",
      "Test Loss: 0.1496507078409195\n",
      "\n",
      "Epoch: 283\n",
      "Train Loss: 0.15415304526686668\n",
      "Test Loss: 0.1537362039089203\n",
      "\n",
      "Epoch: 284\n",
      "Train Loss: 0.15306867038210234\n",
      "Test Loss: 0.14965075254440308\n",
      "\n",
      "Epoch: 285\n",
      "Train Loss: 0.15360620866219202\n",
      "Test Loss: 0.15125468373298645\n",
      "\n",
      "Epoch: 286\n",
      "Train Loss: 0.15310650567213693\n",
      "Test Loss: 0.14879411458969116\n",
      "\n",
      "Epoch: 287\n",
      "Train Loss: 0.15276159346103668\n",
      "Test Loss: 0.14881190657615662\n",
      "\n",
      "Epoch: 288\n",
      "Train Loss: 0.15279103443026543\n",
      "Test Loss: 0.14982348680496216\n",
      "\n",
      "Epoch: 289\n",
      "Train Loss: 0.1533725510040919\n",
      "Test Loss: 0.1516924798488617\n",
      "\n",
      "Epoch: 290\n",
      "Train Loss: 0.15616367012262344\n",
      "Test Loss: 0.15530049800872803\n",
      "\n",
      "Epoch: 291\n",
      "Train Loss: 0.1533002331852913\n",
      "Test Loss: 0.15233635902404785\n",
      "\n",
      "Epoch: 292\n",
      "Train Loss: 0.1548475573460261\n",
      "Test Loss: 0.15561679005622864\n",
      "\n",
      "Epoch: 293\n",
      "Train Loss: 0.1533056398232778\n",
      "Test Loss: 0.15507356822490692\n",
      "\n",
      "Epoch: 294\n",
      "Train Loss: 0.15449276318152746\n",
      "Test Loss: 0.15072974562644958\n",
      "\n",
      "Epoch: 295\n",
      "Train Loss: 0.15432005872329077\n",
      "Test Loss: 0.152800515294075\n",
      "\n",
      "Epoch: 296\n",
      "Train Loss: 0.15507815281550089\n",
      "Test Loss: 0.15527376532554626\n",
      "\n",
      "Epoch: 297\n",
      "Train Loss: 0.15296189238627753\n",
      "Test Loss: 0.1547999531030655\n",
      "\n",
      "Epoch: 298\n",
      "Train Loss: 0.1555121267835299\n",
      "Test Loss: 0.15162669122219086\n",
      "\n",
      "Epoch: 299\n",
      "Train Loss: 0.15337405850489935\n",
      "Test Loss: 0.15180906653404236\n",
      "\n",
      "Epoch: 300\n",
      "Train Loss: 0.15362091114123663\n",
      "Test Loss: 0.1491130143404007\n",
      "\n",
      "Epoch: 301\n",
      "Train Loss: 0.15416564792394638\n",
      "Test Loss: 0.15488970279693604\n",
      "\n",
      "Epoch: 302\n",
      "Train Loss: 0.15199745322267214\n",
      "Test Loss: 0.14910416305065155\n",
      "\n",
      "Epoch: 303\n",
      "Train Loss: 0.15320242693026861\n",
      "Test Loss: 0.15562039613723755\n",
      "\n",
      "Epoch: 304\n",
      "Train Loss: 0.15206910421450934\n",
      "Test Loss: 0.15193724632263184\n",
      "\n",
      "Epoch: 305\n",
      "Train Loss: 0.15396377692619959\n",
      "Test Loss: 0.15023578703403473\n",
      "\n",
      "Epoch: 306\n",
      "Train Loss: 0.15247227003177008\n",
      "Test Loss: 0.1556377112865448\n",
      "\n",
      "Epoch: 307\n",
      "Train Loss: 0.15383748585979143\n",
      "Test Loss: 0.1510009914636612\n",
      "\n",
      "Epoch: 308\n",
      "Train Loss: 0.15312591940164566\n",
      "Test Loss: 0.16398248076438904\n",
      "\n",
      "Epoch: 309\n",
      "Train Loss: 0.1535032093524933\n",
      "Test Loss: 0.1513325423002243\n",
      "\n",
      "Epoch: 310\n",
      "Train Loss: 0.15257826944192251\n",
      "Test Loss: 0.1508309245109558\n",
      "\n",
      "Epoch: 311\n",
      "Train Loss: 0.15268117934465408\n",
      "Test Loss: 0.1528051495552063\n",
      "\n",
      "Epoch: 312\n",
      "Train Loss: 0.15398833403984705\n",
      "Test Loss: 0.1504599004983902\n",
      "\n",
      "Epoch: 313\n",
      "Train Loss: 0.15194068600734076\n",
      "Test Loss: 0.1490708887577057\n",
      "\n",
      "Epoch: 314\n",
      "Train Loss: 0.15117370585600534\n",
      "Test Loss: 0.15352529287338257\n",
      "\n",
      "Epoch: 315\n",
      "Train Loss: 0.152421236038208\n",
      "Test Loss: 0.1506643295288086\n",
      "\n",
      "Epoch: 316\n",
      "Train Loss: 0.1529087945818901\n",
      "Test Loss: 0.15011605620384216\n",
      "\n",
      "Epoch: 317\n",
      "Train Loss: 0.15219241380691528\n",
      "Test Loss: 0.14940869808197021\n",
      "\n",
      "Epoch: 318\n",
      "Train Loss: 0.15324193860093752\n",
      "Test Loss: 0.14941978454589844\n",
      "\n",
      "Epoch: 319\n",
      "Train Loss: 0.15350111573934555\n",
      "Test Loss: 0.14804092049598694\n",
      "\n",
      "Epoch: 320\n",
      "Train Loss: 0.1519148809214433\n",
      "Test Loss: 0.14798061549663544\n",
      "\n",
      "Epoch: 321\n",
      "Train Loss: 0.15163645024100939\n",
      "Test Loss: 0.1542879045009613\n",
      "\n",
      "Epoch: 322\n",
      "Train Loss: 0.15070264786481857\n",
      "Test Loss: 0.14922858774662018\n",
      "\n",
      "Epoch: 323\n",
      "Train Loss: 0.15093612546722093\n",
      "Test Loss: 0.15039703249931335\n",
      "\n",
      "Epoch: 324\n",
      "Train Loss: 0.1514155206580957\n",
      "Test Loss: 0.14870989322662354\n",
      "\n",
      "Epoch: 325\n",
      "Train Loss: 0.15123900026082993\n",
      "Test Loss: 0.15061473846435547\n",
      "\n",
      "Epoch: 326\n",
      "Train Loss: 0.150804802775383\n",
      "Test Loss: 0.1485218107700348\n",
      "\n",
      "Epoch: 327\n",
      "Train Loss: 0.15208163609107336\n",
      "Test Loss: 0.14783895015716553\n",
      "\n",
      "Epoch: 328\n",
      "Train Loss: 0.1506382500131925\n",
      "Test Loss: 0.14935120940208435\n",
      "\n",
      "Epoch: 329\n",
      "Train Loss: 0.15272373209396997\n",
      "Test Loss: 0.14893914759159088\n",
      "\n",
      "Epoch: 330\n",
      "Train Loss: 0.15231077869733176\n",
      "Test Loss: 0.15268419682979584\n",
      "\n",
      "Epoch: 331\n",
      "Train Loss: 0.15122892583409944\n",
      "Test Loss: 0.14978794753551483\n",
      "\n",
      "Epoch: 332\n",
      "Train Loss: 0.1515172819296519\n",
      "Test Loss: 0.14675618708133698\n",
      "\n",
      "Epoch: 333\n",
      "Train Loss: 0.15231477345029512\n",
      "Test Loss: 0.15354695916175842\n",
      "\n",
      "Epoch: 334\n",
      "Train Loss: 0.15395011256138483\n",
      "Test Loss: 0.15502658486366272\n",
      "\n",
      "Epoch: 335\n",
      "Train Loss: 0.1502347340186437\n",
      "Test Loss: 0.153631329536438\n",
      "\n",
      "Epoch: 336\n",
      "Train Loss: 0.15209422384699187\n",
      "Test Loss: 0.14984816312789917\n",
      "\n",
      "Epoch: 337\n",
      "Train Loss: 0.15039601176977158\n",
      "Test Loss: 0.14748166501522064\n",
      "\n",
      "Epoch: 338\n",
      "Train Loss: 0.15330215295155844\n",
      "Test Loss: 0.15017442405223846\n",
      "\n",
      "Epoch: 339\n",
      "Train Loss: 0.15060235311587652\n",
      "Test Loss: 0.1483117789030075\n",
      "\n",
      "Epoch: 340\n",
      "Train Loss: 0.1502634957432747\n",
      "Test Loss: 0.15301747620105743\n",
      "\n",
      "Epoch: 341\n",
      "Train Loss: 0.1519679849346479\n",
      "Test Loss: 0.15400758385658264\n",
      "\n",
      "Epoch: 342\n",
      "Train Loss: 0.1508962127069632\n",
      "Test Loss: 0.14838944375514984\n",
      "\n",
      "Epoch: 343\n",
      "Train Loss: 0.14894241963823637\n",
      "Test Loss: 0.14686425030231476\n",
      "\n",
      "Epoch: 344\n",
      "Train Loss: 0.15214191128810248\n",
      "Test Loss: 0.15090402960777283\n",
      "\n",
      "Epoch: 345\n",
      "Train Loss: 0.15117415164907774\n",
      "Test Loss: 0.15352743864059448\n",
      "\n",
      "Epoch: 346\n",
      "Train Loss: 0.1527618890007337\n",
      "Test Loss: 0.1475336253643036\n",
      "\n",
      "Epoch: 347\n",
      "Train Loss: 0.15037490179141363\n",
      "Test Loss: 0.15102507174015045\n",
      "\n",
      "Epoch: 348\n",
      "Train Loss: 0.1494574025273323\n",
      "Test Loss: 0.14652565121650696\n",
      "\n",
      "Epoch: 349\n",
      "Train Loss: 0.14996304859717688\n",
      "Test Loss: 0.1481008231639862\n",
      "\n",
      "Epoch: 350\n",
      "Train Loss: 0.14938273405035338\n",
      "Test Loss: 0.15828856825828552\n",
      "\n",
      "Epoch: 351\n",
      "Train Loss: 0.15176715205113092\n",
      "Test Loss: 0.14914663136005402\n",
      "\n",
      "Epoch: 352\n",
      "Train Loss: 0.1505838930606842\n",
      "Test Loss: 0.14912356436252594\n",
      "\n",
      "Epoch: 353\n",
      "Train Loss: 0.15070857355991998\n",
      "Test Loss: 0.15213347971439362\n",
      "\n",
      "Epoch: 354\n",
      "Train Loss: 0.15178615475694338\n",
      "Test Loss: 0.14645515382289886\n",
      "\n",
      "Epoch: 355\n",
      "Train Loss: 0.15041455129782358\n",
      "Test Loss: 0.1480698138475418\n",
      "\n",
      "Epoch: 356\n",
      "Train Loss: 0.1491920898358027\n",
      "Test Loss: 0.14770899713039398\n",
      "\n",
      "Epoch: 357\n",
      "Train Loss: 0.1488416555027167\n",
      "Test Loss: 0.14719873666763306\n",
      "\n",
      "Epoch: 358\n",
      "Train Loss: 0.14929704864819845\n",
      "Test Loss: 0.15001916885375977\n",
      "\n",
      "Epoch: 359\n",
      "Train Loss: 0.15178867553671202\n",
      "Test Loss: 0.1504460871219635\n",
      "\n",
      "Epoch: 360\n",
      "Train Loss: 0.14967955897251764\n",
      "Test Loss: 0.14701995253562927\n",
      "\n",
      "Epoch: 361\n",
      "Train Loss: 0.14834042514363924\n",
      "Test Loss: 0.14629271626472473\n",
      "\n",
      "Epoch: 362\n",
      "Train Loss: 0.14883986860513687\n",
      "Test Loss: 0.14756929874420166\n",
      "\n",
      "Epoch: 363\n",
      "Train Loss: 0.1515272632241249\n",
      "Test Loss: 0.14607854187488556\n",
      "\n",
      "Epoch: 364\n",
      "Train Loss: 0.1486868510643641\n",
      "Test Loss: 0.1473853886127472\n",
      "\n",
      "Epoch: 365\n",
      "Train Loss: 0.15057513366142908\n",
      "Test Loss: 0.15218941867351532\n",
      "\n",
      "Epoch: 366\n",
      "Train Loss: 0.1502757283548514\n",
      "Test Loss: 0.15125708281993866\n",
      "\n",
      "Epoch: 367\n",
      "Train Loss: 0.14881151542067528\n",
      "Test Loss: 0.15195909142494202\n",
      "\n",
      "Epoch: 368\n",
      "Train Loss: 0.15043477341532707\n",
      "Test Loss: 0.15022969245910645\n",
      "\n",
      "Epoch: 369\n",
      "Train Loss: 0.14992965385317802\n",
      "Test Loss: 0.14575152099132538\n",
      "\n",
      "Epoch: 370\n",
      "Train Loss: 0.15052995334068933\n",
      "Test Loss: 0.14831215143203735\n",
      "\n",
      "Epoch: 371\n",
      "Train Loss: 0.149803027510643\n",
      "Test Loss: 0.15383343398571014\n",
      "\n",
      "Epoch: 372\n",
      "Train Loss: 0.14884313071767488\n",
      "Test Loss: 0.1450175642967224\n",
      "\n",
      "Epoch: 373\n",
      "Train Loss: 0.15148478746414185\n",
      "Test Loss: 0.1455775499343872\n",
      "\n",
      "Epoch: 374\n",
      "Train Loss: 0.15075420091549555\n",
      "Test Loss: 0.14628922939300537\n",
      "\n",
      "Epoch: 375\n",
      "Train Loss: 0.1495491104821364\n",
      "Test Loss: 0.1464848816394806\n",
      "\n",
      "Epoch: 376\n",
      "Train Loss: 0.1483791172504425\n",
      "Test Loss: 0.14662663638591766\n",
      "\n",
      "Epoch: 377\n",
      "Train Loss: 0.15009146432081857\n",
      "Test Loss: 0.14707554876804352\n",
      "\n",
      "Epoch: 378\n",
      "Train Loss: 0.15112030506134033\n",
      "Test Loss: 0.14873869717121124\n",
      "\n",
      "Epoch: 379\n",
      "Train Loss: 0.15158198277155557\n",
      "Test Loss: 0.15249235928058624\n",
      "\n",
      "Epoch: 380\n",
      "Train Loss: 0.15042751158277193\n",
      "Test Loss: 0.1477692723274231\n",
      "\n",
      "Epoch: 381\n",
      "Train Loss: 0.14978905767202377\n",
      "Test Loss: 0.14829126000404358\n",
      "\n",
      "Epoch: 382\n",
      "Train Loss: 0.14956522112091383\n",
      "Test Loss: 0.14762048423290253\n",
      "\n",
      "Epoch: 383\n",
      "Train Loss: 0.1504496286312739\n",
      "Test Loss: 0.15264154970645905\n",
      "\n",
      "Epoch: 384\n",
      "Train Loss: 0.1517114738623301\n",
      "Test Loss: 0.15279079973697662\n",
      "\n",
      "Epoch: 385\n",
      "Train Loss: 0.15101036677757898\n",
      "Test Loss: 0.14787234365940094\n",
      "\n",
      "Epoch: 386\n",
      "Train Loss: 0.14712597678105035\n",
      "Test Loss: 0.15236696600914001\n",
      "\n",
      "Epoch: 387\n",
      "Train Loss: 0.15113077064355215\n",
      "Test Loss: 0.14750492572784424\n",
      "\n",
      "Epoch: 388\n",
      "Train Loss: 0.1503500392039617\n",
      "Test Loss: 0.1528392732143402\n",
      "\n",
      "Epoch: 389\n",
      "Train Loss: 0.15151560554901758\n",
      "Test Loss: 0.14827150106430054\n",
      "\n",
      "Epoch: 390\n",
      "Train Loss: 0.14860634381572405\n",
      "Test Loss: 0.14768154919147491\n",
      "\n",
      "Epoch: 391\n",
      "Train Loss: 0.14868101477622986\n",
      "Test Loss: 0.14747896790504456\n",
      "\n",
      "Epoch: 392\n",
      "Train Loss: 0.14876262098550797\n",
      "Test Loss: 0.14921535551548004\n",
      "\n",
      "Epoch: 393\n",
      "Train Loss: 0.14926047374804816\n",
      "Test Loss: 0.1448964923620224\n",
      "\n",
      "Epoch: 394\n",
      "Train Loss: 0.14873219653964043\n",
      "Test Loss: 0.15181457996368408\n",
      "\n",
      "Epoch: 395\n",
      "Train Loss: 0.1483679972589016\n",
      "Test Loss: 0.1487288922071457\n",
      "\n",
      "Epoch: 396\n",
      "Train Loss: 0.14762677252292633\n",
      "Test Loss: 0.14774323999881744\n",
      "\n",
      "Epoch: 397\n",
      "Train Loss: 0.14847300574183464\n",
      "Test Loss: 0.14660167694091797\n",
      "\n",
      "Epoch: 398\n",
      "Train Loss: 0.14929709335168204\n",
      "Test Loss: 0.14587023854255676\n",
      "\n",
      "Epoch: 399\n",
      "Train Loss: 0.1500575840473175\n",
      "Test Loss: 0.14664308726787567\n",
      "\n",
      "Epoch: 400\n",
      "Train Loss: 0.14772713681062064\n",
      "Test Loss: 0.14922690391540527\n",
      "\n",
      "Epoch: 401\n",
      "Train Loss: 0.14670649295051894\n",
      "Test Loss: 0.14568814635276794\n",
      "\n",
      "Epoch: 402\n",
      "Train Loss: 0.14864660426974297\n",
      "Test Loss: 0.14873307943344116\n",
      "\n",
      "Epoch: 403\n",
      "Train Loss: 0.14679033433397612\n",
      "Test Loss: 0.14968052506446838\n",
      "\n",
      "Epoch: 404\n",
      "Train Loss: 0.14665533726414046\n",
      "Test Loss: 0.14433369040489197\n",
      "\n",
      "Epoch: 405\n",
      "Train Loss: 0.14752511555949846\n",
      "Test Loss: 0.14519205689430237\n",
      "\n",
      "Epoch: 406\n",
      "Train Loss: 0.1463205317656199\n",
      "Test Loss: 0.14737364649772644\n",
      "\n",
      "Epoch: 407\n",
      "Train Loss: 0.14889842520157495\n",
      "Test Loss: 0.1451474130153656\n",
      "\n",
      "Epoch: 408\n",
      "Train Loss: 0.14915280292431513\n",
      "Test Loss: 0.14578133821487427\n",
      "\n",
      "Epoch: 409\n",
      "Train Loss: 0.14891155809164047\n",
      "Test Loss: 0.15103349089622498\n",
      "\n",
      "Epoch: 410\n",
      "Train Loss: 0.1482922745247682\n",
      "Test Loss: 0.14522485435009003\n",
      "\n",
      "Epoch: 411\n",
      "Train Loss: 0.14561951036254564\n",
      "Test Loss: 0.14844250679016113\n",
      "\n",
      "Epoch: 412\n",
      "Train Loss: 0.14682957033316293\n",
      "Test Loss: 0.14597336947917938\n",
      "\n",
      "Epoch: 413\n",
      "Train Loss: 0.14698460698127747\n",
      "Test Loss: 0.14895805716514587\n",
      "\n",
      "Epoch: 414\n",
      "Train Loss: 0.14751585076252619\n",
      "Test Loss: 0.14584852755069733\n",
      "\n",
      "Epoch: 415\n",
      "Train Loss: 0.1480641265710195\n",
      "Test Loss: 0.14390690624713898\n",
      "\n",
      "Epoch: 416\n",
      "Train Loss: 0.1481492891907692\n",
      "Test Loss: 0.15279966592788696\n",
      "\n",
      "Epoch: 417\n",
      "Train Loss: 0.14789254715045294\n",
      "Test Loss: 0.1451999545097351\n",
      "\n",
      "Epoch: 418\n",
      "Train Loss: 0.14883005370696387\n",
      "Test Loss: 0.14923125505447388\n",
      "\n",
      "Epoch: 419\n",
      "Train Loss: 0.1480675550798575\n",
      "Test Loss: 0.15092764794826508\n",
      "\n",
      "Epoch: 420\n",
      "Train Loss: 0.14742154876391092\n",
      "Test Loss: 0.14553090929985046\n",
      "\n",
      "Epoch: 421\n",
      "Train Loss: 0.14819026862581572\n",
      "Test Loss: 0.1472465991973877\n",
      "\n",
      "Epoch: 422\n",
      "Train Loss: 0.14625057329734167\n",
      "Test Loss: 0.14667509496212006\n",
      "\n",
      "Epoch: 423\n",
      "Train Loss: 0.14948371797800064\n",
      "Test Loss: 0.14642703533172607\n",
      "\n",
      "Epoch: 424\n",
      "Train Loss: 0.14686240628361702\n",
      "Test Loss: 0.1477014273405075\n",
      "\n",
      "Epoch: 425\n",
      "Train Loss: 0.14707853396733603\n",
      "Test Loss: 0.1443726122379303\n",
      "\n",
      "Epoch: 426\n",
      "Train Loss: 0.14752522855997086\n",
      "Test Loss: 0.1508188545703888\n",
      "\n",
      "Epoch: 427\n",
      "Train Loss: 0.14640839273730913\n",
      "Test Loss: 0.1494423896074295\n",
      "\n",
      "Epoch: 428\n",
      "Train Loss: 0.14645105476180711\n",
      "Test Loss: 0.1439746618270874\n",
      "\n",
      "Epoch: 429\n",
      "Train Loss: 0.1470069189866384\n",
      "Test Loss: 0.14446748793125153\n",
      "\n",
      "Epoch: 430\n",
      "Train Loss: 0.14789228762189546\n",
      "Test Loss: 0.14625251293182373\n",
      "\n",
      "Epoch: 431\n",
      "Train Loss: 0.14721914877494177\n",
      "Test Loss: 0.14434614777565002\n",
      "\n",
      "Epoch: 432\n",
      "Train Loss: 0.14866217225790024\n",
      "Test Loss: 0.14538788795471191\n",
      "\n",
      "Epoch: 433\n",
      "Train Loss: 0.14699561148881912\n",
      "Test Loss: 0.14793694019317627\n",
      "\n",
      "Epoch: 434\n",
      "Train Loss: 0.14642518758773804\n",
      "Test Loss: 0.14738132059574127\n",
      "\n",
      "Epoch: 435\n",
      "Train Loss: 0.14658559237917265\n",
      "Test Loss: 0.15087245404720306\n",
      "\n",
      "Epoch: 436\n",
      "Train Loss: 0.14792204896608988\n",
      "Test Loss: 0.14340448379516602\n",
      "\n",
      "Epoch: 437\n",
      "Train Loss: 0.14654254168272018\n",
      "Test Loss: 0.14770297706127167\n",
      "\n",
      "Epoch: 438\n",
      "Train Loss: 0.14672401423255602\n",
      "Test Loss: 0.14325502514839172\n",
      "\n",
      "Epoch: 439\n",
      "Train Loss: 0.14679073790709177\n",
      "Test Loss: 0.14823071658611298\n",
      "\n",
      "Epoch: 440\n",
      "Train Loss: 0.1456853337585926\n",
      "Test Loss: 0.1533433198928833\n",
      "\n",
      "Epoch: 441\n",
      "Train Loss: 0.14843222002188364\n",
      "Test Loss: 0.1442861258983612\n",
      "\n",
      "Epoch: 442\n",
      "Train Loss: 0.14728882412115732\n",
      "Test Loss: 0.1457243412733078\n",
      "\n",
      "Epoch: 443\n",
      "Train Loss: 0.14665120715896288\n",
      "Test Loss: 0.14545588195323944\n",
      "\n",
      "Epoch: 444\n",
      "Train Loss: 0.14776062468687692\n",
      "Test Loss: 0.1454174518585205\n",
      "\n",
      "Epoch: 445\n",
      "Train Loss: 0.14761862779657045\n",
      "Test Loss: 0.15290769934654236\n",
      "\n",
      "Epoch: 446\n",
      "Train Loss: 0.1453719399869442\n",
      "Test Loss: 0.1433340609073639\n",
      "\n",
      "Epoch: 447\n",
      "Train Loss: 0.1492466206351916\n",
      "Test Loss: 0.14483457803726196\n",
      "\n",
      "Epoch: 448\n",
      "Train Loss: 0.14589298392335573\n",
      "Test Loss: 0.14985941350460052\n",
      "\n",
      "Epoch: 449\n",
      "Train Loss: 0.14893588672081629\n",
      "Test Loss: 0.14416563510894775\n",
      "\n",
      "Epoch: 450\n",
      "Train Loss: 0.14851972833275795\n",
      "Test Loss: 0.14696897566318512\n",
      "\n",
      "Epoch: 451\n",
      "Train Loss: 0.14529352883497873\n",
      "Test Loss: 0.14931192994117737\n",
      "\n",
      "Epoch: 452\n",
      "Train Loss: 0.14682888115445772\n",
      "Test Loss: 0.14799010753631592\n",
      "\n",
      "Epoch: 453\n",
      "Train Loss: 0.14433820421497026\n",
      "Test Loss: 0.14704811573028564\n",
      "\n",
      "Epoch: 454\n",
      "Train Loss: 0.14648548141121864\n",
      "Test Loss: 0.14754176139831543\n",
      "\n",
      "Epoch: 455\n",
      "Train Loss: 0.1497915300230185\n",
      "Test Loss: 0.1476496458053589\n",
      "\n",
      "Epoch: 456\n",
      "Train Loss: 0.14626395826538405\n",
      "Test Loss: 0.15632914006710052\n",
      "\n",
      "Epoch: 457\n",
      "Train Loss: 0.1459179942806562\n",
      "Test Loss: 0.14876490831375122\n",
      "\n",
      "Epoch: 458\n",
      "Train Loss: 0.1448059839506944\n",
      "Test Loss: 0.15081313252449036\n",
      "\n",
      "Epoch: 459\n",
      "Train Loss: 0.14876639222105345\n",
      "Test Loss: 0.149652898311615\n",
      "\n",
      "Epoch: 460\n",
      "Train Loss: 0.14590522771080336\n",
      "Test Loss: 0.14723333716392517\n",
      "\n",
      "Epoch: 461\n",
      "Train Loss: 0.1467698042591413\n",
      "Test Loss: 0.1487998068332672\n",
      "\n",
      "Epoch: 462\n",
      "Train Loss: 0.14482500155766806\n",
      "Test Loss: 0.14602045714855194\n",
      "\n",
      "Epoch: 463\n",
      "Train Loss: 0.14669466018676758\n",
      "Test Loss: 0.15011580288410187\n",
      "\n",
      "Epoch: 464\n",
      "Train Loss: 0.14514233792821565\n",
      "Test Loss: 0.14685073494911194\n",
      "\n",
      "Epoch: 465\n",
      "Train Loss: 0.14584210639198622\n",
      "Test Loss: 0.14404359459877014\n",
      "\n",
      "Epoch: 466\n",
      "Train Loss: 0.14490900933742523\n",
      "Test Loss: 0.1432943046092987\n",
      "\n",
      "Epoch: 467\n",
      "Train Loss: 0.1446102832754453\n",
      "Test Loss: 0.1490476131439209\n",
      "\n",
      "Epoch: 468\n",
      "Train Loss: 0.14497137566407522\n",
      "Test Loss: 0.14471110701560974\n",
      "\n",
      "Epoch: 469\n",
      "Train Loss: 0.14703764269749323\n",
      "Test Loss: 0.15619497001171112\n",
      "\n",
      "Epoch: 470\n",
      "Train Loss: 0.14861488963166872\n",
      "Test Loss: 0.14528805017471313\n",
      "\n",
      "Epoch: 471\n",
      "Train Loss: 0.15049648409088454\n",
      "Test Loss: 0.1514292061328888\n",
      "\n",
      "Epoch: 472\n",
      "Train Loss: 0.14675096174081168\n",
      "Test Loss: 0.14800390601158142\n",
      "\n",
      "Epoch: 473\n",
      "Train Loss: 0.14919538920124373\n",
      "Test Loss: 0.1536169946193695\n",
      "\n",
      "Epoch: 474\n",
      "Train Loss: 0.14750070745746294\n",
      "Test Loss: 0.154184028506279\n",
      "\n",
      "Epoch: 475\n",
      "Train Loss: 0.1451988605161508\n",
      "Test Loss: 0.14650264382362366\n",
      "\n",
      "Epoch: 476\n",
      "Train Loss: 0.1460469290614128\n",
      "Test Loss: 0.14987921714782715\n",
      "\n",
      "Epoch: 477\n",
      "Train Loss: 0.14557212591171265\n",
      "Test Loss: 0.14970101416110992\n",
      "\n",
      "Epoch: 478\n",
      "Train Loss: 0.144843099017938\n",
      "Test Loss: 0.1448480635881424\n",
      "\n",
      "Epoch: 479\n",
      "Train Loss: 0.14536891132593155\n",
      "Test Loss: 0.1498597264289856\n",
      "\n",
      "Epoch: 480\n",
      "Train Loss: 0.14546876028180122\n",
      "Test Loss: 0.15112175047397614\n",
      "\n",
      "Epoch: 481\n",
      "Train Loss: 0.14530047525962195\n",
      "Test Loss: 0.14536967873573303\n",
      "\n",
      "Epoch: 482\n",
      "Train Loss: 0.1437154933810234\n",
      "Test Loss: 0.14982156455516815\n",
      "\n",
      "Epoch: 483\n",
      "Train Loss: 0.14611249168713888\n",
      "Test Loss: 0.14284636080265045\n",
      "\n",
      "Epoch: 484\n",
      "Train Loss: 0.14787251750628153\n",
      "Test Loss: 0.1465327888727188\n",
      "\n",
      "Epoch: 485\n",
      "Train Loss: 0.1462340665360292\n",
      "Test Loss: 0.1473492830991745\n",
      "\n",
      "Epoch: 486\n",
      "Train Loss: 0.14279688894748688\n",
      "Test Loss: 0.14391198754310608\n",
      "\n",
      "Epoch: 487\n",
      "Train Loss: 0.14604743445912996\n",
      "Test Loss: 0.14745420217514038\n",
      "\n",
      "Epoch: 488\n",
      "Train Loss: 0.14471385007103285\n",
      "Test Loss: 0.14482825994491577\n",
      "\n",
      "Epoch: 489\n",
      "Train Loss: 0.14544827615221342\n",
      "Test Loss: 0.14704467356204987\n",
      "\n",
      "Epoch: 490\n",
      "Train Loss: 0.1447745884458224\n",
      "Test Loss: 0.14728379249572754\n",
      "\n",
      "Epoch: 491\n",
      "Train Loss: 0.1455491135517756\n",
      "Test Loss: 0.1506933867931366\n",
      "\n",
      "Epoch: 492\n",
      "Train Loss: 0.1448391191661358\n",
      "Test Loss: 0.14907249808311462\n",
      "\n",
      "Epoch: 493\n",
      "Train Loss: 0.1439910208185514\n",
      "Test Loss: 0.14276224374771118\n",
      "\n",
      "Epoch: 494\n",
      "Train Loss: 0.14589611689249674\n",
      "Test Loss: 0.14371159672737122\n",
      "\n",
      "Epoch: 495\n",
      "Train Loss: 0.14666830996672311\n",
      "Test Loss: 0.14442139863967896\n",
      "\n",
      "Epoch: 496\n",
      "Train Loss: 0.14499858394265175\n",
      "Test Loss: 0.14303463697433472\n",
      "\n",
      "Epoch: 497\n",
      "Train Loss: 0.14501783748467764\n",
      "Test Loss: 0.1525914967060089\n",
      "\n",
      "Epoch: 498\n",
      "Train Loss: 0.14400131007035574\n",
      "Test Loss: 0.1489536613225937\n",
      "\n",
      "Epoch: 499\n",
      "Train Loss: 0.1449008323252201\n",
      "Test Loss: 0.14486782252788544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    print(\"Epoch:\",epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_index = np.array([x for x in range(train_tiles.shape[0])])\n",
    "    np.random.shuffle(train_index)\n",
    "\n",
    "    train_index = train_index.T[:700].reshape(-1,BATCH_SIZE)\n",
    "    i=-1\n",
    "    \n",
    "    o_model.train()\n",
    "    for tr_ind in train_index:\n",
    "        i+=1\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = train_tiles[tr_ind]\n",
    "        \n",
    "        x = x.reshape(-1,3,56,56)\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        y = cluster_dists[tr_ind]\n",
    "        y = y.to(DEVICE)\n",
    "    \n",
    "        x2, y2 = model(x)\n",
    "        \n",
    "        _, _, pred, ls = o_model(x2, x)\n",
    "        del x, _\n",
    "        pl = 2*loss(pred,y)\n",
    "        pl += loss(pred,y2)\n",
    "        \n",
    "        for x,y in zip(ls, x2):\n",
    "            pl += 0.15 * loss(x, y)\n",
    "        del y, y2, pred, x2, ls\n",
    "        \n",
    "        o_optimizer.zero_grad()\n",
    "        pl.backward()\n",
    "        o_optimizer.step()\n",
    "        \n",
    "        running_loss += pl.item()\n",
    "    \n",
    "    \n",
    "    o_model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        x = test_tiles\n",
    "        y = te_cluster_dists\n",
    "        y=y.to(DEVICE)\n",
    "        \n",
    "        x = x.reshape(-1,3,56,56).to(DEVICE)\n",
    "        x2, y2 = model(x)\n",
    "        _, _, pred, ls = o_model(x2, x)\n",
    "        del x\n",
    "        tl = 2*loss(pred,y)\n",
    "        tl += loss(pred,y2)\n",
    "        \n",
    "        for x,y in zip(ls, x2):\n",
    "            tl += 0.15 * loss(x, y)\n",
    "            \n",
    "        del y, pred\n",
    "        \n",
    "            \n",
    "        del x2, ls\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(\"Train Loss:\", running_loss/i)\n",
    "    print(\"Test Loss:\", tl.item())\n",
    "    print()\n",
    "    del pl, tl\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b8120cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774\n",
      "tensor([[0.3516, 0.0388, 0.1143, 0.3797, 0.0000, 0.3314, 1.0000, 0.6293, 0.1405,\n",
      "         0.0879, 0.4776, 0.3782, 0.2511, 0.2662, 0.4330, 0.5058, 0.6138, 0.1883,\n",
      "         0.5340, 0.0307, 0.7756, 0.2578, 0.1887, 0.6163]], device='cuda:7')\n",
      "tensor(11, dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0042281374335289\n",
      "0.0041646407917141914\n",
      "tensor([0.4733, 0.6606, 0.6185, 0.4300, 0.5236, 0.4783, 0.4405, 0.4522, 0.6363,\n",
      "        0.6444, 0.3779, 0.3574], device='cuda:7')\n",
      "tensor([0.3849, 0.6935, 0.6376, 0.3644, 0.4968, 0.4570, 0.5054, 0.3239, 0.6116,\n",
      "        0.6522, 0.4109, 0.2441])\n",
      "tensor([0.3893, 0.7049, 0.6508, 0.3685, 0.5042, 0.4642, 0.5034, 0.3186, 0.6272,\n",
      "        0.6657, 0.4097, 0.2434], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "n+=1\n",
    "model.eval()\n",
    "o_model.eval()\n",
    "print(n)\n",
    "with torch.no_grad():\n",
    "    in_img = train_tiles[n].reshape(3,56,56).unsqueeze(0).to(DEVICE)\n",
    "    x, y = model(in_img)\n",
    "    box, base, pred, l = o_model(x, in_img)\n",
    "\n",
    "    print(box)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "    axes[0].imshow(in_img.reshape(56,56,3).detach().cpu().numpy())\n",
    "    #axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(base.reshape(56,56,3).detach().cpu().numpy())\n",
    "    #axes[1].axis('off')\n",
    "    print(train_cluster[n])\n",
    "    plt.show()\n",
    "    print(loss(pred,y).item())\n",
    "    print(loss(pred,cluster_dists[n].to(DEVICE)).item())\n",
    "    print(pred[0])\n",
    "    print(cluster_dists[n])\n",
    "    print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "82d51ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_d_model1.pickle\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_o_model1.pickle\"\n",
    "torch.save(o_model.state_dict(), SAVE_PATH)\n",
    "\n",
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_d_model1_backup.pickle\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_o_model1_backup.pickle\"\n",
    "torch.save(o_model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "d345c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4961, 0.5171, 0.4878, 0.5181, 0.4893, 0.4994], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "o_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = o_model(conv[169].unsqueeze(0))[0]\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "2f8fd7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAALklJREFUeJzt3X9sled9//8X4J+AfYwNsTHY/Ej4EZJBFDeAm2TriFeEoigZ/JF1kca6aFUzQAlk2oK0hrbaZNZITZqO0KpjoEljrEyjFZWWLqKNo27AwAlqfhAXMhI7+BdQ/AMDtgP35w+++BvD/b7gXNz4OrafD+lIzXX5Oufyfe7Du+f4dd73mCiKIgEAMMTGht4AAGB0ogABAIKgAAEAgqAAAQCCoAABAIKgAAEAgqAAAQCCoAABAIKgAAEAgqAAAQCCyLpdd7xlyxa99NJLam1t1aJFi/T9739fixcvvuG6y5cvq7m5WQUFBRozZszt2h4A4DaJokjd3d0qLy/X2LGO9znRbbBr164oJycn+qd/+qfo/fffj/78z/88Kioqitra2m64tqmpKZLEjRs3btyG+a2pqcn57/2YKEq+GemSJUv0wAMP6B/+4R8kXXlXU1FRoXXr1umFF15wru3s7FRRUVHSW4KkrKz4N7wTJ0401zz88MPm3B//8R/Hji9YsMBcM3PmTHPO51RM+l3y5cuXY8dde7PmXHtz/b9Ca51rD5cuXUpr/Eb3Z81Zx0eSPvvss7TGfbmO67hx42LH8/PzzTW9vb3mXEdHR+x4fX29uebw4cPm3NmzZ2PHJ0yYYK5ZunRp7PjcuXPNNdOmTTPnLK7n1vq3Izs7O3a8u7tbc+fOVUdHh1KplH2/6W3xxvr6+lRfX6+NGzcOjI0dO1Y1NTXav3//dT/f29s76ATo7u5Oekv4/1gvXNcL2jrBJGn8+PGx4wUFBeaawsJCc44C5F43XAtQf3+/ucaH67ha/1D6FiDr+FnnviTl5uaaczk5OWmNux7L9X8cXa8zi+tcsf4dcP37IN349Zl4COH06dO6dOmSSktLB42XlpaqtbX1up+vra1VKpUauFVUVCS9JQBABgqegtu4caM6OzsHbk1NTaG3BAAYAol/BDd58mSNGzdObW1tg8bb2tpUVlZ23c/n5uY637ICAEamxAtQTk6OqqqqtG/fPj3xxBOSrnxuvG/fPq1duzbphxvg+qzxNuQsMnYPLtYeXJ/9uv6AbK1L+nd13Z/P34CSvj+fv625ZML5anH97cqas4IBkvtvShbX/Vlzrn339PSYc42NjbHjDQ0N5pqjR4+acyUlJWmNS1f+T30c19+AXK9p6+9keXl5aa+xjqv189f93E39VJo2bNig1atX6wtf+IIWL16sV155RT09PfrqV796Ox4OADAM3ZYC9OSTT+rUqVN68cUX1draqvvuu0+vv/76dcEEAMDodds6Iaxdu/a2fuQGABjegqfgAACjEwUIABDEbfsIDpnH59vtrjnr/ny+YX+jx/K5vyTXuPik4DIh6ZZ0GtBKoPn+rj4dJiyubgdWexxJ5vcSP/30U3PNqVOnzDmrQ4GrE4LVbcCV7HO9lqznybUH67Gs58KVVBx0vzf1UwAAJIwCBAAIggIEAAiCAgQACIICBAAIggIEAAhixMSwM7nZZ6ZIunFn0nxi3c7rzSe4B5ekL4qX5B58o7rpPo7LzUZyr2U11HTtwWqe62o4GnedsqusGLZ1pVTXHiT74nyumLi1d9caV6NS6/lwNRBN93m/2dcl74AAAEFQgAAAQVCAAABBUIAAAEFQgAAAQYyYFBxuj6QTXi5JNjf1bcI5VM1Nk2686pOC832sdPeQNNexu3jxYuz46dOnzTWuFFxLS0vs+Llz58w1LlZy7fz58+YaK1WX9HmcZPPcm/153gEBAIKgAAEAgqAAAQCCoAABAIKgAAEAgqAAAQCCIIYdWJLRxxtJOpqZ7nXib8Qnhp3ufd3K3FCtsWLGrjVWg0lXDDsTYtO+c5YLFy7Ejre3t5trrKi1JJ06dSp23Ip7S+7mq1bcuru721xjPU++TV6t88h1vNN9bVqNZK/FOyAAQBAUIABAEBQgAEAQFCAAQBAUIABAEBQgAEAQxLBHESuSm5OTY67Jzs4256wYaCZ00Pbdg7Uu6ai1K6bqE8O2uI6DT8zZdX8+Hbl9or+uTtRW3PrkyZPmmra2NnPOike7Ytiu38l6rU2cONFcM2HChNjxvLw8c43P+e9zvhLDBgAMSxQgAEAQFCAAQBAUIABAEBQgAEAQIyYFN5RNPZM0lHuz0kjjx4831+Tn55tzPg0wffikc1yNGl1zPg1RfZJzrqTUZ599lvb9+ezbtQfrd8rKsv/J8Fnj2l9fX1/s+OnTp801H3/8cVrjkt1w1LWH3t5ec40rIVdSUhI7XllZaa6ZMmVK7LiVjpP8mtCSggMAjBoUIABAEBQgAEAQFCAAQBAUIABAEBQgAEAQIyaGjRuz4seuqLVrzorXDmUzUp+Gmkk/jhV59Wm06cungalPDNvFJ5bviuv29PTEjruahzY1NcWOt7a2mmvOnDljznV1dcWOW1F5yf2amTZtWuz4rFmzzDWpVCp23NVE2HVck3x9+sT/P493QACAIChAAIAgKEAAgCAoQACAIChAAIAgRkwKLulUUdKXtE2aT0NBK43kurSvay7pS3InecxvthnitXzSZK7mphbX72o9Tz6X13Yl3XyOkc85npuba865Gnf+9re/jR1vaWkx1zQ3N6d1XzeasxqfuhqBTp482ZybPXt2WuOSfblu13k3VJdbv1W8AwIABEEBAgAEQQECAARBAQIABEEBAgAEQQECAASRdgz7rbfe0ksvvaT6+nq1tLRoz549euKJJwbmoyjSpk2b9KMf/UgdHR168MEHtXXrVs2ZMyfJfd92rrjpUDbbTJLVPLSwsNBcYzVClOwoqqtJYtJ8YsGuNdZc0o07hyom62oE6tMs1XXsrAadrjVnz541506ePBk77mos2t7eHjtuxaklqbOz05w7f/587Hh5ebm5Zv78+ebcjBkzYsenTJlirvF5PbmeW5+vGljnlxUFv9mvJqT9Dqinp0eLFi3Sli1bYue/853v6NVXX9UPfvADHTx4UBMmTNDy5cudeX8AwOiT9jugFStWaMWKFbFzURTplVde0d/8zd/o8ccflyT98z//s0pLS/WTn/xEf/RHf3RruwUAjBiJ/g3oxIkTam1tVU1NzcBYKpXSkiVLtH///tg1vb296urqGnQDAIx8iRagq5/NlpaWDhovLS01P7etra1VKpUauFVUVCS5JQBAhgqegtu4caM6OzsHbtYVDQEAI0uiBaisrEzS9ZfMbWtrG5i7Vm5urgoLCwfdAAAjX6LdsGfNmqWysjLt27dP9913n6Qr11Q/ePCgnnnmmSQfKqgku167orU+cWGX7Ozs2PHi4mJzjSseOmnSpNhxVwdtF5+OztZx8D2uFp/7cz2OqxO1T6zbp4O2i7U/V6y7v78/dvzcuXPmGqt7tXTlb8pxXDFsq7P1qVOnzDWuvztbx2H69OnmmgceeMCcs/7EMH78eHONdcyt2PuN5qz7S7Lju/WVj+t+Lq171ZWT6fjx4wP/feLECR05ckTFxcWqrKzUc889p7/927/VnDlzNGvWLH3jG99QeXn5oO8KAQCQdgE6fPiwfv/3f3/gvzds2CBJWr16tXbs2KG/+qu/Uk9Pj772ta+po6NDDz30kF5//XXv/1cMABiZ0i5AX/rSl274jdlvf/vb+va3v31LGwMAjGzBU3AAgNGJAgQACCLRFBzCc6VPrOah06ZNM9dMnTrVnLMalfo2I02yEagr/eVKcll8Eom+KUafNKD1vLvOB1eyyTp+rvSelXbzSbq55j799FNzzbVfAbmqo6PDXGOlQyX7/J87d665ZsGCBeaclSp1vWZ8moe6+LzOrHPFei3d7GuMd0AAgCAoQACAIChAAIAgKEAAgCAoQACAIChAAIAgRnUMO+mGlT58H8fauytSasWmZ86caa5xXZ+poKAgdtwVKU06zmxxPbc32yjxZvfgiq/6sKLOPo/jisO6Ytg+DTDPnDkTO3706FFzzW9+8xtzzophf/LJJ+YaK4Z94cIFc82sWbPMuaqqqtjxRYsWmWvuvPNOcy43Nzd23PU8uaLvSbpRh5s4xLABAMMSBQgAEAQFCAAQBAUIABAEBQgAEMSoTsENZ1bKpLCw0FxTWloaO15WVmauKSoqMuesNNlQXg7bpxlp0pK8hLbr/lwpOJ+Gla653t7e2HHXpa0bGxtjxz9/BeVrffTRR+aclXaz0naS/TuVl5eba1zNQ63La7uSc1bTX8mvEajPa8an0azvuXIreAcEAAiCAgQACIICBAAIggIEAAiCAgQACIICBAAIYlTHsIeq4ejtYMV4S0pKzDVWFNW1Zvz48eacFR11Nax0seKhrkipTwNTV3NHn0i19Viu+3L9Ttb+fPbmG+/t7u6OHf/444/NNceOHYsdd0WtrYajkh3rdrHOZVfz0C9+8Yvm3OLFi2PHra80SO5j7tNoNunzK8kY9q02EOYdEAAgCAoQACAIChAAIAgKEAAgCAoQACAIChAAIIhRHcPOdK6YpU837OLi4thxV9Q66biwqwv0zV5H/ma4YqCuyKvP/flEUV178Imj+xy7jo4Oc+7TTz+NHf/Nb35jrrG6Xp88edJcc/bsWXPOOr+mTp1qrpk/f37s+NKlS801roi29Vh5eXnmGtfXEKzfyXWuWM+tT0d112MRwwYAjBoUIABAEBQgAEAQFCAAQBAUIABAEKTgEpRkI8sbsRJRruvRFxQUxI5nZfmdBlaixzedY/1OPgkvV8PR/v5+c856PnyaRfqkilxcz5M153qctrY2c85Kux09etRcYzUdbW1tNdf09fWZc9b5umDBAnPNgw8+GDv+0EMPmWtmzJhhzlkJUdf54ErBWXNJvy58mvGGaM7MOyAAQBAUIABAEBQgAEAQFCAAQBAUIABAEBQgAEAQxLCHSNIRR+v+srOzzTW5ublpjUvu6K8VA3VFSn3nLEnGpl1zPlFwnyisZO/d9Tv19vbGjp8/f95c09jYaM793//9X+x4c3OzucZqbuqKJd9xxx3mXHl5eez44sWLzTVWY9HS0lJzTX5+vjnn87r1iVT7NPD1eb241hHDBgCMGhQgAEAQFCAAQBAUIABAEBQgAEAQFCAAQBAjJobtG0m0+EQSk45s+sSFXfdnRbRzcnLMNa6ItrVuKJ8LK87sijm77s+KvFrxWdcaF1cXaGvOtcaKQJ86dcpcc+LECXPuk08+iR0/e/asucbqMu76asBdd91lzi1dujR23BXDvvPOO2PHXc/fxYsXzTnrawg+ncldfKLbrvPYFX1Pkk8n+M/jHRAAIAgKEAAgCAoQACAIChAAIAgKEAAgiLTiGrW1tfqP//gPffjhh8rPz9cXv/hF/f3f/73mzZs38DMXL17U888/r127dqm3t1fLly/Xa6+95mwGiHiudJWVtLGuYS9JBQUFseOupJvPded90ns3mrNYaTdX404XK43kOg7WGlcSz5Vo6+zsjB0/c+aMuaa1tTV2/OTJk2mvkexkmCtNVlJSEjs+Y8YMc83ChQvNufvuuy92vKKiwlwzceLE2PELFy6Ya6z0novrOLjmLD5Nen2bh/qc45YhTcHV1dVpzZo1OnDggN544w319/fry1/+snp6egZ+Zv369dq7d692796turo6NTc3a+XKlek8DABgFEjrHdDrr78+6L937NihO+64Q/X19frd3/1ddXZ2atu2bdq5c6eWLVsmSdq+fbvuvvtuHThwwMz1AwBGn1v6G9DVjwqKi4slSfX19erv71dNTc3Az8yfP1+VlZXav39/7H309vaqq6tr0A0AMPJ5F6DLly/rueee04MPPqh7771X0pXPlHNyclRUVDToZ0tLS83Pm2tra5VKpQZurs93AQAjh3cBWrNmjd577z3t2rXrljawceNGdXZ2Dtyamppu6f4AAMODVy+4tWvX6mc/+5neeustTZ8+fWC8rKxMfX196ujoGPQuqK2tTWVlZbH3lZub60xhAQBGprQKUBRFWrdunfbs2aM333xTs2bNGjRfVVWl7Oxs7du3T6tWrZIkNTQ0qLGxUdXV1cnt2thbknwaaibdwNQVi7QaPKZSKXPNtR+NXuVqRupiNTz0jVr7xECtuLXr+XM9jjXnWmPtwRW17u7uNufa2tpixz/++GNzjTXn+kTBtT8rSmzFnKX//2/B15ozZ465xopaS9LcuXNjx11fNbCeC1cs36epp6vBqk+kOukGvi4+e7COkU8z4M9LqwCtWbNGO3fu1E9/+lMVFBQM/F0nlUopPz9fqVRKTz/9tDZs2KDi4mIVFhZq3bp1qq6uJgEHABgkrQK0detWSdKXvvSlQePbt2/Xn/7pn0qSXn75ZY0dO1arVq0a9EVUAAA+L+2P4G4kLy9PW7Zs0ZYtW7w3BQAY+egFBwAIggIEAAhixFySO2mujxt9Eis+yRNXoq28vDx2vLKyMu01rmSTz+WFXYkjn0tlD2WqyGrC2dvba6757W9/Gzvuuhx2S0tL2nOuNdYluc+dO2eucaXJrESbdQ5J0tSpU2PHXZfdnjZtmjln7c+n2adPitE157NG8tu7j6RfM+k2/b3ZZsC8AwIABEEBAgAEQQECAARBAQIABEEBAgAEQQECAARBDNtDknFhVzx08uTJ5pzVqNEVebUi2oWFheYaV6NSq1GjK2p9s/HMz3NFV62YuE+kVLKbhLa3t5trGhoaYsePHTtmrnHNnTlzJnb8woUL5horsux6bvPy8sw5q3v9tQ2IP8+KVJeWlpprJkyYYM5Z54rrNWM9767moa7zwTrHXeextUay9+c6x33+vfFpuOu6P+v3tfZ2s42ZeQcEAAiCAgQACIICBAAIggIEAAiCAgQACIICBAAIghi2B59uslbH6UmTJplrFixYYM5VVVXFjru6FVuRalcMta+vz5yzopmuztFWt2nXPnwirz09Peaas2fPmnNWx+mTJ0+aa06cOBE73tjYmPbjSHZM1hXLnzJlSuy41aFacsejrfNoxowZ5hprf65u6664sMX1OrPizD5fkXDxiYIPpaT3kO7XSG72eeUdEAAgCAoQACAIChAAIAgKEAAgCAoQACAIUnAJcjUULCoqih2fM2eOueb+++8355YuXRo7bqWhJDtl5mpy6ZNSOn/+vDl37tw5c85KyLnWWIm2pqYmc83x48fNOSu55krBdXR0xI5bjU0lqb+/35yzmsa6EmizZ89Oe43rXLHmSkpKzDW5ubmx465ElqtppU/jTqs5rSvp6ZOQsx5H8nvNuJKePqlbn2Puei7SbWDqOj6D7vemfgoAgIRRgAAAQVCAAABBUIAAAEFQgAAAQVCAAABBEMMeIlZEtbi42Fzjaj5pxWFdcdPW1tbYcStGLLmbelqxaVes22fOtQdr783NzeaaTz75xJw7ffp0Wo8j2U1eCwoKzDWuOPO8efNix++55x5zTUVFRey4qzmt9dUAyd57Xl6eucaK5LrOSR8+zWlde3DFjy2umPNwbVTqE4lPt0npdT93Uz8FAEDCKEAAgCAoQACAIChAAIAgKEAAgCBIwQ0RqznfhAkTzDXjx48356zklety01ZDzQ8++MBc42rqeebMmdhx1yW5rZSSZF/+23UZb6vxaVdXl7nGdYxclyC3WM1D77rrLnPN7/zO75hzc+fOjR2/8847zTVWYtKVdHOdX9nZ2bHjrufPSpr5NgK1klSu+7PmXM1fXVyNTy0+zUiT5pNoc+073aTgzab9wh8pAMCoRAECAARBAQIABEEBAgAEQQECAARBAQIABEEMe4hY0WSr+aUkNTQ0mHNWQ0ZXxLilpSV2/OjRo+Yaq4GpJHV2dsaOu6K6rkaSVoTWdX8+8VpXQ82ysrLYcVdjWCs2bTUVlaS7777bnLMaiE6aNMlcY0WqXb+rK2JsxW5dcVzXc2txRX+try74NPR0/a4+UfBMiFonzacp660aeUcRADAsUIAAAEFQgAAAQVCAAABBUIAAAEFQgAAAQRDDTpArxtjd3R073tjYaK7p6ekx5z788MPYcas7tGRHtF0dr619S3Y82ieO6+IT/XVFlqdNm2bOWdFpV2zamnN1r7ai1pIdqU46Jut6nqzHckXirTWumLMrHm115PaJYfvGpodrDNt1jHzOI59jfjMy+ygCAEYsChAAIAgKEAAgCAoQACAIChAAIIi0UnBbt27V1q1b9fHHH0uS7rnnHr344otasWKFJOnixYt6/vnntWvXLvX29mr58uV67bXXVFpa6rW5uORFiIZ5N8uVKrLSae3t7eaarq4ucy4nJyd2vK+vz1xz4cKF2PGOjg5zjev+rOfCJ9kkSalUKna8oKDAXGPNTZ061Vwze/Zsc85qLDpnzhxzjZWqc533rt/JOn5Ws1bXnG9jWJ9Em09izHU+JJmCc52TLtZjZXoKbrhI6yhOnz5dmzdvVn19vQ4fPqxly5bp8ccf1/vvvy9JWr9+vfbu3avdu3errq5Ozc3NWrly5W3ZOABgeEvrHdBjjz026L//7u/+Tlu3btWBAwc0ffp0bdu2TTt37tSyZcskSdu3b9fdd9+tAwcOaOnSpcntGgAw7Hm/j7x06ZJ27dqlnp4eVVdXq76+Xv39/aqpqRn4mfnz56uyslL79+8376e3t1ddXV2DbgCAkS/tAvTuu+9q4sSJys3N1de//nXt2bNHCxYsUGtrq3JyclRUVDTo50tLS50XNautrVUqlRq4VVRUpP1LAACGn7QL0Lx583TkyBEdPHhQzzzzjFavXq0PPvjAewMbN25UZ2fnwM3VFgYAMHKk3QsuJydHd911lySpqqpKhw4d0ve+9z09+eST6uvrU0dHx6B3QW1tbeZljiUpNzdXubm56e8cADCs3XIz0suXL6u3t1dVVVXKzs7Wvn37tGrVKklSQ0ODGhsbVV1dnfb9jhkzJq245VDGs63Hcu3BikD39vaaa1xRT+vY+DTudMV7fY5rVpZ9Wk2YMMGcq6ysjB2fOXOmucb6yNa1ZtasWebc9OnTY8d9modaUXkp+SixTwNYn3PFtQfreXdFrV3nivVYt6sxZjoy+esgN5IJx++qtArQxo0btWLFClVWVqq7u1s7d+7Um2++qZ///OdKpVJ6+umntWHDBhUXF6uwsFDr1q1TdXU1CTgAwHXSKkDt7e36kz/5E7W0tCiVSmnhwoX6+c9/rj/4gz+QJL388ssaO3asVq1aNeiLqAAAXCutArRt2zbnfF5enrZs2aItW7bc0qYAACMf/SQAAEFQgAAAQXBJ7iFipYqSvny1i5VGuvbLw59nJbxcc67LYU+ZMsWcsy6H7WoeaqXWXI1AJ0+ebM75NET1SWv5pKhcqUhrzqd5qGudT6LNlXTz2YOP4ZxaG8l4BwQACIICBAAIggIEAAiCAgQACIICBAAIggIEAAhixMSwk26wNxJjm1aE1hWNdnUyv+OOO2LHrWi0ZDccla5cwDCOq7GotXdX01NXQ02f88g6V3wj9q5ossXat899ue7PJ4btOt4+e3AdV59GwT5c50kmNftMR9JfG7gZvAMCAARBAQIABEEBAgAEQQECAARBAQIABEEBAgAEkbEx7EyOQVvR1pycHHNNbm5u7HheXl7aa1yP5Vpjdb12dZuuqKgw58rLy2PHXdFtn1h3cXGxucbqyO3qwOzic95Za3yjutZc0mt8OlH7rPGN9/p0kPeJYY/ESLWPEP/m8g4IABAEBQgAEAQFCAAQBAUIABAEBQgAEETGpuCksEk4V/rFaq6Yn59vrrESaCUlJeaaVCqV9lxhYaG5prS0NHZ8zpw55hpXI1ArIWf9rpI0ceJEc86ncealS5dix3t7e801PqmnpNNfSTdE9bmvoToOLkOVgvOVCU2OR3ISj3dAAIAgKEAAgCAoQACAIChAAIAgKEAAgCAoQACAIDI6hh3Huia9q6nnhAkT0hqX3I1Frbj1pEmTzDVWBHrq1KnmmsmTJ5tzVoNOV8zZike7Hsf1O1lRcNdz4YqUfvbZZ7HjVtRasiO5rsdxNSq11iXZpPR28ImC+9yfD9dx8JnzWTOUjWFdRnKk2gfvgAAAQVCAAABBUIAAAEFQgAAAQVCAAABBDLsUnJVOczXutC4d7UqguRJy1mNZl5SWpOnTp8eOz5gxw1zjuny1lVxzJdCsBKFPQ0jXOldqzUq6SVJ/f3/ae7BSRdbvKvlfrttiHYehbJppIa11RdIpON/HCtlgORPxDggAEAQFCAAQBAUIABAEBQgAEAQFCAAQBAUIABBExsawp06dGnv9eSt+7Iozz5kzJ3b8zjvvNNf4NDe1mn1KdvPQkpISc42rsai1B1f8OO54Su7YtBWNds35xrot1r5dc641rpistW7cuHHmGp8GmEmz9p10JHikxbMlv/N1JB6HEHgHBAAIggIEAAiCAgQACIICBAAIggIEAAiCAgQACCJjY9iVlZWxXYsrKytjf37hwoXmfd13332x4/fee6+5xhW7taLOPp2oXY+TdDfloerE63oc15x1LFyRV2uN67i6WOt8Omj7HgdrzjdabvGJxA9l/Nh6LJ89+J77Ps+Fy1Dtfbh08eYdEAAgCAoQACAIChAAIAgKEAAgCAoQACCIW0rBbd68WRs3btSzzz6rV155RZJ08eJFPf/889q1a5d6e3u1fPlyvfbaayotLU3rvh999NHYVJnV1HP69Onmfc2cOTN2fNKkSeYaV+rDSsDk5uaaa3xSWZ999pk552ogarFSLq5Ej6u5qXV/rnRV0skrn2akSafJhuK+biTplNJQJa9cz4VPk1fr/BqqBOhwFuIYeb8DOnTokH74wx9eF39ev3699u7dq927d6uurk7Nzc1auXLlLW8UADCyeBWgc+fO6amnntKPfvSjQe8iOjs7tW3bNn33u9/VsmXLVFVVpe3bt+t//ud/dODAgcQ2DQAY/rwK0Jo1a/Too4+qpqZm0Hh9fb36+/sHjc+fP1+VlZXav39/7H319vaqq6tr0A0AMPKl/TegXbt26e2339ahQ4eum2ttbVVOTs51F2YrLS1Va2tr7P3V1tbqW9/6VrrbAAAMc2m9A2pqatKzzz6rf/mXf3G2nUnHxo0b1dnZOXBrampK5H4BAJktrQJUX1+v9vZ23X///crKylJWVpbq6ur06quvKisrS6Wlperr61NHR8egdW1tbSorK4u9z9zcXBUWFg66AQBGvrQ+gnvkkUf07rvvDhr76le/qvnz5+uv//qvVVFRoezsbO3bt0+rVq2SJDU0NKixsVHV1dVpbWzlypUqKCi4btyKM7viwuPHj48dz8nJMde4ItCZHOn0icK6IuKuOatBp28MO8nor+s4+MS6ffbguybJRrM+sXcpM+Lo1jrX8fH5asBQvp59ouVDGecfamkVoIKCgus6SE+YMEElJSUD408//bQ2bNig4uJiFRYWat26daqurtbSpUuT2zUAYNhL/HIML7/8ssaOHatVq1YN+iIqAACfNybKsM+Turq6lEql9N577932j+BcnQt8PoLLhE4Irsfx6YTg4vOxz3D9CG4oPyLxOQ7WcU36Izif4+B7fll8rqU0VOfdjeaSXJPJurq6NGnSJHV2djr/rk8vOABAEBQgAEAQGXtJ7pKSkrQi2T6XbXZ9NODzkVnSyaZM/6jI53F8Po7xST0N50agPpI+Dkl+BJfJxydT7s/nHE9aiD3wDggAEAQFCAAQBAUIABAEBQgAEAQFCAAQBAUIABBExsaw8/PzlZ+fn8h9WfFCV+zQarTp4hPD9o2o+sTELb578FnnimH7NGq0DGX82PqWve9xTTJSnXQXAp89ZHoMe7RF9jNhD1fxDggAEAQFCAAQBAUIABAEBQgAEAQFCAAQBAUIABBExsawM1kmdJXOBJkcyU26s2/S+86EPfjIhD0MZ5nwms6kruW8AwIABEEBAgAEQQECAARBAQIABEEBAgAEkbEpuCiKYlMZSaZIfO8rybRIJqRifCWdpsmEZNhQPbdD9bv6NMhNWibsYSgl/ZrOhOfpduEdEAAgCAoQACAIChAAIAgKEAAgCAoQACAIChAAIIiMjWEj82VChHY0NYZNeg8j8XcaiTL5GN3qvwG8AwIABEEBAgAEQQECAARBAQIABEEBAgAEQQECAASRsTFsqxs2hrdMiJT67CET9g0MFzf7euEdEAAgCAoQACAIChAAIAgKEAAgCAoQACCIjE3BjRkzhuRRwjieN8YxAm6e9XohBQcAyGgUIABAEBQgAEAQFCAAQBAUIABAEBQgAEAQGRvDhluScWGixwBC4B0QACAIChAAIAgKEAAgCAoQACAIChAAIIi0CtA3v/nNgSahV2/z588fmL948aLWrFmjkpISTZw4UatWrVJbW1vimx4trj3Wn78leX8AEELa74DuuecetbS0DNx+9atfDcytX79ee/fu1e7du1VXV6fm5matXLky0Q0DAEaGtL8HlJWVpbKysuvGOzs7tW3bNu3cuVPLli2TJG3fvl133323Dhw4oKVLl976bgEAI0ba74COHTum8vJyzZ49W0899ZQaGxslSfX19erv71dNTc3Az86fP1+VlZXav3+/eX+9vb3q6uoadAMAjHxpFaAlS5Zox44dev3117V161adOHFCDz/8sLq7u9Xa2qqcnBwVFRUNWlNaWqrW1lbzPmtra5VKpQZuFRUVXr8IAGB4SesjuBUrVgz874ULF2rJkiWaMWOGfvzjHys/P99rAxs3btSGDRsG/rurq4siBACjwC3FsIuKijR37lwdP35cZWVl6uvrU0dHx6CfaWtri/2b0VW5ubkqLCwcdAMAjHy3VIDOnTunjz76SFOnTlVVVZWys7O1b9++gfmGhgY1Njaqurr6ljd6lSuaPNJuSR8HAMgkaX0E95d/+Zd67LHHNGPGDDU3N2vTpk0aN26cvvKVryiVSunpp5/Whg0bVFxcrMLCQq1bt07V1dUk4AAA10mrAH366af6yle+ojNnzmjKlCl66KGHdODAAU2ZMkWS9PLLL2vs2LFatWqVent7tXz5cr322mu3ZeMAgOFtTBRFUehNfF5XV5dSqZTOnj0b+/cgPkq6guMAIFNd/Xe8s7PT+Xd9esEBAIKgAAEAgqAAAQCCSLsX3FAhOszfeQCMbLwDAgAEQQECAARBAQIABEEBAgAEQQECAARBCg4AEATvgAAAQVCAAABBUIAAAEFQgAAAQVCAAABBUIAAAEFQgAAAQVCAAABBUIAAAEFQgAAAQVCAAABBUIAAAEFQgAAAQVCAAABBUIAAAEFQgAAAQVCAAABBUIAAAEFk3CW5oyiSJHV1dQXeCQDAx9V/v6/+e27JuALU3d0tSaqoqAi8EwDAreju7lYqlTLnx0Q3KlFD7PLly2publZBQYHGjBmjrq4uVVRUqKmpSYWFhaG3FwzH4QqOwxUchys4Dldk2nGIokjd3d0qLy/X2LH2X3oy7h3Q2LFjNX369OvGCwsLM+LAhsZxuILjcAXH4QqOwxWZdBxc73yuIoQAAAiCAgQACCLjC1Bubq42bdqk3Nzc0FsJiuNwBcfhCo7DFRyHK4brcci4EAIAYHTI+HdAAICRiQIEAAiCAgQACIICBAAIggIEAAgiowvQli1bNHPmTOXl5WnJkiX63//939Bbuq3eeustPfbYYyovL9eYMWP0k5/8ZNB8FEV68cUXNXXqVOXn56umpkbHjh0Ls9nbqLa2Vg888IAKCgp0xx136IknnlBDQ8Ogn7l48aLWrFmjkpISTZw4UatWrVJbW1ugHd8eW7du1cKFCwe+3V5dXa3//M//HJgfDccgzubNmzVmzBg999xzA2Oj4Vh885vf1JgxYwbd5s+fPzA/HI9Bxhagf/u3f9OGDRu0adMmvf3221q0aJGWL1+u9vb20Fu7bXp6erRo0SJt2bIldv473/mOXn31Vf3gBz/QwYMHNWHCBC1fvlwXL14c4p3eXnV1dVqzZo0OHDigN954Q/39/fryl7+snp6egZ9Zv3699u7dq927d6uurk7Nzc1auXJlwF0nb/r06dq8ebPq6+t1+PBhLVu2TI8//rjef/99SaPjGFzr0KFD+uEPf6iFCxcOGh8tx+Kee+5RS0vLwO1Xv/rVwNywPAZRhlq8eHG0Zs2agf++dOlSVF5eHtXW1gbc1dCRFO3Zs2fgvy9fvhyVlZVFL7300sBYR0dHlJubG/3rv/5rgB0Onfb29khSVFdXF0XRld87Ozs72r1798DPHD16NJIU7d+/P9Q2h8SkSZOif/zHfxyVx6C7uzuaM2dO9MYbb0S/93u/Fz377LNRFI2e82HTpk3RokWLYueG6zHIyHdAfX19qq+vV01NzcDY2LFjVVNTo/379wfcWTgnTpxQa2vroGOSSqW0ZMmSEX9MOjs7JUnFxcWSpPr6evX39w86FvPnz1dlZeWIPRaXLl3Srl271NPTo+rq6lF5DNasWaNHH3100O8sja7z4dixYyovL9fs2bP11FNPqbGxUdLwPQYZ1w1bkk6fPq1Lly6ptLR00Hhpaak+/PDDQLsKq7W1VZJij8nVuZHo8uXLeu655/Tggw/q3nvvlXTlWOTk5KioqGjQz47EY/Huu++qurpaFy9e1MSJE7Vnzx4tWLBAR44cGTXHQJJ27dqlt99+W4cOHbpubrScD0uWLNGOHTs0b948tbS06Fvf+pYefvhhvffee8P2GGRkAQKuWrNmjd57771Bn3WPJvPmzdORI0fU2dmpf//3f9fq1atVV1cXeltDqqmpSc8++6zeeOMN5eXlhd5OMCtWrBj43wsXLtSSJUs0Y8YM/fjHP1Z+fn7AnfnLyI/gJk+erHHjxl2X4Ghra1NZWVmgXYV19fceTcdk7dq1+tnPfqZf/vKXg64RVVZWpr6+PnV0dAz6+ZF4LHJycnTXXXepqqpKtbW1WrRokb73ve+NqmNQX1+v9vZ23X///crKylJWVpbq6ur06quvKisrS6WlpaPmWHxeUVGR5s6dq+PHjw/b8yEjC1BOTo6qqqq0b9++gbHLly9r3759qq6uDrizcGbNmqWysrJBx6Srq0sHDx4cccckiiKtXbtWe/bs0S9+8QvNmjVr0HxVVZWys7MHHYuGhgY1NjaOuGNxrcuXL6u3t3dUHYNHHnlE7777ro4cOTJw+8IXvqCnnnpq4H+PlmPxeefOndNHH32kqVOnDt/zIXQKwrJr164oNzc32rFjR/TBBx9EX/va16KioqKotbU19NZum+7u7uidd96J3nnnnUhS9N3vfjd65513ok8++SSKoijavHlzVFRUFP30pz+Nfv3rX0ePP/54NGvWrOjChQuBd56sZ555JkqlUtGbb74ZtbS0DNzOnz8/8DNf//rXo8rKyugXv/hFdPjw4ai6ujqqrq4OuOvkvfDCC1FdXV104sSJ6Ne//nX0wgsvRGPGjIn+67/+K4qi0XEMLJ9PwUXR6DgWzz//fPTmm29GJ06ciP77v/87qqmpiSZPnhy1t7dHUTQ8j0HGFqAoiqLvf//7UWVlZZSTkxMtXrw4OnDgQOgt3Va//OUvI0nX3VavXh1F0ZUo9je+8Y2otLQ0ys3NjR555JGooaEh7KZvg7hjICnavn37wM9cuHAh+ou/+Ito0qRJ0fjx46M//MM/jFpaWsJt+jb4sz/7s2jGjBlRTk5ONGXKlOiRRx4ZKD5RNDqOgeXaAjQajsWTTz4ZTZ06NcrJyYmmTZsWPfnkk9Hx48cH5ofjMeB6QACAIDLyb0AAgJGPAgQACIICBAAIggIEAAiCAgQACIICBAAIggIEAAiCAgQACIICBAAIggIEAAiCAgQACOL/AUZdZo4JKC58AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_tiles[169].reshape(56,56,3).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "dbacdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = torch.ones(56,56,3)\n",
    "for o in out.reshape(3,2):\n",
    "    x_min = int(o[0] * 56)\n",
    "    y_min = int(o[1] * 56)\n",
    "    x_max = x_min+14\n",
    "    y_max = y_min+14\n",
    "    base[x_min:x_max, y_min:y_max] = test_tiles[169][x_min:x_max, y_min:y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "eaa50c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.patches as patches\n",
    "plt.imshow(base.reshape(56,56,3).detach().cpu().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6185691356658936\n",
      "Test Loss: 0.013077893294394016\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 4.466363310813904\n",
      "Test Loss: 0.08383168280124664\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.9857504367828369\n",
      "Test Loss: 0.1306329220533371\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.4335505776107311\n",
      "Test Loss: 0.11838601529598236\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.1894276663661003\n",
      "Test Loss: 0.17145520448684692\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.11076735612004995\n",
      "Test Loss: 0.20163533091545105\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.06546270055696368\n",
      "Test Loss: 0.23045168817043304\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.03978912718594074\n",
      "Test Loss: 0.25447165966033936\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.030275192810222507\n",
      "Test Loss: 0.25443536043167114\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.024784686043858528\n",
      "Test Loss: 0.2580438256263733\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.02040827041491866\n",
      "Test Loss: 0.25746673345565796\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.023005058988928795\n",
      "Test Loss: 0.26734060049057007\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.017401195713318884\n",
      "Test Loss: 0.2632371783256531\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.016578912502154708\n",
      "Test Loss: 0.2604631185531616\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.015443754266016185\n",
      "Test Loss: 0.25787028670310974\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.013517873478122056\n",
      "Test Loss: 0.2509487569332123\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.012707430869340897\n",
      "Test Loss: 0.2664503753185272\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.010244129807688296\n",
      "Test Loss: 0.264952152967453\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.01376846560742706\n",
      "Test Loss: 0.2591142952442169\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.014018056215718389\n",
      "Test Loss: 0.2530756890773773\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.014790704473853111\n",
      "Test Loss: 0.24640822410583496\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.015644261497072875\n",
      "Test Loss: 0.2564164400100708\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.011659952229820192\n",
      "Test Loss: 0.2638177275657654\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.013651449466124177\n",
      "Test Loss: 0.25329136848449707\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.011883809580467641\n",
      "Test Loss: 0.2565382421016693\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.009422098228242248\n",
      "Test Loss: 0.2624457776546478\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.009408520651049912\n",
      "Test Loss: 0.2642855942249298\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.009222712134942412\n",
      "Test Loss: 0.25244584679603577\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.008430076180957258\n",
      "Test Loss: 0.2540249228477478\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.010139122372493148\n",
      "Test Loss: 0.26229336857795715\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.009419935755431652\n",
      "Test Loss: 0.2636673152446747\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.00809960108017549\n",
      "Test Loss: 0.25353023409843445\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.009064150508493185\n",
      "Test Loss: 0.25543496012687683\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.010349620482884347\n",
      "Test Loss: 0.2495541274547577\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.008806869736872613\n",
      "Test Loss: 0.2494097203016281\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.011779112741351128\n",
      "Test Loss: 0.26040464639663696\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.012417705729603767\n",
      "Test Loss: 0.2539421021938324\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.011891240486875176\n",
      "Test Loss: 0.256619930267334\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.010040451656095684\n",
      "Test Loss: 0.25648000836372375\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.010442016995511949\n",
      "Test Loss: 0.242721825838089\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.009263826708775014\n",
      "Test Loss: 0.2560822069644928\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.010113262571394444\n",
      "Test Loss: 0.25213539600372314\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.00838752428535372\n",
      "Test Loss: 0.25727030634880066\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.008899270032998174\n",
      "Test Loss: 0.25322720408439636\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.009923417121171951\n",
      "Test Loss: 0.25688278675079346\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.010956532671116292\n",
      "Test Loss: 0.25237274169921875\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.01004152896348387\n",
      "Test Loss: 0.26082274317741394\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.008749631466343999\n",
      "Test Loss: 0.26311612129211426\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.008452643523924053\n",
      "Test Loss: 0.2584606111049652\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.009560647769831121\n",
      "Test Loss: 0.2674320936203003\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.008458346943370998\n",
      "Test Loss: 0.25485238432884216\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.007539874583017081\n",
      "Test Loss: 0.25388818979263306\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.009844215237535536\n",
      "Test Loss: 0.2627265155315399\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.00863578071584925\n",
      "Test Loss: 0.2589925229549408\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.011617343639954925\n",
      "Test Loss: 0.2546611428260803\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.008776089351158589\n",
      "Test Loss: 0.2640848457813263\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.007850251975469291\n",
      "Test Loss: 0.25793296098709106\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.007010665431153029\n",
      "Test Loss: 0.25082191824913025\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.0071445347857661545\n",
      "Test Loss: 0.25478968024253845\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.009321293968241662\n",
      "Test Loss: 0.25722572207450867\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.009068682440556586\n",
      "Test Loss: 0.24556753039360046\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.007098610629327595\n",
      "Test Loss: 0.2569270133972168\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.009093654400203377\n",
      "Test Loss: 0.2521864175796509\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.009048448700923473\n",
      "Test Loss: 0.24977341294288635\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.007840666745323688\n",
      "Test Loss: 0.2611459195613861\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.0069102622219361365\n",
      "Test Loss: 0.2606334388256073\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.007082512776833028\n",
      "Test Loss: 0.2579692304134369\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.006933493714313954\n",
      "Test Loss: 0.26131173968315125\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.006016913859639317\n",
      "Test Loss: 0.2547260820865631\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.00834111764561385\n",
      "Test Loss: 0.26214486360549927\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.0080983770894818\n",
      "Test Loss: 0.2482110857963562\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.009450481971725821\n",
      "Test Loss: 0.2554300129413605\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.008239179383963346\n",
      "Test Loss: 0.2593255341053009\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.006627022929023951\n",
      "Test Loss: 0.24651959538459778\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.006590350763872266\n",
      "Test Loss: 0.25535449385643005\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.0059867805684916675\n",
      "Test Loss: 0.25477898120880127\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.006238856236450374\n",
      "Test Loss: 0.25087282061576843\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.006214148772414774\n",
      "Test Loss: 0.2536798417568207\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.006272053869906813\n",
      "Test Loss: 0.25763827562332153\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.006521261704619974\n",
      "Test Loss: 0.25514259934425354\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.007310443790629506\n",
      "Test Loss: 0.25005465745925903\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.007280357473064214\n",
      "Test Loss: 0.25859764218330383\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.006618111918214709\n",
      "Test Loss: 0.25533491373062134\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.009322126628831029\n",
      "Test Loss: 0.25129351019859314\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.006789031031075865\n",
      "Test Loss: 0.252718061208725\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.007515294419135898\n",
      "Test Loss: 0.25315287709236145\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.005690881342161447\n",
      "Test Loss: 0.25622111558914185\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.0057818416389636695\n",
      "Test Loss: 0.2512740194797516\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.005785306333564222\n",
      "Test Loss: 0.2692524492740631\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.0056966987904161215\n",
      "Test Loss: 0.2539668679237366\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.00559129478642717\n",
      "Test Loss: 0.2577890455722809\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.005265555460937321\n",
      "Test Loss: 0.24837446212768555\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.005421906360425055\n",
      "Test Loss: 0.24961645901203156\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.004974423558451235\n",
      "Test Loss: 0.25063982605934143\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.005654076172504574\n",
      "Test Loss: 0.2524847090244293\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.004472580447327346\n",
      "Test Loss: 0.25676077604293823\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.004803334129974246\n",
      "Test Loss: 0.25619637966156006\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.0047206374001689255\n",
      "Test Loss: 0.25497129559516907\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.007082578958943486\n",
      "Test Loss: 0.2501315474510193\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.006156212359201163\n",
      "Test Loss: 0.2596830725669861\n",
      "\n",
      "Epoch: 100\n",
      "Train Loss: 0.006449726119171828\n",
      "Test Loss: 0.261655330657959\n",
      "\n",
      "Epoch: 101\n",
      "Train Loss: 0.005414095532614738\n",
      "Test Loss: 0.2543416917324066\n",
      "\n",
      "Epoch: 102\n",
      "Train Loss: 0.006668562185950577\n",
      "Test Loss: 0.2578762173652649\n",
      "\n",
      "Epoch: 103\n",
      "Train Loss: 0.007771709410008043\n",
      "Test Loss: 0.25327757000923157\n",
      "\n",
      "Epoch: 104\n",
      "Train Loss: 0.006176411698106676\n",
      "Test Loss: 0.24498350918293\n",
      "\n",
      "Epoch: 105\n",
      "Train Loss: 0.006727155472617596\n",
      "Test Loss: 0.25503045320510864\n",
      "\n",
      "Epoch: 106\n",
      "Train Loss: 0.007043016550596803\n",
      "Test Loss: 0.25523459911346436\n",
      "\n",
      "Epoch: 107\n",
      "Train Loss: 0.007384247204754502\n",
      "Test Loss: 0.24202056229114532\n",
      "\n",
      "Epoch: 108\n",
      "Train Loss: 0.006946515990421176\n",
      "Test Loss: 0.2572095990180969\n",
      "\n",
      "Epoch: 109\n",
      "Train Loss: 0.006510727573186159\n",
      "Test Loss: 0.2630656659603119\n",
      "\n",
      "Epoch: 110\n",
      "Train Loss: 0.006130154535640031\n",
      "Test Loss: 0.2558428645133972\n",
      "\n",
      "Epoch: 111\n",
      "Train Loss: 0.007991276565007865\n",
      "Test Loss: 0.25433892011642456\n",
      "\n",
      "Epoch: 112\n",
      "Train Loss: 0.006690210197120905\n",
      "Test Loss: 0.2616724669933319\n",
      "\n",
      "Epoch: 113\n",
      "Train Loss: 0.0068044509389437735\n",
      "Test Loss: 0.25195401906967163\n",
      "\n",
      "Epoch: 114\n",
      "Train Loss: 0.006608703930396587\n",
      "Test Loss: 0.25273433327674866\n",
      "\n",
      "Epoch: 115\n",
      "Train Loss: 0.005942277843132615\n",
      "Test Loss: 0.25563210248947144\n",
      "\n",
      "Epoch: 116\n",
      "Train Loss: 0.006976240081712604\n",
      "Test Loss: 0.2728048264980316\n",
      "\n",
      "Epoch: 117\n",
      "Train Loss: 0.006239530455786735\n",
      "Test Loss: 0.25984886288642883\n",
      "\n",
      "Epoch: 118\n",
      "Train Loss: 0.00858033902477473\n",
      "Test Loss: 0.258298397064209\n",
      "\n",
      "Epoch: 119\n",
      "Train Loss: 0.007092522049788386\n",
      "Test Loss: 0.2526280879974365\n",
      "\n",
      "Epoch: 120\n",
      "Train Loss: 0.006342911859974265\n",
      "Test Loss: 0.2507616877555847\n",
      "\n",
      "Epoch: 121\n",
      "Train Loss: 0.006339833373203874\n",
      "Test Loss: 0.25466012954711914\n",
      "\n",
      "Epoch: 122\n",
      "Train Loss: 0.00579629885032773\n",
      "Test Loss: 0.26500287652015686\n",
      "\n",
      "Epoch: 123\n",
      "Train Loss: 0.008132131886668503\n",
      "Test Loss: 0.2539193034172058\n",
      "\n",
      "Epoch: 124\n",
      "Train Loss: 0.006211173604242504\n",
      "Test Loss: 0.24840612709522247\n",
      "\n",
      "Epoch: 125\n",
      "Train Loss: 0.007028693973552436\n",
      "Test Loss: 0.26567938923835754\n",
      "\n",
      "Epoch: 126\n",
      "Train Loss: 0.005675203807186335\n",
      "Test Loss: 0.2597346603870392\n",
      "\n",
      "Epoch: 127\n",
      "Train Loss: 0.005573511065449566\n",
      "Test Loss: 0.26221171021461487\n",
      "\n",
      "Epoch: 128\n",
      "Train Loss: 0.005138027248904109\n",
      "Test Loss: 0.25513002276420593\n",
      "\n",
      "Epoch: 129\n",
      "Train Loss: 0.005691752827260643\n",
      "Test Loss: 0.2569164037704468\n",
      "\n",
      "Epoch: 130\n",
      "Train Loss: 0.004847769334446639\n",
      "Test Loss: 0.2697068154811859\n",
      "\n",
      "Epoch: 131\n",
      "Train Loss: 0.00557390577159822\n",
      "Test Loss: 0.262681245803833\n",
      "\n",
      "Epoch: 132\n",
      "Train Loss: 0.006338816892821342\n",
      "Test Loss: 0.25787830352783203\n",
      "\n",
      "Epoch: 133\n",
      "Train Loss: 0.0059910526615567505\n",
      "Test Loss: 0.25849589705467224\n",
      "\n",
      "Epoch: 134\n",
      "Train Loss: 0.004959711688570678\n",
      "Test Loss: 0.2568293511867523\n",
      "\n",
      "Epoch: 135\n",
      "Train Loss: 0.004637342761270702\n",
      "Test Loss: 0.2584306001663208\n",
      "\n",
      "Epoch: 136\n",
      "Train Loss: 0.006339925923384726\n",
      "Test Loss: 0.2631992995738983\n",
      "\n",
      "Epoch: 137\n",
      "Train Loss: 0.0057650088565424085\n",
      "Test Loss: 0.26467105746269226\n",
      "\n",
      "Epoch: 138\n",
      "Train Loss: 0.005317436822224408\n",
      "Test Loss: 0.26255273818969727\n",
      "\n",
      "Epoch: 139\n",
      "Train Loss: 0.0050221397541463375\n",
      "Test Loss: 0.25736936926841736\n",
      "\n",
      "Epoch: 140\n",
      "Train Loss: 0.005274777067825198\n",
      "Test Loss: 0.2585274279117584\n",
      "\n",
      "Epoch: 141\n",
      "Train Loss: 0.005838293000124395\n",
      "Test Loss: 0.24193960428237915\n",
      "\n",
      "Epoch: 142\n",
      "Train Loss: 0.005373886553570628\n",
      "Test Loss: 0.2590087950229645\n",
      "\n",
      "Epoch: 143\n",
      "Train Loss: 0.004860788700170815\n",
      "Test Loss: 0.26747769117355347\n",
      "\n",
      "Epoch: 144\n",
      "Train Loss: 0.004916995181702077\n",
      "Test Loss: 0.2617873549461365\n",
      "\n",
      "Epoch: 145\n",
      "Train Loss: 0.0053630112670362\n",
      "Test Loss: 0.26419079303741455\n",
      "\n",
      "Epoch: 146\n",
      "Train Loss: 0.005474035628139973\n",
      "Test Loss: 0.2553528845310211\n",
      "\n",
      "Epoch: 147\n",
      "Train Loss: 0.0049192041042260826\n",
      "Test Loss: 0.2548011541366577\n",
      "\n",
      "Epoch: 148\n",
      "Train Loss: 0.006069914379622787\n",
      "Test Loss: 0.26069456338882446\n",
      "\n",
      "Epoch: 149\n",
      "Train Loss: 0.005423541238997132\n",
      "Test Loss: 0.2589174807071686\n",
      "\n",
      "Epoch: 150\n",
      "Train Loss: 0.006730892113409936\n",
      "Test Loss: 0.2545209527015686\n",
      "\n",
      "Epoch: 151\n",
      "Train Loss: 0.005594007729087025\n",
      "Test Loss: 0.25450605154037476\n",
      "\n",
      "Epoch: 152\n",
      "Train Loss: 0.007358807779382914\n",
      "Test Loss: 0.2617764174938202\n",
      "\n",
      "Epoch: 153\n",
      "Train Loss: 0.00628897687420249\n",
      "Test Loss: 0.25926849246025085\n",
      "\n",
      "Epoch: 154\n",
      "Train Loss: 0.005601209180895239\n",
      "Test Loss: 0.2629666030406952\n",
      "\n",
      "Epoch: 155\n",
      "Train Loss: 0.005449948366731405\n",
      "Test Loss: 0.26525041460990906\n",
      "\n",
      "Epoch: 156\n",
      "Train Loss: 0.004915219178656116\n",
      "Test Loss: 0.2587552070617676\n",
      "\n",
      "Epoch: 157\n",
      "Train Loss: 0.005838905286509544\n",
      "Test Loss: 0.2556140720844269\n",
      "\n",
      "Epoch: 158\n",
      "Train Loss: 0.005855640978552401\n",
      "Test Loss: 0.2610490024089813\n",
      "\n",
      "Epoch: 159\n",
      "Train Loss: 0.005665889009833336\n",
      "Test Loss: 0.25063517689704895\n",
      "\n",
      "Epoch: 160\n",
      "Train Loss: 0.005601711454801261\n",
      "Test Loss: 0.2528243362903595\n",
      "\n",
      "Epoch: 161\n",
      "Train Loss: 0.004006581933936104\n",
      "Test Loss: 0.2608585059642792\n",
      "\n",
      "Epoch: 162\n",
      "Train Loss: 0.004218555433908477\n",
      "Test Loss: 0.2657274007797241\n",
      "\n",
      "Epoch: 163\n",
      "Train Loss: 0.005135759420227259\n",
      "Test Loss: 0.25197747349739075\n",
      "\n",
      "Epoch: 164\n",
      "Train Loss: 0.005156963248737156\n",
      "Test Loss: 0.260580450296402\n",
      "\n",
      "Epoch: 165\n",
      "Train Loss: 0.004511915787588805\n",
      "Test Loss: 0.2565947473049164\n",
      "\n",
      "Epoch: 166\n",
      "Train Loss: 0.005031250417232513\n",
      "Test Loss: 0.2580367922782898\n",
      "\n",
      "Epoch: 167\n",
      "Train Loss: 0.005257779615931213\n",
      "Test Loss: 0.25802382826805115\n",
      "\n",
      "Epoch: 168\n",
      "Train Loss: 0.005357011745218188\n",
      "Test Loss: 0.2535325884819031\n",
      "\n",
      "Epoch: 169\n",
      "Train Loss: 0.005071908759418875\n",
      "Test Loss: 0.25502264499664307\n",
      "\n",
      "Epoch: 170\n",
      "Train Loss: 0.005328405706677586\n",
      "Test Loss: 0.2593061923980713\n",
      "\n",
      "Epoch: 171\n",
      "Train Loss: 0.004607561393640935\n",
      "Test Loss: 0.25138410925865173\n",
      "\n",
      "Epoch: 172\n",
      "Train Loss: 0.004601381195243448\n",
      "Test Loss: 0.25879234075546265\n",
      "\n",
      "Epoch: 173\n",
      "Train Loss: 0.005599008174613118\n",
      "Test Loss: 0.2644544839859009\n",
      "\n",
      "Epoch: 174\n",
      "Train Loss: 0.006064088374841958\n",
      "Test Loss: 0.26081737875938416\n",
      "\n",
      "Epoch: 175\n",
      "Train Loss: 0.004943204519804567\n",
      "Test Loss: 0.25120237469673157\n",
      "\n",
      "Epoch: 176\n",
      "Train Loss: 0.0040440550073981285\n",
      "Test Loss: 0.26441243290901184\n",
      "\n",
      "Epoch: 177\n",
      "Train Loss: 0.0047338439035229385\n",
      "Test Loss: 0.2536883056163788\n",
      "\n",
      "Epoch: 178\n",
      "Train Loss: 0.004622163658495992\n",
      "Test Loss: 0.2533856928348541\n",
      "\n",
      "Epoch: 179\n",
      "Train Loss: 0.004560190427582711\n",
      "Test Loss: 0.2546738386154175\n",
      "\n",
      "Epoch: 180\n",
      "Train Loss: 0.004768532118760049\n",
      "Test Loss: 0.25981760025024414\n",
      "\n",
      "Epoch: 181\n",
      "Train Loss: 0.005312818335369229\n",
      "Test Loss: 0.24866220355033875\n",
      "\n",
      "Epoch: 182\n",
      "Train Loss: 0.006376754376105964\n",
      "Test Loss: 0.25958383083343506\n",
      "\n",
      "Epoch: 183\n",
      "Train Loss: 0.004976904776412994\n",
      "Test Loss: 0.2647448480129242\n",
      "\n",
      "Epoch: 184\n",
      "Train Loss: 0.005627244419883937\n",
      "Test Loss: 0.2584283649921417\n",
      "\n",
      "Epoch: 185\n",
      "Train Loss: 0.0050953071331605315\n",
      "Test Loss: 0.2613508105278015\n",
      "\n",
      "Epoch: 186\n",
      "Train Loss: 0.004894105135463178\n",
      "Test Loss: 0.26345115900039673\n",
      "\n",
      "Epoch: 187\n",
      "Train Loss: 0.004794301959918812\n",
      "Test Loss: 0.2559313178062439\n",
      "\n",
      "Epoch: 188\n",
      "Train Loss: 0.006043007946573198\n",
      "Test Loss: 0.25167742371559143\n",
      "\n",
      "Epoch: 189\n",
      "Train Loss: 0.00704470940399915\n",
      "Test Loss: 0.2612594962120056\n",
      "\n",
      "Epoch: 190\n",
      "Train Loss: 0.004890247364528477\n",
      "Test Loss: 0.2725663185119629\n",
      "\n",
      "Epoch: 191\n",
      "Train Loss: 0.0052184221567586064\n",
      "Test Loss: 0.2609841227531433\n",
      "\n",
      "Epoch: 192\n",
      "Train Loss: 0.004346083733253181\n",
      "Test Loss: 0.2571460008621216\n",
      "\n",
      "Epoch: 193\n",
      "Train Loss: 0.004835619067307562\n",
      "Test Loss: 0.24530740082263947\n",
      "\n",
      "Epoch: 194\n",
      "Train Loss: 0.005971215839963406\n",
      "Test Loss: 0.2623685598373413\n",
      "\n",
      "Epoch: 195\n",
      "Train Loss: 0.005628231388982385\n",
      "Test Loss: 0.2604106664657593\n",
      "\n",
      "Epoch: 196\n",
      "Train Loss: 0.00606492831138894\n",
      "Test Loss: 0.2585051357746124\n",
      "\n",
      "Epoch: 197\n",
      "Train Loss: 0.004845447023399174\n",
      "Test Loss: 0.2662985920906067\n",
      "\n",
      "Epoch: 198\n",
      "Train Loss: 0.0050253423396497965\n",
      "Test Loss: 0.2532404959201813\n",
      "\n",
      "Epoch: 199\n",
      "Train Loss: 0.0064208898693323135\n",
      "Test Loss: 0.2560775876045227\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd752207",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = Discriminator(3, 128, 12)\n",
    "s_model = s_model.to(DEVICE)\n",
    "s_optimizer = torch.optim.Adam(s_model.parameters(), lr = LR)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "dfb925a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "50947497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1761, 0.3635, 0.7116, 0.4861, 0.2940, 0.4402, 0.5499, 0.1935, 0.6286,\n",
      "         0.5097, 0.4684, 0.3761]], device='cuda:7')\n",
      "tensor(2, device='cuda:7')\n",
      "tensor([0.1921, 0.3630, 0.7462, 0.5108, 0.3015, 0.4393, 0.5710, 0.2034, 0.6278,\n",
      "        0.5121, 0.5224, 0.3982])\n",
      "tensor(2)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "n+=1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(train_tiles[n].reshape(3,56,56).to(DEVICE).unsqueeze(0))\n",
    "    print(out)\n",
    "    print(out.argmax())\n",
    "    print(cluster_dists[n])\n",
    "    print(cluster_dists[n].argmax())\n",
    "    print(train_cluster[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "06ab80ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2593, 0.4116, 0.7252, 0.5696, 0.4040, 0.4303, 0.6239, 0.2999, 0.5534,\n",
      "         0.5677, 0.5029, 0.3411]], device='cuda:7')\n",
      "tensor(2, device='cuda:7')\n",
      "tensor([0.2681, 0.4193, 0.7577, 0.5917, 0.4077, 0.4306, 0.6630, 0.3285, 0.6058,\n",
      "        0.5836, 0.5645, 0.3457])\n",
      "tensor(2)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "n+=1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(test_tiles[n].reshape(3,56,56).to(DEVICE).unsqueeze(0))\n",
    "    print(out)\n",
    "    print(out.argmax())\n",
    "    print(te_cluster_dists[n])\n",
    "    print(te_cluster_dists[n].argmax())\n",
    "    print(test_cluster[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1068f829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 56, 56, 3])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(train_tiles[tr_ind].shape)\n",
    "print(train_cluster[tr_ind].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b09cfad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  8,  4,  3,  1,  0,  2, 11,  2,  7,  1, 11,  5,  5, 11,  7, 10,  1,\n",
       "         0,  5,  9, 10,  2,  6,  2,  2,  3,  8, 11, 10,  1,  2,  3,  0,  2,  0,\n",
       "         2,  9,  2, 11, 10,  0,  1,  1,  2,  0,  2, 10,  0, 11,  2,  2, 10, 11,\n",
       "        10,  1,  0,  9,  9,  7,  8,  3,  9,  8,  9,  6,  3,  1,  4,  5, 11, 11,\n",
       "        11,  4,  2,  1,  1, 11, 10, 11,  0,  6,  1,  5,  4, 11,  5,  6,  2,  2,\n",
       "         0,  0,  1,  7,  1, 11,  2,  1, 11,  1], dtype=torch.int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cluster[tr_ind]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_thesis_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
