{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f826cf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "from torch_geometric.data import Dataset, download_url, Data\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from torch_geometric import nn as gnn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import crop\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from torch.nn import Sequential as Seq\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation')\n",
    "from onegraph_rest_free_graphds import OneGraphDS\n",
    "from gan_based_models import Discriminator\n",
    "from second_stage_vig_based_models import FullModel\n",
    "from vig_based_functions import act_layer, get_multi_shot_set\n",
    "from vig_graph_modules import GrapherSetEdges, FFN\n",
    "from perceptual_loss import VGGPerceptualLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43e70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/paulraae/MS_Thesis_Data/tester_eye_data/test_202_with_tiles_processed.pk1', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbe554d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'image', 'features', 'factor', 'cluster', 'centroids', 'umap_reducer', 'tiles', 'tile_features', 'tile_cluster', 'tile_centroids', 'tile_images', 'tile_factor', 'all_tiles', 'tile_knn_edge_index'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7142536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([549, 224, 224, 3])\n",
      "torch.Size([188, 224, 224, 3])\n",
      "torch.Size([549])\n",
      "torch.Size([188])\n",
      "torch.Size([549, 6])\n",
      "torch.Size([188, 6])\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"whatmatters_split_eye_tile_dataset.pickle\"\n",
    "with open(SAVE_PATH, 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "train_tiles = data[\"train_tiles\"]\n",
    "test_tiles = data[\"test_tiles\"]\n",
    "train_cluster = data[\"train_cluster\"]\n",
    "test_cluster = data[\"test_cluster\"]\n",
    "cluster_dists = data[\"train_cluster_dists\"]\n",
    "te_cluster_dists = data[\"test_cluster_dists\"]\n",
    "\n",
    "print(train_tiles.shape)\n",
    "print(test_tiles.shape)\n",
    "print(train_cluster.shape)\n",
    "print(test_cluster.shape)\n",
    "print(cluster_dists.shape)\n",
    "print(te_cluster_dists.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f208ccf",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dd4c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([202, 224, 224, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "#B,H,W,C = train_tiles.shape\n",
    "#B2,H,W,C = test_tiles.shape\n",
    "train_tiles = torch.Tensor(data[\"image\"])\n",
    "train_tiles = train_tiles/255\n",
    "#x = train_tiles.reshape(B,-1)\n",
    "train_tiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "544cd2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=6)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KMeans</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.cluster.KMeans.html\">?<span>Documentation for KMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_clusters',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_clusters&nbsp;</td>\n",
       "            <td class=\"value\">6</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('init',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">init&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;k-means++&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_init',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_init&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">300</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy_x',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy_x&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('algorithm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">algorithm&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lloyd&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "KMeans(n_clusters=6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "x = train_tiles.reshape(train_tiles.shape[0], -1)\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73da190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tiles_clusters = torch.Tensor(kmeans.labels_).to(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c96ce6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 50 41 47 32 14]\n",
      "torch.Size([202, 224, 224, 3])\n",
      "torch.Size([202])\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(train_tiles_clusters, return_counts=True)\n",
    "\n",
    "print(counts)\n",
    "print(train_tiles.shape)\n",
    "print(train_tiles_clusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "544cd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.Tensor(kmeans.cluster_centers_).reshape(6,224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caea83cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 37 30 35 24 10]\n",
      "149\n",
      "[ 5 13 11 12  8  4]\n",
      "53\n",
      "[ True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "train_counts=np.array(np.floor(counts*0.75), dtype=np.int64)\n",
    "print(train_counts)\n",
    "print(train_counts.sum())\n",
    "print(counts - train_counts)\n",
    "print((counts - train_counts).sum())\n",
    "print((train_counts+(counts-train_counts)) == counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2e4d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "test_ind = []\n",
    "train_ind = []\n",
    "unique, counts = np.unique(train_tiles_clusters, return_counts=True)\n",
    "\n",
    "for cluster, count in zip(np.unique(train_tiles_clusters), train_counts):\n",
    "    main_array = np.where(train_tiles_clusters == cluster)[0]\n",
    "    train_values = rng.choice(main_array, size=count, replace=False)\n",
    "    test_values = np.setdiff1d(main_array, train_values)\n",
    "    \n",
    "    train_ind.extend(train_values)\n",
    "    test_ind.extend(test_values)\n",
    "\n",
    "train_ind = np.array(train_ind)\n",
    "test_ind = np.array(test_ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28f2a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149,)\n",
      "(53,)\n"
     ]
    }
   ],
   "source": [
    "print(train_ind.shape)\n",
    "print(test_ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8cd78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([149, 224, 224, 3])\n",
      "torch.Size([53, 224, 224, 3])\n",
      "torch.Size([149])\n",
      "torch.Size([53])\n"
     ]
    }
   ],
   "source": [
    "test_tiles = train_tiles[test_ind]\n",
    "train_tiles = train_tiles[train_ind]\n",
    "\n",
    "train_cluster = train_tiles_clusters[train_ind]\n",
    "test_cluster = train_tiles_clusters[test_ind]\n",
    "\n",
    "print(train_tiles.shape)\n",
    "print(test_tiles.shape)\n",
    "print(train_cluster.shape)\n",
    "print(test_cluster.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2fc549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n+=1\n",
    "#print(n)\n",
    "#print(\"Cluster:\",train_cluster[n].item())\n",
    "cluster_dists = []\n",
    "with torch.no_grad():\n",
    "    for tiles in train_tiles:\n",
    "        aux = []\n",
    "        for centroid in centroids:\n",
    "            aux.append(torch.dist(tiles, centroid, p=2))\n",
    "        aux = torch.Tensor(aux)\n",
    "        cluster_dists.append(aux)\n",
    "    cluster_dists = torch.stack(cluster_dists)\n",
    "\n",
    "cluster_dists = (cluster_dists - torch.min(cluster_dists))/(torch.max(cluster_dists) - torch.min(cluster_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e18e8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n+=1\n",
    "#print(n)\n",
    "#print(\"Cluster:\",train_cluster[n].item())\n",
    "te_cluster_dists = []\n",
    "with torch.no_grad():\n",
    "    for tiles in test_tiles:\n",
    "        aux = []\n",
    "        for centroid in centroids:\n",
    "            aux.append(torch.dist(tiles, centroid, p=2))\n",
    "        aux = torch.Tensor(aux)\n",
    "        te_cluster_dists.append(aux)\n",
    "    te_cluster_dists = torch.stack(te_cluster_dists)\n",
    "\n",
    "te_cluster_dists = (te_cluster_dists - torch.min(te_cluster_dists))/(torch.max(te_cluster_dists) - torch.min(te_cluster_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a94cec4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149, 6])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76292617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([149, 224, 224, 3])\n",
      "torch.Size([53, 224, 224, 3])\n",
      "torch.Size([149])\n",
      "torch.Size([53])\n",
      "torch.Size([149, 6])\n",
      "torch.Size([53, 6])\n"
     ]
    }
   ],
   "source": [
    "print(train_tiles.shape)\n",
    "print(test_tiles.shape)\n",
    "print(train_cluster.shape)\n",
    "print(test_cluster.shape)\n",
    "print(cluster_dists.shape)\n",
    "print(te_cluster_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34de371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_split_eye_tile_dataset.pickle\"\n",
    "data = {\"train_tiles\":train_tiles, \"test_tiles\":test_tiles, \"train_cluster\":train_cluster, \"test_cluster\":test_cluster, \"train_cluster_dists\":cluster_dists, \"test_cluster_dists\":te_cluster_dists}\n",
    "with open('whatmatters_split_eye_full_dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18da714",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ad446a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscDownBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "      super().__init__()\n",
    "      self.convs = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(True),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      return self.convs(x)\n",
    "    \n",
    "class DiscStrideBlock(nn.Module):\n",
    "  def __init__(self, in_dim):\n",
    "    super().__init__()\n",
    "    self.convs = nn.Sequential(\n",
    "      nn.Conv2d(in_dim, in_dim, 3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(in_dim),\n",
    "      nn.ReLU(True),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.convs(x)\n",
    "\n",
    "class DiscBlock(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim):\n",
    "    super().__init__()\n",
    "    self.down = DiscDownBlock(in_dim, out_dim)\n",
    "    self.stride = DiscStrideBlock(out_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.down(x)\n",
    "    return self.stride(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_cats):\n",
    "      super().__init__()\n",
    "      self.block1 = DiscBlock(in_dim, out_dim//16)\n",
    "      self.block2 = DiscBlock(out_dim//16, out_dim//8)\n",
    "      self.block3 = DiscBlock(out_dim//8, out_dim//4)\n",
    "      self.block4 = DiscBlock(out_dim//4, out_dim//2)\n",
    "      self.block5 = DiscBlock(out_dim//2, out_dim)\n",
    "\n",
    "      self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(out_dim*7*7, 8192),\n",
    "        nn.BatchNorm1d(8192),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(8192, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(4096, num_cats),\n",
    "        nn.Sigmoid(),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      x1 = self.block1(x)\n",
    "      x2 = self.block2(x1)\n",
    "      x3 = self.block3(x2)\n",
    "      x4 = self.block4(x3)\n",
    "      x5 = self.block5(x4)\n",
    "      x = self.classifier(x5)\n",
    "      return [x1,x2,x3,x4,x5], x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32b4d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorBlock(nn.Module):\n",
    "  def __init__(self, out_dim, img_size):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(out_dim*(img_size//2)*(img_size//2), 8192),\n",
    "      nn.BatchNorm1d(8192),\n",
    "      nn.ReLU(True),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear(x)\n",
    "    \n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_cats ,num_patches = 3, stride = 14, img_size=224, device=\"cuda:7\"):\n",
    "      super().__init__()\n",
    "      self.block1 = DetectorBlock(out_dim//16, img_size)\n",
    "      self.block2 = DetectorBlock(out_dim//8, img_size//2)\n",
    "      self.block3 = DetectorBlock(out_dim//4, img_size//4)\n",
    "      self.block4 = DetectorBlock(out_dim//2, img_size//8)\n",
    "      self.block5 = DetectorBlock(out_dim, img_size//16)\n",
    "      \n",
    "      self.regressor = nn.Sequential(\n",
    "        nn.Linear(8192*5, 8192),\n",
    "        nn.BatchNorm1d(8192),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(8192, 4096),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(4096, num_patches*2),\n",
    "        #nn.Sigmoid()\n",
    "      )\n",
    "      self.classifier = Discriminator(in_dim, out_dim, num_cats)\n",
    "      self.stride = stride\n",
    "      self.device = device\n",
    "\n",
    "    def forward(self, x, x_in):\n",
    "      B,C,H,W = x[0].shape\n",
    "      x1 = self.block1(x[0])\n",
    "      x2 = self.block2(x[1])\n",
    "      x3 = self.block3(x[2])\n",
    "      x4 = self.block4(x[3])\n",
    "      x5 = self.block5(x[4])\n",
    "      \n",
    "      x = torch.cat([x1,x2,x3,x4,x5], dim=1)\n",
    "      \n",
    "      x1 = self.regressor(x)\n",
    "      x1 = (x1 - torch.min(x1))/(torch.max(x1) - torch.min(x1))\n",
    "      \n",
    "      base = torch.ones(B,224,224,3)\n",
    "      #base = (base - torch.min(base))/(torch.max(base) - torch.min(base))\n",
    "      i=-1\n",
    "      for b,xx in zip(base,x1):\n",
    "        i+=1\n",
    "        for o in xx.reshape(-1,2):\n",
    "          x_min = int(o[0] * 224)\n",
    "          y_min = int(o[1] * 224)\n",
    "          x_max = x_min+self.stride\n",
    "          y_max = y_min+self.stride\n",
    "          b[x_min:x_max, y_min:y_max] = x_in[i].reshape(224,224,3)[x_min:x_max, y_min:y_max]\n",
    "      \n",
    "      base = base.to(self.device)\n",
    "\n",
    "      x2, x = self.classifier(base.reshape(B,3,224,224))\n",
    "      return x1, base, x, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bec5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 20\n",
    "DEVICE = \"cuda:6\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fc4ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.00005\n",
    "model = Discriminator(3, 128, 6)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0918ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_d_model1.pickle\"\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a0d16e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.14143567625433207\n",
      "Test Loss: 0.20692051947116852\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.33360218070447445\n",
      "Test Loss: 0.13108967244625092\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.30186210572719574\n",
      "Test Loss: 0.09353609383106232\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.21862110681831837\n",
      "Test Loss: 0.0849086195230484\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.16758451610803604\n",
      "Test Loss: 0.08707345277070999\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.12232277449220419\n",
      "Test Loss: 0.09826349467039108\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.09886367339640856\n",
      "Test Loss: 0.11360981315374374\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.09263916965574026\n",
      "Test Loss: 0.1458495408296585\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.08870517648756504\n",
      "Test Loss: 0.16251416504383087\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.07348323380574584\n",
      "Test Loss: 0.1701788306236267\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.07337563904002309\n",
      "Test Loss: 0.16114622354507446\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.06814371654763818\n",
      "Test Loss: 0.17156201601028442\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.0643030907958746\n",
      "Test Loss: 0.17125996947288513\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.04925112705677748\n",
      "Test Loss: 0.17824898660182953\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.04527394310571253\n",
      "Test Loss: 0.17198389768600464\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.04638704378157854\n",
      "Test Loss: 0.17213968932628632\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.03929187124595046\n",
      "Test Loss: 0.17542341351509094\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.0405572650488466\n",
      "Test Loss: 0.17192710936069489\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.03572464641183615\n",
      "Test Loss: 0.16743457317352295\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.04259634669870138\n",
      "Test Loss: 0.16761016845703125\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.03725567739456892\n",
      "Test Loss: 0.17354971170425415\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.03740070224739611\n",
      "Test Loss: 0.17069761455059052\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.03662368655204773\n",
      "Test Loss: 0.17525550723075867\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.035065362229943275\n",
      "Test Loss: 0.16725163161754608\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.03441601060330868\n",
      "Test Loss: 0.17152464389801025\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.0349330916069448\n",
      "Test Loss: 0.16979925334453583\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.02712738560512662\n",
      "Test Loss: 0.16660912334918976\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.03169237286783755\n",
      "Test Loss: 0.16952377557754517\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.0270954635925591\n",
      "Test Loss: 0.17600150406360626\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.026639638235792518\n",
      "Test Loss: 0.17247697710990906\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.027385915629565716\n",
      "Test Loss: 0.17008379101753235\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.024852452566847205\n",
      "Test Loss: 0.16922080516815186\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.02760682301595807\n",
      "Test Loss: 0.16574601829051971\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.02712157624773681\n",
      "Test Loss: 0.168190136551857\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.027763429330661893\n",
      "Test Loss: 0.17039409279823303\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.023646257119253278\n",
      "Test Loss: 0.1594201624393463\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.025241261115297675\n",
      "Test Loss: 0.169109508395195\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.02228755713440478\n",
      "Test Loss: 0.1661778837442398\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.02117006853222847\n",
      "Test Loss: 0.16657349467277527\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.021928004454821348\n",
      "Test Loss: 0.16648206114768982\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.02239726623520255\n",
      "Test Loss: 0.1626008301973343\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.021384089021012187\n",
      "Test Loss: 0.1657123863697052\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.02073140279389918\n",
      "Test Loss: 0.16831164062023163\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.01882224716246128\n",
      "Test Loss: 0.16721782088279724\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.01978646160569042\n",
      "Test Loss: 0.16365712881088257\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.018208827823400497\n",
      "Test Loss: 0.16168281435966492\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.018883215030655265\n",
      "Test Loss: 0.16867321729660034\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.019822465255856514\n",
      "Test Loss: 0.16643908619880676\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.018260152311995625\n",
      "Test Loss: 0.16527292132377625\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.01538435195107013\n",
      "Test Loss: 0.1712767332792282\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.012690196628682315\n",
      "Test Loss: 0.1705104112625122\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.011844090302474797\n",
      "Test Loss: 0.16631950438022614\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.012601972441188991\n",
      "Test Loss: 0.1672576665878296\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.014351993333548307\n",
      "Test Loss: 0.1727009415626526\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.012053768034093082\n",
      "Test Loss: 0.16431747376918793\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.013274491997435689\n",
      "Test Loss: 0.1698743999004364\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.013856561854481697\n",
      "Test Loss: 0.16731660068035126\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.014106226386502385\n",
      "Test Loss: 0.1660313755273819\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.014194672112353146\n",
      "Test Loss: 0.16827188432216644\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.013991903513669968\n",
      "Test Loss: 0.16614237427711487\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.011944239027798176\n",
      "Test Loss: 0.1652924120426178\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.012145551037974656\n",
      "Test Loss: 0.17437076568603516\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.011867684195749462\n",
      "Test Loss: 0.17086903750896454\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.012480059871450067\n",
      "Test Loss: 0.16147980093955994\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.014631078112870455\n",
      "Test Loss: 0.16816626489162445\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.013505792245268822\n",
      "Test Loss: 0.17261776328086853\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.012576217530295253\n",
      "Test Loss: 0.16504952311515808\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.010380124091170728\n",
      "Test Loss: 0.17006522417068481\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.009410802798811346\n",
      "Test Loss: 0.17279000580310822\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.008637581078801304\n",
      "Test Loss: 0.16966956853866577\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.011614606250077486\n",
      "Test Loss: 0.1699022650718689\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.009148928744252771\n",
      "Test Loss: 0.1685781627893448\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.011101861717179418\n",
      "Test Loss: 0.1702263206243515\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.008637923398055136\n",
      "Test Loss: 0.16753600537776947\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.009054149151779711\n",
      "Test Loss: 0.16675449907779694\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.009762560715898871\n",
      "Test Loss: 0.17339515686035156\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.008259095251560211\n",
      "Test Loss: 0.17217838764190674\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.008172767178621143\n",
      "Test Loss: 0.16859039664268494\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.00863479619147256\n",
      "Test Loss: 0.1770927757024765\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.009306760563049465\n",
      "Test Loss: 0.17337386310100555\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.008135791984386742\n",
      "Test Loss: 0.16795341670513153\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.008906652685254812\n",
      "Test Loss: 0.1754857897758484\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.00819627137389034\n",
      "Test Loss: 0.17300209403038025\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.007842311053536832\n",
      "Test Loss: 0.17237192392349243\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.007588464708533138\n",
      "Test Loss: 0.17411723732948303\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.010001691756770015\n",
      "Test Loss: 0.1736653745174408\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.007687554694712162\n",
      "Test Loss: 0.17046253383159637\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.00783964607398957\n",
      "Test Loss: 0.17041577398777008\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.008405840082559735\n",
      "Test Loss: 0.1711629182100296\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.009147730830591172\n",
      "Test Loss: 0.1695355474948883\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.007905372767709196\n",
      "Test Loss: 0.17191162705421448\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.005884892540052533\n",
      "Test Loss: 0.17420069873332977\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.007419737637974322\n",
      "Test Loss: 0.1686507612466812\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.007918408082332462\n",
      "Test Loss: 0.17008356750011444\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.005667311022989452\n",
      "Test Loss: 0.16672292351722717\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.006819054251536727\n",
      "Test Loss: 0.17016173899173737\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.006213318964000791\n",
      "Test Loss: 0.1731284260749817\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.005758965387940407\n",
      "Test Loss: 0.17098869383335114\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.006195023714099079\n",
      "Test Loss: 0.1718137413263321\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.008014466089662164\n",
      "Test Loss: 0.1680142879486084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch:\",epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_index = np.array([x for x in range(train_tiles.shape[0])])\n",
    "    np.random.shuffle(train_index)\n",
    "\n",
    "    train_index = train_index.T[:140].reshape(-1,BATCH_SIZE)\n",
    "    i=-1\n",
    "    \n",
    "    model.train()\n",
    "    for tr_ind in train_index:\n",
    "        i+=1\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = train_tiles[tr_ind]\n",
    "        y = cluster_dists[tr_ind]\n",
    "        \n",
    "        x = x.reshape(-1,3,224,224)\n",
    "        y = y.to(DEVICE)\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        conv, pred = model(x)\n",
    "        del x\n",
    "        pl = loss(pred,y)\n",
    "        del y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pl.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += pl.item()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        x = test_tiles\n",
    "        y = te_cluster_dists\n",
    "        \n",
    "        x = x.reshape(-1,3,224,224).to(DEVICE)\n",
    "        y = y.to(torch.long).to(DEVICE)\n",
    "        conv, pred = model(x)\n",
    "        tl = loss(pred,y)\n",
    "        del x\n",
    "        del y\n",
    "    \n",
    "    \n",
    "    print(\"Train Loss:\", running_loss)\n",
    "    print(\"Test Loss:\", tl.item())\n",
    "    print()\n",
    "    del pl, tl\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63f751d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "826b655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_model = Detector(3, 128, 6, 20, 24, 224, DEVICE)\n",
    "o_model = o_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "330ccff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "O_LR = 0.0001\n",
    "o_optimizer = torch.optim.Adam(o_model.parameters(), lr = O_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6a86f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44e2f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7380074361960093\n",
      "Test Loss: 0.39000555872917175\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.7015319267908732\n",
      "Test Loss: 0.3788076639175415\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.6977047721544901\n",
      "Test Loss: 0.40337809920310974\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.6898615260918936\n",
      "Test Loss: 0.43214184045791626\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.6608399152755737\n",
      "Test Loss: 0.48822927474975586\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.6564395328362783\n",
      "Test Loss: 0.4466821849346161\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.6442128916581472\n",
      "Test Loss: 0.4968212842941284\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.634418119986852\n",
      "Test Loss: 0.47967463731765747\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.6238374213377634\n",
      "Test Loss: 0.4751279950141907\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.6134564876556396\n",
      "Test Loss: 0.4890499413013458\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.6058171143134435\n",
      "Test Loss: 0.4844872057437897\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.5955079446236292\n",
      "Test Loss: 0.4669140875339508\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.5945746749639511\n",
      "Test Loss: 0.46927234530448914\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.5954389969507853\n",
      "Test Loss: 0.46633380651474\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.5772321373224258\n",
      "Test Loss: 0.4675973653793335\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.5813584476709366\n",
      "Test Loss: 0.470686137676239\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.575650210181872\n",
      "Test Loss: 0.46542033553123474\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.5699863235155741\n",
      "Test Loss: 0.4644448161125183\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.5712136775255203\n",
      "Test Loss: 0.47059038281440735\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.5609917541344961\n",
      "Test Loss: 0.45915254950523376\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.5617624719937643\n",
      "Test Loss: 0.45863065123558044\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.5542506525913874\n",
      "Test Loss: 0.4663649797439575\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.5578951140244802\n",
      "Test Loss: 0.44357216358184814\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.5486636708180109\n",
      "Test Loss: 0.46494734287261963\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.5493647207816442\n",
      "Test Loss: 0.43681207299232483\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.5432802041371664\n",
      "Test Loss: 0.43679898977279663\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.5294788926839828\n",
      "Test Loss: 0.428824782371521\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.5271037121613821\n",
      "Test Loss: 0.4231763482093811\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.5298473089933395\n",
      "Test Loss: 0.4289800822734833\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.5181580434242884\n",
      "Test Loss: 0.41929787397384644\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.520160103837649\n",
      "Test Loss: 0.42798954248428345\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.5152072906494141\n",
      "Test Loss: 0.42604392766952515\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.5183852811654409\n",
      "Test Loss: 0.41092318296432495\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.510212371746699\n",
      "Test Loss: 0.41625702381134033\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.5029818216959635\n",
      "Test Loss: 0.41457703709602356\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.5010500500599543\n",
      "Test Loss: 0.42120832204818726\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.4936433881521225\n",
      "Test Loss: 0.40172407031059265\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.48668650289376575\n",
      "Test Loss: 0.40782830119132996\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.48435687522093457\n",
      "Test Loss: 0.4122060239315033\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.48873192071914673\n",
      "Test Loss: 0.3944879472255707\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.48601194222768146\n",
      "Test Loss: 0.38099026679992676\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.482734536131223\n",
      "Test Loss: 0.40140777826309204\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.4747610886891683\n",
      "Test Loss: 0.39252960681915283\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.476599117120107\n",
      "Test Loss: 0.40670672059059143\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.4735524157683055\n",
      "Test Loss: 0.39781251549720764\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.47493574519952136\n",
      "Test Loss: 0.3861958086490631\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.46627042690912884\n",
      "Test Loss: 0.3888317048549652\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.46270013352235156\n",
      "Test Loss: 0.39393311738967896\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.4631315718094508\n",
      "Test Loss: 0.39126867055892944\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.4583342671394348\n",
      "Test Loss: 0.37628060579299927\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.4563892086346944\n",
      "Test Loss: 0.3733308017253876\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.4609026064475377\n",
      "Test Loss: 0.3771686851978302\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.4542383402585983\n",
      "Test Loss: 0.37000855803489685\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.4501689523458481\n",
      "Test Loss: 0.3738744258880615\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.4494449893633525\n",
      "Test Loss: 0.3700518012046814\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.4480187992254893\n",
      "Test Loss: 0.363486111164093\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.4457683314879735\n",
      "Test Loss: 0.36381202936172485\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.4441526085138321\n",
      "Test Loss: 0.37311580777168274\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.4452696641286214\n",
      "Test Loss: 0.3660506308078766\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.43959201872348785\n",
      "Test Loss: 0.36761778593063354\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.4386880745490392\n",
      "Test Loss: 0.3638555705547333\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.4418879995743434\n",
      "Test Loss: 0.36467820405960083\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.43444186449050903\n",
      "Test Loss: 0.3711583614349365\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.43239599963029224\n",
      "Test Loss: 0.36146771907806396\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.43553126355012256\n",
      "Test Loss: 0.35618558526039124\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.42845726509888965\n",
      "Test Loss: 0.3631518483161926\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.42269473771254223\n",
      "Test Loss: 0.37071916460990906\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.4232769658168157\n",
      "Test Loss: 0.3655720055103302\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.4320057233174642\n",
      "Test Loss: 0.3641042411327362\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.4368392378091812\n",
      "Test Loss: 0.35073286294937134\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.4315016021331151\n",
      "Test Loss: 0.3704313635826111\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.4263705015182495\n",
      "Test Loss: 0.35161927342414856\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.43160847822825116\n",
      "Test Loss: 0.35552841424942017\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.4224312702814738\n",
      "Test Loss: 0.3441823720932007\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.40689945717652637\n",
      "Test Loss: 0.34463393688201904\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.41305598616600037\n",
      "Test Loss: 0.3483554720878601\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.4114990284045537\n",
      "Test Loss: 0.3523746430873871\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.41455669204394024\n",
      "Test Loss: 0.35295194387435913\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.4175130973259608\n",
      "Test Loss: 0.34813392162323\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.4120484044154485\n",
      "Test Loss: 0.34966403245925903\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.4160365362962087\n",
      "Test Loss: 0.3445584177970886\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.4160670091708501\n",
      "Test Loss: 0.3440724015235901\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.4021218866109848\n",
      "Test Loss: 0.3408304750919342\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.40453540285428363\n",
      "Test Loss: 0.3361984193325043\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.4100198447704315\n",
      "Test Loss: 0.33581578731536865\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.4089551518360774\n",
      "Test Loss: 0.33254820108413696\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.4008486618598302\n",
      "Test Loss: 0.3381206691265106\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.3968110928932826\n",
      "Test Loss: 0.34808221459388733\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.40133138994375867\n",
      "Test Loss: 0.33905649185180664\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.4052065958579381\n",
      "Test Loss: 0.34156104922294617\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.4006019135316213\n",
      "Test Loss: 0.34974491596221924\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.40601779023806256\n",
      "Test Loss: 0.34189561009407043\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.39973295231660205\n",
      "Test Loss: 0.34111252427101135\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.400456503033638\n",
      "Test Loss: 0.34295952320098877\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.4001867820819219\n",
      "Test Loss: 0.33297550678253174\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.39368222653865814\n",
      "Test Loss: 0.3278752863407135\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.3980789879957835\n",
      "Test Loss: 0.33032646775245667\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.39659713705380756\n",
      "Test Loss: 0.333871066570282\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.3870497792959213\n",
      "Test Loss: 0.3345663249492645\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.396797979871432\n",
      "Test Loss: 0.3311024308204651\n",
      "\n",
      "Epoch: 100\n",
      "Train Loss: 0.39408203462759656\n",
      "Test Loss: 0.3319211006164551\n",
      "\n",
      "Epoch: 101\n",
      "Train Loss: 0.3931161016225815\n",
      "Test Loss: 0.33510661125183105\n",
      "\n",
      "Epoch: 102\n",
      "Train Loss: 0.3904734750588735\n",
      "Test Loss: 0.32893913984298706\n",
      "\n",
      "Epoch: 103\n",
      "Train Loss: 0.3876986453930537\n",
      "Test Loss: 0.32505667209625244\n",
      "\n",
      "Epoch: 104\n",
      "Train Loss: 0.38534613450368244\n",
      "Test Loss: 0.33575522899627686\n",
      "\n",
      "Epoch: 105\n",
      "Train Loss: 0.38540669282277423\n",
      "Test Loss: 0.3223121762275696\n",
      "\n",
      "Epoch: 106\n",
      "Train Loss: 0.39360131820042926\n",
      "Test Loss: 0.32940396666526794\n",
      "\n",
      "Epoch: 107\n",
      "Train Loss: 0.3918563276529312\n",
      "Test Loss: 0.3238815665245056\n",
      "\n",
      "Epoch: 108\n",
      "Train Loss: 0.39227986832459766\n",
      "Test Loss: 0.31921476125717163\n",
      "\n",
      "Epoch: 109\n",
      "Train Loss: 0.3918590048948924\n",
      "Test Loss: 0.3285093903541565\n",
      "\n",
      "Epoch: 110\n",
      "Train Loss: 0.3856040785710017\n",
      "Test Loss: 0.32193633913993835\n",
      "\n",
      "Epoch: 111\n",
      "Train Loss: 0.38080182671546936\n",
      "Test Loss: 0.3189873993396759\n",
      "\n",
      "Epoch: 112\n",
      "Train Loss: 0.3862146983544032\n",
      "Test Loss: 0.3189546465873718\n",
      "\n",
      "Epoch: 113\n",
      "Train Loss: 0.38859525322914124\n",
      "Test Loss: 0.32552847266197205\n",
      "\n",
      "Epoch: 114\n",
      "Train Loss: 0.3849897136290868\n",
      "Test Loss: 0.31981921195983887\n",
      "\n",
      "Epoch: 115\n",
      "Train Loss: 0.38528429468472797\n",
      "Test Loss: 0.3186771869659424\n",
      "\n",
      "Epoch: 116\n",
      "Train Loss: 0.3817882140477498\n",
      "Test Loss: 0.31884750723838806\n",
      "\n",
      "Epoch: 117\n",
      "Train Loss: 0.37796586255232495\n",
      "Test Loss: 0.3194231688976288\n",
      "\n",
      "Epoch: 118\n",
      "Train Loss: 0.3829471915960312\n",
      "Test Loss: 0.31860077381134033\n",
      "\n",
      "Epoch: 119\n",
      "Train Loss: 0.3902278294165929\n",
      "Test Loss: 0.3153565526008606\n",
      "\n",
      "Epoch: 120\n",
      "Train Loss: 0.3798997104167938\n",
      "Test Loss: 0.3188585042953491\n",
      "\n",
      "Epoch: 121\n",
      "Train Loss: 0.37651089827219647\n",
      "Test Loss: 0.325313538312912\n",
      "\n",
      "Epoch: 122\n",
      "Train Loss: 0.37605265776316327\n",
      "Test Loss: 0.32699599862098694\n",
      "\n",
      "Epoch: 123\n",
      "Train Loss: 0.3793647736310959\n",
      "Test Loss: 0.3193211555480957\n",
      "\n",
      "Epoch: 124\n",
      "Train Loss: 0.37795962393283844\n",
      "Test Loss: 0.3146976828575134\n",
      "\n",
      "Epoch: 125\n",
      "Train Loss: 0.3781433254480362\n",
      "Test Loss: 0.3204209804534912\n",
      "\n",
      "Epoch: 126\n",
      "Train Loss: 0.3748020976781845\n",
      "Test Loss: 0.31835147738456726\n",
      "\n",
      "Epoch: 127\n",
      "Train Loss: 0.37912778556346893\n",
      "Test Loss: 0.3146438002586365\n",
      "\n",
      "Epoch: 128\n",
      "Train Loss: 0.3806878676017125\n",
      "Test Loss: 0.31665343046188354\n",
      "\n",
      "Epoch: 129\n",
      "Train Loss: 0.3741430689891179\n",
      "Test Loss: 0.31341612339019775\n",
      "\n",
      "Epoch: 130\n",
      "Train Loss: 0.37755776941776276\n",
      "Test Loss: 0.31619295477867126\n",
      "\n",
      "Epoch: 131\n",
      "Train Loss: 0.37354769309361774\n",
      "Test Loss: 0.3188663125038147\n",
      "\n",
      "Epoch: 132\n",
      "Train Loss: 0.374183585246404\n",
      "Test Loss: 0.3121255934238434\n",
      "\n",
      "Epoch: 133\n",
      "Train Loss: 0.37129470705986023\n",
      "Test Loss: 0.3170546293258667\n",
      "\n",
      "Epoch: 134\n",
      "Train Loss: 0.37380404273668927\n",
      "Test Loss: 0.30695027112960815\n",
      "\n",
      "Epoch: 135\n",
      "Train Loss: 0.37241385380427044\n",
      "Test Loss: 0.31243276596069336\n",
      "\n",
      "Epoch: 136\n",
      "Train Loss: 0.3697383503119151\n",
      "Test Loss: 0.3126473128795624\n",
      "\n",
      "Epoch: 137\n",
      "Train Loss: 0.36808371047178906\n",
      "Test Loss: 0.3136247992515564\n",
      "\n",
      "Epoch: 138\n",
      "Train Loss: 0.3645437608162562\n",
      "Test Loss: 0.31152451038360596\n",
      "\n",
      "Epoch: 139\n",
      "Train Loss: 0.3723631699879964\n",
      "Test Loss: 0.3139802813529968\n",
      "\n",
      "Epoch: 140\n",
      "Train Loss: 0.3732120345036189\n",
      "Test Loss: 0.3134225010871887\n",
      "\n",
      "Epoch: 141\n",
      "Train Loss: 0.37000354131062824\n",
      "Test Loss: 0.3108159303665161\n",
      "\n",
      "Epoch: 142\n",
      "Train Loss: 0.35696886479854584\n",
      "Test Loss: 0.3084951341152191\n",
      "\n",
      "Epoch: 143\n",
      "Train Loss: 0.37088393171628314\n",
      "Test Loss: 0.3101406991481781\n",
      "\n",
      "Epoch: 144\n",
      "Train Loss: 0.3673096348841985\n",
      "Test Loss: 0.31387320160865784\n",
      "\n",
      "Epoch: 145\n",
      "Train Loss: 0.36316969990730286\n",
      "Test Loss: 0.30486056208610535\n",
      "\n",
      "Epoch: 146\n",
      "Train Loss: 0.37503620982170105\n",
      "Test Loss: 0.308147132396698\n",
      "\n",
      "Epoch: 147\n",
      "Train Loss: 0.363630548119545\n",
      "Test Loss: 0.31009891629219055\n",
      "\n",
      "Epoch: 148\n",
      "Train Loss: 0.36964639524618786\n",
      "Test Loss: 0.3143980801105499\n",
      "\n",
      "Epoch: 149\n",
      "Train Loss: 0.3697764625151952\n",
      "Test Loss: 0.3086535930633545\n",
      "\n",
      "Epoch: 150\n",
      "Train Loss: 0.36785827577114105\n",
      "Test Loss: 0.3082790970802307\n",
      "\n",
      "Epoch: 151\n",
      "Train Loss: 0.36564283569653827\n",
      "Test Loss: 0.3094103932380676\n",
      "\n",
      "Epoch: 152\n",
      "Train Loss: 0.3671625703573227\n",
      "Test Loss: 0.304445743560791\n",
      "\n",
      "Epoch: 153\n",
      "Train Loss: 0.3682650526364644\n",
      "Test Loss: 0.31693315505981445\n",
      "\n",
      "Epoch: 154\n",
      "Train Loss: 0.3621119111776352\n",
      "Test Loss: 0.31691551208496094\n",
      "\n",
      "Epoch: 155\n",
      "Train Loss: 0.36798351009686786\n",
      "Test Loss: 0.3132801055908203\n",
      "\n",
      "Epoch: 156\n",
      "Train Loss: 0.35919523735841113\n",
      "Test Loss: 0.3089454770088196\n",
      "\n",
      "Epoch: 157\n",
      "Train Loss: 0.36113767325878143\n",
      "Test Loss: 0.30755770206451416\n",
      "\n",
      "Epoch: 158\n",
      "Train Loss: 0.36263223985830945\n",
      "Test Loss: 0.3061947226524353\n",
      "\n",
      "Epoch: 159\n",
      "Train Loss: 0.3620622555414836\n",
      "Test Loss: 0.30214807391166687\n",
      "\n",
      "Epoch: 160\n",
      "Train Loss: 0.373017023007075\n",
      "Test Loss: 0.30918627977371216\n",
      "\n",
      "Epoch: 161\n",
      "Train Loss: 0.3662042071421941\n",
      "Test Loss: 0.3029903471469879\n",
      "\n",
      "Epoch: 162\n",
      "Train Loss: 0.3635324090719223\n",
      "Test Loss: 0.30751436948776245\n",
      "\n",
      "Epoch: 163\n",
      "Train Loss: 0.35827913880348206\n",
      "Test Loss: 0.30681174993515015\n",
      "\n",
      "Epoch: 164\n",
      "Train Loss: 0.358021542429924\n",
      "Test Loss: 0.30666017532348633\n",
      "\n",
      "Epoch: 165\n",
      "Train Loss: 0.35520315170288086\n",
      "Test Loss: 0.30106037855148315\n",
      "\n",
      "Epoch: 166\n",
      "Train Loss: 0.36393533647060394\n",
      "Test Loss: 0.30174919962882996\n",
      "\n",
      "Epoch: 167\n",
      "Train Loss: 0.36149386564890545\n",
      "Test Loss: 0.2987237274646759\n",
      "\n",
      "Epoch: 168\n",
      "Train Loss: 0.35660254458586377\n",
      "Test Loss: 0.3067628741264343\n",
      "\n",
      "Epoch: 169\n",
      "Train Loss: 0.35537274181842804\n",
      "Test Loss: 0.30280619859695435\n",
      "\n",
      "Epoch: 170\n",
      "Train Loss: 0.3519906798998515\n",
      "Test Loss: 0.3132564425468445\n",
      "\n",
      "Epoch: 171\n",
      "Train Loss: 0.3577307065327962\n",
      "Test Loss: 0.30434104800224304\n",
      "\n",
      "Epoch: 172\n",
      "Train Loss: 0.3526279479265213\n",
      "Test Loss: 0.30034297704696655\n",
      "\n",
      "Epoch: 173\n",
      "Train Loss: 0.3625521908203761\n",
      "Test Loss: 0.29970264434814453\n",
      "\n",
      "Epoch: 174\n",
      "Train Loss: 0.3556669404109319\n",
      "Test Loss: 0.30467087030410767\n",
      "\n",
      "Epoch: 175\n",
      "Train Loss: 0.35477347671985626\n",
      "Test Loss: 0.3002675473690033\n",
      "\n",
      "Epoch: 176\n",
      "Train Loss: 0.3541831523180008\n",
      "Test Loss: 0.3035500645637512\n",
      "\n",
      "Epoch: 177\n",
      "Train Loss: 0.3535398989915848\n",
      "Test Loss: 0.30661773681640625\n",
      "\n",
      "Epoch: 178\n",
      "Train Loss: 0.35925765335559845\n",
      "Test Loss: 0.31117093563079834\n",
      "\n",
      "Epoch: 179\n",
      "Train Loss: 0.36413603524367016\n",
      "Test Loss: 0.30196860432624817\n",
      "\n",
      "Epoch: 180\n",
      "Train Loss: 0.3567856103181839\n",
      "Test Loss: 0.3075937330722809\n",
      "\n",
      "Epoch: 181\n",
      "Train Loss: 0.3512375553448995\n",
      "Test Loss: 0.30315497517585754\n",
      "\n",
      "Epoch: 182\n",
      "Train Loss: 0.3530152390400569\n",
      "Test Loss: 0.30356264114379883\n",
      "\n",
      "Epoch: 183\n",
      "Train Loss: 0.34997057914733887\n",
      "Test Loss: 0.29590144753456116\n",
      "\n",
      "Epoch: 184\n",
      "Train Loss: 0.35420692960421246\n",
      "Test Loss: 0.30857008695602417\n",
      "\n",
      "Epoch: 185\n",
      "Train Loss: 0.3527277112007141\n",
      "Test Loss: 0.3019106686115265\n",
      "\n",
      "Epoch: 186\n",
      "Train Loss: 0.3530934900045395\n",
      "Test Loss: 0.29888659715652466\n",
      "\n",
      "Epoch: 187\n",
      "Train Loss: 0.35624560713768005\n",
      "Test Loss: 0.30077627301216125\n",
      "\n",
      "Epoch: 188\n",
      "Train Loss: 0.35594000418980914\n",
      "Test Loss: 0.29992589354515076\n",
      "\n",
      "Epoch: 189\n",
      "Train Loss: 0.35320088764031726\n",
      "Test Loss: 0.30455341935157776\n",
      "\n",
      "Epoch: 190\n",
      "Train Loss: 0.3520873039960861\n",
      "Test Loss: 0.2977413237094879\n",
      "\n",
      "Epoch: 191\n",
      "Train Loss: 0.3496439605951309\n",
      "Test Loss: 0.30016955733299255\n",
      "\n",
      "Epoch: 192\n",
      "Train Loss: 0.35399221380551654\n",
      "Test Loss: 0.3016335368156433\n",
      "\n",
      "Epoch: 193\n",
      "Train Loss: 0.34822575747966766\n",
      "Test Loss: 0.3032360374927521\n",
      "\n",
      "Epoch: 194\n",
      "Train Loss: 0.34289613862832385\n",
      "Test Loss: 0.30186527967453003\n",
      "\n",
      "Epoch: 195\n",
      "Train Loss: 0.35916196803251904\n",
      "Test Loss: 0.2969015836715698\n",
      "\n",
      "Epoch: 196\n",
      "Train Loss: 0.3555272966623306\n",
      "Test Loss: 0.29934847354888916\n",
      "\n",
      "Epoch: 197\n",
      "Train Loss: 0.3486425230900447\n",
      "Test Loss: 0.29720091819763184\n",
      "\n",
      "Epoch: 198\n",
      "Train Loss: 0.35058265924453735\n",
      "Test Loss: 0.29943782091140747\n",
      "\n",
      "Epoch: 199\n",
      "Train Loss: 0.3507196654876073\n",
      "Test Loss: 0.2993474304676056\n",
      "\n",
      "Epoch: 200\n",
      "Train Loss: 0.3504131535689036\n",
      "Test Loss: 0.29795852303504944\n",
      "\n",
      "Epoch: 201\n",
      "Train Loss: 0.3521839479605357\n",
      "Test Loss: 0.30383098125457764\n",
      "\n",
      "Epoch: 202\n",
      "Train Loss: 0.3432370275259018\n",
      "Test Loss: 0.30794915556907654\n",
      "\n",
      "Epoch: 203\n",
      "Train Loss: 0.3494994391997655\n",
      "Test Loss: 0.3017416000366211\n",
      "\n",
      "Epoch: 204\n",
      "Train Loss: 0.3495640158653259\n",
      "Test Loss: 0.29588979482650757\n",
      "\n",
      "Epoch: 205\n",
      "Train Loss: 0.34719906747341156\n",
      "Test Loss: 0.3013310730457306\n",
      "\n",
      "Epoch: 206\n",
      "Train Loss: 0.3415905435880025\n",
      "Test Loss: 0.3001198172569275\n",
      "\n",
      "Epoch: 207\n",
      "Train Loss: 0.3499348411957423\n",
      "Test Loss: 0.299340158700943\n",
      "\n",
      "Epoch: 208\n",
      "Train Loss: 0.34551513691743213\n",
      "Test Loss: 0.30281829833984375\n",
      "\n",
      "Epoch: 209\n",
      "Train Loss: 0.35210930307706195\n",
      "Test Loss: 0.29523277282714844\n",
      "\n",
      "Epoch: 210\n",
      "Train Loss: 0.346268430352211\n",
      "Test Loss: 0.2958086431026459\n",
      "\n",
      "Epoch: 211\n",
      "Train Loss: 0.34981532394886017\n",
      "Test Loss: 0.29990413784980774\n",
      "\n",
      "Epoch: 212\n",
      "Train Loss: 0.34800780812899273\n",
      "Test Loss: 0.29301923513412476\n",
      "\n",
      "Epoch: 213\n",
      "Train Loss: 0.34748712678750354\n",
      "Test Loss: 0.2974778711795807\n",
      "\n",
      "Epoch: 214\n",
      "Train Loss: 0.3449159363905589\n",
      "Test Loss: 0.3037479817867279\n",
      "\n",
      "Epoch: 215\n",
      "Train Loss: 0.34301339089870453\n",
      "Test Loss: 0.29491570591926575\n",
      "\n",
      "Epoch: 216\n",
      "Train Loss: 0.34241361916065216\n",
      "Test Loss: 0.29266980290412903\n",
      "\n",
      "Epoch: 217\n",
      "Train Loss: 0.3484756449858348\n",
      "Test Loss: 0.29526612162590027\n",
      "\n",
      "Epoch: 218\n",
      "Train Loss: 0.3469166159629822\n",
      "Test Loss: 0.2900243103504181\n",
      "\n",
      "Epoch: 219\n",
      "Train Loss: 0.3458404888709386\n",
      "Test Loss: 0.29347458481788635\n",
      "\n",
      "Epoch: 220\n",
      "Train Loss: 0.34894288579622906\n",
      "Test Loss: 0.2948782742023468\n",
      "\n",
      "Epoch: 221\n",
      "Train Loss: 0.3402773290872574\n",
      "Test Loss: 0.29469114542007446\n",
      "\n",
      "Epoch: 222\n",
      "Train Loss: 0.33997754752635956\n",
      "Test Loss: 0.295775830745697\n",
      "\n",
      "Epoch: 223\n",
      "Train Loss: 0.34319843848546344\n",
      "Test Loss: 0.29849427938461304\n",
      "\n",
      "Epoch: 224\n",
      "Train Loss: 0.3506345748901367\n",
      "Test Loss: 0.2923196256160736\n",
      "\n",
      "Epoch: 225\n",
      "Train Loss: 0.3485021193822225\n",
      "Test Loss: 0.2974855303764343\n",
      "\n",
      "Epoch: 226\n",
      "Train Loss: 0.34668779373168945\n",
      "Test Loss: 0.2925260365009308\n",
      "\n",
      "Epoch: 227\n",
      "Train Loss: 0.3387093146642049\n",
      "Test Loss: 0.29252856969833374\n",
      "\n",
      "Epoch: 228\n",
      "Train Loss: 0.34553829332192737\n",
      "Test Loss: 0.2980215847492218\n",
      "\n",
      "Epoch: 229\n",
      "Train Loss: 0.3379868119955063\n",
      "Test Loss: 0.30011075735092163\n",
      "\n",
      "Epoch: 230\n",
      "Train Loss: 0.34294218321641284\n",
      "Test Loss: 0.3002901077270508\n",
      "\n",
      "Epoch: 231\n",
      "Train Loss: 0.341265673438708\n",
      "Test Loss: 0.3086487948894501\n",
      "\n",
      "Epoch: 232\n",
      "Train Loss: 0.3441159625848134\n",
      "Test Loss: 0.29241859912872314\n",
      "\n",
      "Epoch: 233\n",
      "Train Loss: 0.3419849673906962\n",
      "Test Loss: 0.29659074544906616\n",
      "\n",
      "Epoch: 234\n",
      "Train Loss: 0.3427559832731883\n",
      "Test Loss: 0.29024288058280945\n",
      "\n",
      "Epoch: 235\n",
      "Train Loss: 0.34388825794061023\n",
      "Test Loss: 0.29453203082084656\n",
      "\n",
      "Epoch: 236\n",
      "Train Loss: 0.33926405509312946\n",
      "Test Loss: 0.2921394407749176\n",
      "\n",
      "Epoch: 237\n",
      "Train Loss: 0.34394759436448413\n",
      "Test Loss: 0.2879382073879242\n",
      "\n",
      "Epoch: 238\n",
      "Train Loss: 0.339072123169899\n",
      "Test Loss: 0.29474055767059326\n",
      "\n",
      "Epoch: 239\n",
      "Train Loss: 0.33932991822560626\n",
      "Test Loss: 0.29768508672714233\n",
      "\n",
      "Epoch: 240\n",
      "Train Loss: 0.34112580120563507\n",
      "Test Loss: 0.2976723909378052\n",
      "\n",
      "Epoch: 241\n",
      "Train Loss: 0.34433407088120777\n",
      "Test Loss: 0.2899481952190399\n",
      "\n",
      "Epoch: 242\n",
      "Train Loss: 0.34783605734507245\n",
      "Test Loss: 0.2915615439414978\n",
      "\n",
      "Epoch: 243\n",
      "Train Loss: 0.3424711922804515\n",
      "Test Loss: 0.2937602400779724\n",
      "\n",
      "Epoch: 244\n",
      "Train Loss: 0.3426085561513901\n",
      "Test Loss: 0.29151690006256104\n",
      "\n",
      "Epoch: 245\n",
      "Train Loss: 0.3406073848406474\n",
      "Test Loss: 0.29296374320983887\n",
      "\n",
      "Epoch: 246\n",
      "Train Loss: 0.3416181405385335\n",
      "Test Loss: 0.29884907603263855\n",
      "\n",
      "Epoch: 247\n",
      "Train Loss: 0.339873731136322\n",
      "Test Loss: 0.29152870178222656\n",
      "\n",
      "Epoch: 248\n",
      "Train Loss: 0.33668410281340283\n",
      "Test Loss: 0.2967137098312378\n",
      "\n",
      "Epoch: 249\n",
      "Train Loss: 0.3377821097771327\n",
      "Test Loss: 0.2983032763004303\n",
      "\n",
      "Epoch: 250\n",
      "Train Loss: 0.3353693087895711\n",
      "Test Loss: 0.28939172625541687\n",
      "\n",
      "Epoch: 251\n",
      "Train Loss: 0.3406863957643509\n",
      "Test Loss: 0.2970784306526184\n",
      "\n",
      "Epoch: 252\n",
      "Train Loss: 0.33916446566581726\n",
      "Test Loss: 0.2906917631626129\n",
      "\n",
      "Epoch: 253\n",
      "Train Loss: 0.34053897857666016\n",
      "Test Loss: 0.29449883103370667\n",
      "\n",
      "Epoch: 254\n",
      "Train Loss: 0.3409365763266881\n",
      "Test Loss: 0.29210203886032104\n",
      "\n",
      "Epoch: 255\n",
      "Train Loss: 0.3394506573677063\n",
      "Test Loss: 0.3005133271217346\n",
      "\n",
      "Epoch: 256\n",
      "Train Loss: 0.33837245404720306\n",
      "Test Loss: 0.2926318943500519\n",
      "\n",
      "Epoch: 257\n",
      "Train Loss: 0.3403764267762502\n",
      "Test Loss: 0.2934575378894806\n",
      "\n",
      "Epoch: 258\n",
      "Train Loss: 0.3385254144668579\n",
      "Test Loss: 0.2926449477672577\n",
      "\n",
      "Epoch: 259\n",
      "Train Loss: 0.3366682877143224\n",
      "Test Loss: 0.2923022508621216\n",
      "\n",
      "Epoch: 260\n",
      "Train Loss: 0.34025517602761585\n",
      "Test Loss: 0.29657843708992004\n",
      "\n",
      "Epoch: 261\n",
      "Train Loss: 0.3366626699765523\n",
      "Test Loss: 0.29280728101730347\n",
      "\n",
      "Epoch: 262\n",
      "Train Loss: 0.33409005403518677\n",
      "Test Loss: 0.2940118610858917\n",
      "\n",
      "Epoch: 263\n",
      "Train Loss: 0.3360499193271001\n",
      "Test Loss: 0.295815110206604\n",
      "\n",
      "Epoch: 264\n",
      "Train Loss: 0.3393947680791219\n",
      "Test Loss: 0.2901669442653656\n",
      "\n",
      "Epoch: 265\n",
      "Train Loss: 0.33564900358517963\n",
      "Test Loss: 0.29633039236068726\n",
      "\n",
      "Epoch: 266\n",
      "Train Loss: 0.33381084104379016\n",
      "Test Loss: 0.290134072303772\n",
      "\n",
      "Epoch: 267\n",
      "Train Loss: 0.3354560434818268\n",
      "Test Loss: 0.2925838828086853\n",
      "\n",
      "Epoch: 268\n",
      "Train Loss: 0.33376351992289227\n",
      "Test Loss: 0.29406237602233887\n",
      "\n",
      "Epoch: 269\n",
      "Train Loss: 0.3364521563053131\n",
      "Test Loss: 0.2933782637119293\n",
      "\n",
      "Epoch: 270\n",
      "Train Loss: 0.33727672696113586\n",
      "Test Loss: 0.2897583246231079\n",
      "\n",
      "Epoch: 271\n",
      "Train Loss: 0.33096611499786377\n",
      "Test Loss: 0.28711801767349243\n",
      "\n",
      "Epoch: 272\n",
      "Train Loss: 0.33336034913857776\n",
      "Test Loss: 0.2936919927597046\n",
      "\n",
      "Epoch: 273\n",
      "Train Loss: 0.3424654280145963\n",
      "Test Loss: 0.2911851704120636\n",
      "\n",
      "Epoch: 274\n",
      "Train Loss: 0.33259545266628265\n",
      "Test Loss: 0.2963049113750458\n",
      "\n",
      "Epoch: 275\n",
      "Train Loss: 0.3373791128396988\n",
      "Test Loss: 0.30126115679740906\n",
      "\n",
      "Epoch: 276\n",
      "Train Loss: 0.33423012495040894\n",
      "Test Loss: 0.3054274022579193\n",
      "\n",
      "Epoch: 277\n",
      "Train Loss: 0.3364865630865097\n",
      "Test Loss: 0.2891536355018616\n",
      "\n",
      "Epoch: 278\n",
      "Train Loss: 0.33913011352221173\n",
      "Test Loss: 0.2911425232887268\n",
      "\n",
      "Epoch: 279\n",
      "Train Loss: 0.3316635737816493\n",
      "Test Loss: 0.28869229555130005\n",
      "\n",
      "Epoch: 280\n",
      "Train Loss: 0.3320853263139725\n",
      "Test Loss: 0.284921258687973\n",
      "\n",
      "Epoch: 281\n",
      "Train Loss: 0.3367324322462082\n",
      "Test Loss: 0.28317898511886597\n",
      "\n",
      "Epoch: 282\n",
      "Train Loss: 0.34052950143814087\n",
      "Test Loss: 0.2828504741191864\n",
      "\n",
      "Epoch: 283\n",
      "Train Loss: 0.33929672340552014\n",
      "Test Loss: 0.29001563787460327\n",
      "\n",
      "Epoch: 284\n",
      "Train Loss: 0.32950082421302795\n",
      "Test Loss: 0.2865007221698761\n",
      "\n",
      "Epoch: 285\n",
      "Train Loss: 0.33970403174559277\n",
      "Test Loss: 0.28986409306526184\n",
      "\n",
      "Epoch: 286\n",
      "Train Loss: 0.33213600019613904\n",
      "Test Loss: 0.2876412272453308\n",
      "\n",
      "Epoch: 287\n",
      "Train Loss: 0.3335081587235133\n",
      "Test Loss: 0.288931667804718\n",
      "\n",
      "Epoch: 288\n",
      "Train Loss: 0.32964709401130676\n",
      "Test Loss: 0.2846079170703888\n",
      "\n",
      "Epoch: 289\n",
      "Train Loss: 0.3341810554265976\n",
      "Test Loss: 0.28429070115089417\n",
      "\n",
      "Epoch: 290\n",
      "Train Loss: 0.33143221338589984\n",
      "Test Loss: 0.28345322608947754\n",
      "\n",
      "Epoch: 291\n",
      "Train Loss: 0.3280891726414363\n",
      "Test Loss: 0.28739798069000244\n",
      "\n",
      "Epoch: 292\n",
      "Train Loss: 0.3303395211696625\n",
      "Test Loss: 0.28324317932128906\n",
      "\n",
      "Epoch: 293\n",
      "Train Loss: 0.33323990801970166\n",
      "Test Loss: 0.2890499532222748\n",
      "\n",
      "Epoch: 294\n",
      "Train Loss: 0.3290083408355713\n",
      "Test Loss: 0.28276002407073975\n",
      "\n",
      "Epoch: 295\n",
      "Train Loss: 0.3298157701889674\n",
      "Test Loss: 0.291574090719223\n",
      "\n",
      "Epoch: 296\n",
      "Train Loss: 0.32708246012528736\n",
      "Test Loss: 0.28540265560150146\n",
      "\n",
      "Epoch: 297\n",
      "Train Loss: 0.32771719992160797\n",
      "Test Loss: 0.28432953357696533\n",
      "\n",
      "Epoch: 298\n",
      "Train Loss: 0.325872078537941\n",
      "Test Loss: 0.2827305793762207\n",
      "\n",
      "Epoch: 299\n",
      "Train Loss: 0.3263322015603383\n",
      "Test Loss: 0.288979172706604\n",
      "\n",
      "Epoch: 300\n",
      "Train Loss: 0.3317980468273163\n",
      "Test Loss: 0.2806704640388489\n",
      "\n",
      "Epoch: 301\n",
      "Train Loss: 0.33182210723559064\n",
      "Test Loss: 0.2881162464618683\n",
      "\n",
      "Epoch: 302\n",
      "Train Loss: 0.3278094033400218\n",
      "Test Loss: 0.2873916029930115\n",
      "\n",
      "Epoch: 303\n",
      "Train Loss: 0.3318599760532379\n",
      "Test Loss: 0.286164253950119\n",
      "\n",
      "Epoch: 304\n",
      "Train Loss: 0.3307739645242691\n",
      "Test Loss: 0.28884702920913696\n",
      "\n",
      "Epoch: 305\n",
      "Train Loss: 0.3279073238372803\n",
      "Test Loss: 0.2868957817554474\n",
      "\n",
      "Epoch: 306\n",
      "Train Loss: 0.3237355550130208\n",
      "Test Loss: 0.2854188084602356\n",
      "\n",
      "Epoch: 307\n",
      "Train Loss: 0.3270440498987834\n",
      "Test Loss: 0.28294849395751953\n",
      "\n",
      "Epoch: 308\n",
      "Train Loss: 0.33047813425461453\n",
      "Test Loss: 0.2823711633682251\n",
      "\n",
      "Epoch: 309\n",
      "Train Loss: 0.3298296978076299\n",
      "Test Loss: 0.28531089425086975\n",
      "\n",
      "Epoch: 310\n",
      "Train Loss: 0.3303161958853404\n",
      "Test Loss: 0.28039082884788513\n",
      "\n",
      "Epoch: 311\n",
      "Train Loss: 0.3280247300863266\n",
      "Test Loss: 0.2847263216972351\n",
      "\n",
      "Epoch: 312\n",
      "Train Loss: 0.32179001967112225\n",
      "Test Loss: 0.28366491198539734\n",
      "\n",
      "Epoch: 313\n",
      "Train Loss: 0.3318547358115514\n",
      "Test Loss: 0.28883659839630127\n",
      "\n",
      "Epoch: 314\n",
      "Train Loss: 0.32526082793871564\n",
      "Test Loss: 0.29383713006973267\n",
      "\n",
      "Epoch: 315\n",
      "Train Loss: 0.32719190418720245\n",
      "Test Loss: 0.27891793847084045\n",
      "\n",
      "Epoch: 316\n",
      "Train Loss: 0.3306504835685094\n",
      "Test Loss: 0.2848762273788452\n",
      "\n",
      "Epoch: 317\n",
      "Train Loss: 0.3271346737941106\n",
      "Test Loss: 0.2843892574310303\n",
      "\n",
      "Epoch: 318\n",
      "Train Loss: 0.32839468121528625\n",
      "Test Loss: 0.27752208709716797\n",
      "\n",
      "Epoch: 319\n",
      "Train Loss: 0.3264537851015727\n",
      "Test Loss: 0.2829601764678955\n",
      "\n",
      "Epoch: 320\n",
      "Train Loss: 0.33213910460472107\n",
      "Test Loss: 0.2757020592689514\n",
      "\n",
      "Epoch: 321\n",
      "Train Loss: 0.3304306815067927\n",
      "Test Loss: 0.2778331935405731\n",
      "\n",
      "Epoch: 322\n",
      "Train Loss: 0.326075146595637\n",
      "Test Loss: 0.28057825565338135\n",
      "\n",
      "Epoch: 323\n",
      "Train Loss: 0.3344441701968511\n",
      "Test Loss: 0.28283950686454773\n",
      "\n",
      "Epoch: 324\n",
      "Train Loss: 0.3258518651127815\n",
      "Test Loss: 0.2852853536605835\n",
      "\n",
      "Epoch: 325\n",
      "Train Loss: 0.3293113360802333\n",
      "Test Loss: 0.27686333656311035\n",
      "\n",
      "Epoch: 326\n",
      "Train Loss: 0.32879961530367535\n",
      "Test Loss: 0.278762549161911\n",
      "\n",
      "Epoch: 327\n",
      "Train Loss: 0.32985371847947437\n",
      "Test Loss: 0.281462162733078\n",
      "\n",
      "Epoch: 328\n",
      "Train Loss: 0.32308084269364673\n",
      "Test Loss: 0.2920112907886505\n",
      "\n",
      "Epoch: 329\n",
      "Train Loss: 0.3277144432067871\n",
      "Test Loss: 0.28416895866394043\n",
      "\n",
      "Epoch: 330\n",
      "Train Loss: 0.3278912603855133\n",
      "Test Loss: 0.2849259078502655\n",
      "\n",
      "Epoch: 331\n",
      "Train Loss: 0.3283601353565852\n",
      "Test Loss: 0.28370025753974915\n",
      "\n",
      "Epoch: 332\n",
      "Train Loss: 0.32475953300793964\n",
      "Test Loss: 0.28417468070983887\n",
      "\n",
      "Epoch: 333\n",
      "Train Loss: 0.3241090029478073\n",
      "Test Loss: 0.2802395522594452\n",
      "\n",
      "Epoch: 334\n",
      "Train Loss: 0.3258742441733678\n",
      "Test Loss: 0.28185752034187317\n",
      "\n",
      "Epoch: 335\n",
      "Train Loss: 0.32497333983580273\n",
      "Test Loss: 0.28200608491897583\n",
      "\n",
      "Epoch: 336\n",
      "Train Loss: 0.32354674736658734\n",
      "Test Loss: 0.2821839451789856\n",
      "\n",
      "Epoch: 337\n",
      "Train Loss: 0.3154684205849965\n",
      "Test Loss: 0.28014257550239563\n",
      "\n",
      "Epoch: 338\n",
      "Train Loss: 0.3222753554582596\n",
      "Test Loss: 0.27795886993408203\n",
      "\n",
      "Epoch: 339\n",
      "Train Loss: 0.32601353029410046\n",
      "Test Loss: 0.28661325573921204\n",
      "\n",
      "Epoch: 340\n",
      "Train Loss: 0.326006884376208\n",
      "Test Loss: 0.2788518965244293\n",
      "\n",
      "Epoch: 341\n",
      "Train Loss: 0.3298302044471105\n",
      "Test Loss: 0.28345340490341187\n",
      "\n",
      "Epoch: 342\n",
      "Train Loss: 0.32375705738862354\n",
      "Test Loss: 0.2809806168079376\n",
      "\n",
      "Epoch: 343\n",
      "Train Loss: 0.3293299873669942\n",
      "Test Loss: 0.28064605593681335\n",
      "\n",
      "Epoch: 344\n",
      "Train Loss: 0.326100821296374\n",
      "Test Loss: 0.28010696172714233\n",
      "\n",
      "Epoch: 345\n",
      "Train Loss: 0.3250551074743271\n",
      "Test Loss: 0.2824995815753937\n",
      "\n",
      "Epoch: 346\n",
      "Train Loss: 0.32237684230009717\n",
      "Test Loss: 0.28231677412986755\n",
      "\n",
      "Epoch: 347\n",
      "Train Loss: 0.3187008624275525\n",
      "Test Loss: 0.2879628837108612\n",
      "\n",
      "Epoch: 348\n",
      "Train Loss: 0.31816095610459644\n",
      "Test Loss: 0.28143948316574097\n",
      "\n",
      "Epoch: 349\n",
      "Train Loss: 0.32558324684699375\n",
      "Test Loss: 0.2853730618953705\n",
      "\n",
      "Epoch: 350\n",
      "Train Loss: 0.3191562642653783\n",
      "Test Loss: 0.2822742462158203\n",
      "\n",
      "Epoch: 351\n",
      "Train Loss: 0.3233703722556432\n",
      "Test Loss: 0.2795058786869049\n",
      "\n",
      "Epoch: 352\n",
      "Train Loss: 0.32672762374083203\n",
      "Test Loss: 0.28196778893470764\n",
      "\n",
      "Epoch: 353\n",
      "Train Loss: 0.321890726685524\n",
      "Test Loss: 0.2795426845550537\n",
      "\n",
      "Epoch: 354\n",
      "Train Loss: 0.32329828043778736\n",
      "Test Loss: 0.2759367823600769\n",
      "\n",
      "Epoch: 355\n",
      "Train Loss: 0.328085795044899\n",
      "Test Loss: 0.27926716208457947\n",
      "\n",
      "Epoch: 356\n",
      "Train Loss: 0.323485145966212\n",
      "Test Loss: 0.2773553431034088\n",
      "\n",
      "Epoch: 357\n",
      "Train Loss: 0.3232476885120074\n",
      "Test Loss: 0.27926698327064514\n",
      "\n",
      "Epoch: 358\n",
      "Train Loss: 0.31720391909281415\n",
      "Test Loss: 0.2830715477466583\n",
      "\n",
      "Epoch: 359\n",
      "Train Loss: 0.32158706585566205\n",
      "Test Loss: 0.2848169207572937\n",
      "\n",
      "Epoch: 360\n",
      "Train Loss: 0.32333109776179\n",
      "Test Loss: 0.2894687056541443\n",
      "\n",
      "Epoch: 361\n",
      "Train Loss: 0.32275482018788654\n",
      "Test Loss: 0.2832633852958679\n",
      "\n",
      "Epoch: 362\n",
      "Train Loss: 0.3230533053477605\n",
      "Test Loss: 0.2825234830379486\n",
      "\n",
      "Epoch: 363\n",
      "Train Loss: 0.32260256508986157\n",
      "Test Loss: 0.27878960967063904\n",
      "\n",
      "Epoch: 364\n",
      "Train Loss: 0.3232078601916631\n",
      "Test Loss: 0.27994951605796814\n",
      "\n",
      "Epoch: 365\n",
      "Train Loss: 0.3218083729346593\n",
      "Test Loss: 0.28215405344963074\n",
      "\n",
      "Epoch: 366\n",
      "Train Loss: 0.318050779402256\n",
      "Test Loss: 0.28484177589416504\n",
      "\n",
      "Epoch: 367\n",
      "Train Loss: 0.3207507034142812\n",
      "Test Loss: 0.2829799950122833\n",
      "\n",
      "Epoch: 368\n",
      "Train Loss: 0.3188674698273341\n",
      "Test Loss: 0.2785187363624573\n",
      "\n",
      "Epoch: 369\n",
      "Train Loss: 0.32310082018375397\n",
      "Test Loss: 0.2762346863746643\n",
      "\n",
      "Epoch: 370\n",
      "Train Loss: 0.31832873324553174\n",
      "Test Loss: 0.2791173458099365\n",
      "\n",
      "Epoch: 371\n",
      "Train Loss: 0.3200932443141937\n",
      "Test Loss: 0.27803266048431396\n",
      "\n",
      "Epoch: 372\n",
      "Train Loss: 0.3187894821166992\n",
      "Test Loss: 0.2788226008415222\n",
      "\n",
      "Epoch: 373\n",
      "Train Loss: 0.32124442358811695\n",
      "Test Loss: 0.2742919325828552\n",
      "\n",
      "Epoch: 374\n",
      "Train Loss: 0.3134772131840388\n",
      "Test Loss: 0.2781312167644501\n",
      "\n",
      "Epoch: 375\n",
      "Train Loss: 0.3150910884141922\n",
      "Test Loss: 0.2792187035083771\n",
      "\n",
      "Epoch: 376\n",
      "Train Loss: 0.3136449009180069\n",
      "Test Loss: 0.276583194732666\n",
      "\n",
      "Epoch: 377\n",
      "Train Loss: 0.31740622222423553\n",
      "Test Loss: 0.2734218239784241\n",
      "\n",
      "Epoch: 378\n",
      "Train Loss: 0.32088929414749146\n",
      "Test Loss: 0.2846505045890808\n",
      "\n",
      "Epoch: 379\n",
      "Train Loss: 0.31867919365564984\n",
      "Test Loss: 0.27336734533309937\n",
      "\n",
      "Epoch: 380\n",
      "Train Loss: 0.32177673776944477\n",
      "Test Loss: 0.28284090757369995\n",
      "\n",
      "Epoch: 381\n",
      "Train Loss: 0.3209569106499354\n",
      "Test Loss: 0.2794800400733948\n",
      "\n",
      "Epoch: 382\n",
      "Train Loss: 0.31579020122687024\n",
      "Test Loss: 0.2874615788459778\n",
      "\n",
      "Epoch: 383\n",
      "Train Loss: 0.31992173939943314\n",
      "Test Loss: 0.27573877573013306\n",
      "\n",
      "Epoch: 384\n",
      "Train Loss: 0.3176566958427429\n",
      "Test Loss: 0.28088614344596863\n",
      "\n",
      "Epoch: 385\n",
      "Train Loss: 0.3183164745569229\n",
      "Test Loss: 0.2787556052207947\n",
      "\n",
      "Epoch: 386\n",
      "Train Loss: 0.3263566146294276\n",
      "Test Loss: 0.2782770097255707\n",
      "\n",
      "Epoch: 387\n",
      "Train Loss: 0.3177048787474632\n",
      "Test Loss: 0.27845293283462524\n",
      "\n",
      "Epoch: 388\n",
      "Train Loss: 0.31854649384816486\n",
      "Test Loss: 0.280841201543808\n",
      "\n",
      "Epoch: 389\n",
      "Train Loss: 0.31646187355120975\n",
      "Test Loss: 0.27857351303100586\n",
      "\n",
      "Epoch: 390\n",
      "Train Loss: 0.3135451277097066\n",
      "Test Loss: 0.27122604846954346\n",
      "\n",
      "Epoch: 391\n",
      "Train Loss: 0.3187095920244853\n",
      "Test Loss: 0.27585601806640625\n",
      "\n",
      "Epoch: 392\n",
      "Train Loss: 0.3229013532400131\n",
      "Test Loss: 0.2739858627319336\n",
      "\n",
      "Epoch: 393\n",
      "Train Loss: 0.3099066416422526\n",
      "Test Loss: 0.2746831178665161\n",
      "\n",
      "Epoch: 394\n",
      "Train Loss: 0.31480249265829724\n",
      "Test Loss: 0.2767980098724365\n",
      "\n",
      "Epoch: 395\n",
      "Train Loss: 0.31384385873874027\n",
      "Test Loss: 0.27786344289779663\n",
      "\n",
      "Epoch: 396\n",
      "Train Loss: 0.3153915802637736\n",
      "Test Loss: 0.2744559943675995\n",
      "\n",
      "Epoch: 397\n",
      "Train Loss: 0.31502586603164673\n",
      "Test Loss: 0.27304619550704956\n",
      "\n",
      "Epoch: 398\n",
      "Train Loss: 0.31521060566107434\n",
      "Test Loss: 0.2730467617511749\n",
      "\n",
      "Epoch: 399\n",
      "Train Loss: 0.3144794702529907\n",
      "Test Loss: 0.27418702840805054\n",
      "\n",
      "Epoch: 400\n",
      "Train Loss: 0.3088523993889491\n",
      "Test Loss: 0.28287315368652344\n",
      "\n",
      "Epoch: 401\n",
      "Train Loss: 0.31682782371838886\n",
      "Test Loss: 0.2762291133403778\n",
      "\n",
      "Epoch: 402\n",
      "Train Loss: 0.3141533186038335\n",
      "Test Loss: 0.27608397603034973\n",
      "\n",
      "Epoch: 403\n",
      "Train Loss: 0.31642167270183563\n",
      "Test Loss: 0.27465230226516724\n",
      "\n",
      "Epoch: 404\n",
      "Train Loss: 0.31133397420247394\n",
      "Test Loss: 0.2794228494167328\n",
      "\n",
      "Epoch: 405\n",
      "Train Loss: 0.3169528196255366\n",
      "Test Loss: 0.27112069725990295\n",
      "\n",
      "Epoch: 406\n",
      "Train Loss: 0.31243422627449036\n",
      "Test Loss: 0.28271952271461487\n",
      "\n",
      "Epoch: 407\n",
      "Train Loss: 0.31385080764691037\n",
      "Test Loss: 0.27421820163726807\n",
      "\n",
      "Epoch: 408\n",
      "Train Loss: 0.31736041108767193\n",
      "Test Loss: 0.2786700427532196\n",
      "\n",
      "Epoch: 409\n",
      "Train Loss: 0.3135792265335719\n",
      "Test Loss: 0.2790762186050415\n",
      "\n",
      "Epoch: 410\n",
      "Train Loss: 0.315015306075414\n",
      "Test Loss: 0.2786986231803894\n",
      "\n",
      "Epoch: 411\n",
      "Train Loss: 0.3169424583514531\n",
      "Test Loss: 0.2760795056819916\n",
      "\n",
      "Epoch: 412\n",
      "Train Loss: 0.3110884502530098\n",
      "Test Loss: 0.2828713655471802\n",
      "\n",
      "Epoch: 413\n",
      "Train Loss: 0.31353987008333206\n",
      "Test Loss: 0.27736881375312805\n",
      "\n",
      "Epoch: 414\n",
      "Train Loss: 0.3138941079378128\n",
      "Test Loss: 0.2721770107746124\n",
      "\n",
      "Epoch: 415\n",
      "Train Loss: 0.3107923741141955\n",
      "Test Loss: 0.2800661027431488\n",
      "\n",
      "Epoch: 416\n",
      "Train Loss: 0.3103695561488469\n",
      "Test Loss: 0.268748939037323\n",
      "\n",
      "Epoch: 417\n",
      "Train Loss: 0.31402766207853955\n",
      "Test Loss: 0.272051602602005\n",
      "\n",
      "Epoch: 418\n",
      "Train Loss: 0.31567732989788055\n",
      "Test Loss: 0.27209052443504333\n",
      "\n",
      "Epoch: 419\n",
      "Train Loss: 0.31261816372474033\n",
      "Test Loss: 0.27647238969802856\n",
      "\n",
      "Epoch: 420\n",
      "Train Loss: 0.31601287921269733\n",
      "Test Loss: 0.27302834391593933\n",
      "\n",
      "Epoch: 421\n",
      "Train Loss: 0.3108762130141258\n",
      "Test Loss: 0.2653977870941162\n",
      "\n",
      "Epoch: 422\n",
      "Train Loss: 0.3126982202132543\n",
      "Test Loss: 0.27038976550102234\n",
      "\n",
      "Epoch: 423\n",
      "Train Loss: 0.3089660306771596\n",
      "Test Loss: 0.26596254110336304\n",
      "\n",
      "Epoch: 424\n",
      "Train Loss: 0.31317973136901855\n",
      "Test Loss: 0.27909815311431885\n",
      "\n",
      "Epoch: 425\n",
      "Train Loss: 0.3093516081571579\n",
      "Test Loss: 0.2704680860042572\n",
      "\n",
      "Epoch: 426\n",
      "Train Loss: 0.3121042599280675\n",
      "Test Loss: 0.2688235640525818\n",
      "\n",
      "Epoch: 427\n",
      "Train Loss: 0.30798395971457165\n",
      "Test Loss: 0.2695762515068054\n",
      "\n",
      "Epoch: 428\n",
      "Train Loss: 0.3121723209818204\n",
      "Test Loss: 0.2701391577720642\n",
      "\n",
      "Epoch: 429\n",
      "Train Loss: 0.3118114098906517\n",
      "Test Loss: 0.26942577958106995\n",
      "\n",
      "Epoch: 430\n",
      "Train Loss: 0.3129376570383708\n",
      "Test Loss: 0.2709994614124298\n",
      "\n",
      "Epoch: 431\n",
      "Train Loss: 0.30837416648864746\n",
      "Test Loss: 0.2709106504917145\n",
      "\n",
      "Epoch: 432\n",
      "Train Loss: 0.30738478899002075\n",
      "Test Loss: 0.27240124344825745\n",
      "\n",
      "Epoch: 433\n",
      "Train Loss: 0.3063133309284846\n",
      "Test Loss: 0.27293661236763\n",
      "\n",
      "Epoch: 434\n",
      "Train Loss: 0.31342389931281406\n",
      "Test Loss: 0.2725760042667389\n",
      "\n",
      "Epoch: 435\n",
      "Train Loss: 0.31122657656669617\n",
      "Test Loss: 0.2681656777858734\n",
      "\n",
      "Epoch: 436\n",
      "Train Loss: 0.31173241635163623\n",
      "Test Loss: 0.26971063017845154\n",
      "\n",
      "Epoch: 437\n",
      "Train Loss: 0.3102402438720067\n",
      "Test Loss: 0.2733226418495178\n",
      "\n",
      "Epoch: 438\n",
      "Train Loss: 0.3113675018151601\n",
      "Test Loss: 0.26934558153152466\n",
      "\n",
      "Epoch: 439\n",
      "Train Loss: 0.309872530400753\n",
      "Test Loss: 0.2800534963607788\n",
      "\n",
      "Epoch: 440\n",
      "Train Loss: 0.30766192078590393\n",
      "Test Loss: 0.2748911678791046\n",
      "\n",
      "Epoch: 441\n",
      "Train Loss: 0.3110676482319832\n",
      "Test Loss: 0.2697753310203552\n",
      "\n",
      "Epoch: 442\n",
      "Train Loss: 0.30946899950504303\n",
      "Test Loss: 0.26689279079437256\n",
      "\n",
      "Epoch: 443\n",
      "Train Loss: 0.3098970154921214\n",
      "Test Loss: 0.26773446798324585\n",
      "\n",
      "Epoch: 444\n",
      "Train Loss: 0.31550130993127823\n",
      "Test Loss: 0.2679155766963959\n",
      "\n",
      "Epoch: 445\n",
      "Train Loss: 0.31143346428871155\n",
      "Test Loss: 0.2670275866985321\n",
      "\n",
      "Epoch: 446\n",
      "Train Loss: 0.3093733886877696\n",
      "Test Loss: 0.2713082432746887\n",
      "\n",
      "Epoch: 447\n",
      "Train Loss: 0.3105760415395101\n",
      "Test Loss: 0.2637600898742676\n",
      "\n",
      "Epoch: 448\n",
      "Train Loss: 0.3109271725018819\n",
      "Test Loss: 0.2715432345867157\n",
      "\n",
      "Epoch: 449\n",
      "Train Loss: 0.307025246322155\n",
      "Test Loss: 0.2709674537181854\n",
      "\n",
      "Epoch: 450\n",
      "Train Loss: 0.31190147747596103\n",
      "Test Loss: 0.2686239778995514\n",
      "\n",
      "Epoch: 451\n",
      "Train Loss: 0.30755331615606946\n",
      "Test Loss: 0.2671634554862976\n",
      "\n",
      "Epoch: 452\n",
      "Train Loss: 0.3114601771036784\n",
      "Test Loss: 0.26590263843536377\n",
      "\n",
      "Epoch: 453\n",
      "Train Loss: 0.30746154487133026\n",
      "Test Loss: 0.26965466141700745\n",
      "\n",
      "Epoch: 454\n",
      "Train Loss: 0.3124472349882126\n",
      "Test Loss: 0.2712080180644989\n",
      "\n",
      "Epoch: 455\n",
      "Train Loss: 0.31025249262650806\n",
      "Test Loss: 0.2661589980125427\n",
      "\n",
      "Epoch: 456\n",
      "Train Loss: 0.3060069754719734\n",
      "Test Loss: 0.27423667907714844\n",
      "\n",
      "Epoch: 457\n",
      "Train Loss: 0.30951592326164246\n",
      "Test Loss: 0.26599520444869995\n",
      "\n",
      "Epoch: 458\n",
      "Train Loss: 0.3103965570529302\n",
      "Test Loss: 0.26326823234558105\n",
      "\n",
      "Epoch: 459\n",
      "Train Loss: 0.30599349985520047\n",
      "Test Loss: 0.2633373737335205\n",
      "\n",
      "Epoch: 460\n",
      "Train Loss: 0.3007637709379196\n",
      "Test Loss: 0.2718776762485504\n",
      "\n",
      "Epoch: 461\n",
      "Train Loss: 0.3026934340596199\n",
      "Test Loss: 0.2670818269252777\n",
      "\n",
      "Epoch: 462\n",
      "Train Loss: 0.3028787150979042\n",
      "Test Loss: 0.26870301365852356\n",
      "\n",
      "Epoch: 463\n",
      "Train Loss: 0.3045746684074402\n",
      "Test Loss: 0.26398348808288574\n",
      "\n",
      "Epoch: 464\n",
      "Train Loss: 0.3041452020406723\n",
      "Test Loss: 0.2680370509624481\n",
      "\n",
      "Epoch: 465\n",
      "Train Loss: 0.3049627145131429\n",
      "Test Loss: 0.26416683197021484\n",
      "\n",
      "Epoch: 466\n",
      "Train Loss: 0.30712411801020306\n",
      "Test Loss: 0.26440754532814026\n",
      "\n",
      "Epoch: 467\n",
      "Train Loss: 0.30581610401471454\n",
      "Test Loss: 0.2645193636417389\n",
      "\n",
      "Epoch: 468\n",
      "Train Loss: 0.30638223389784497\n",
      "Test Loss: 0.2628563344478607\n",
      "\n",
      "Epoch: 469\n",
      "Train Loss: 0.3037890742222468\n",
      "Test Loss: 0.2637474834918976\n",
      "\n",
      "Epoch: 470\n",
      "Train Loss: 0.30330203225215274\n",
      "Test Loss: 0.2629197835922241\n",
      "\n",
      "Epoch: 471\n",
      "Train Loss: 0.3043757552901904\n",
      "Test Loss: 0.266551673412323\n",
      "\n",
      "Epoch: 472\n",
      "Train Loss: 0.30370231221119565\n",
      "Test Loss: 0.2652561366558075\n",
      "\n",
      "Epoch: 473\n",
      "Train Loss: 0.3040548687179883\n",
      "Test Loss: 0.2639048993587494\n",
      "\n",
      "Epoch: 474\n",
      "Train Loss: 0.30704569568236667\n",
      "Test Loss: 0.2695161998271942\n",
      "\n",
      "Epoch: 475\n",
      "Train Loss: 0.3050171236197154\n",
      "Test Loss: 0.2699650526046753\n",
      "\n",
      "Epoch: 476\n",
      "Train Loss: 0.29958922415971756\n",
      "Test Loss: 0.2682693302631378\n",
      "\n",
      "Epoch: 477\n",
      "Train Loss: 0.30611558506886166\n",
      "Test Loss: 0.27011722326278687\n",
      "\n",
      "Epoch: 478\n",
      "Train Loss: 0.30632716914017993\n",
      "Test Loss: 0.26109686493873596\n",
      "\n",
      "Epoch: 479\n",
      "Train Loss: 0.3054625665148099\n",
      "Test Loss: 0.2677813768386841\n",
      "\n",
      "Epoch: 480\n",
      "Train Loss: 0.30557161072889966\n",
      "Test Loss: 0.2610723376274109\n",
      "\n",
      "Epoch: 481\n",
      "Train Loss: 0.30325449506441754\n",
      "Test Loss: 0.2704172134399414\n",
      "\n",
      "Epoch: 482\n",
      "Train Loss: 0.306076059738795\n",
      "Test Loss: 0.27951791882514954\n",
      "\n",
      "Epoch: 483\n",
      "Train Loss: 0.3027128850420316\n",
      "Test Loss: 0.28075405955314636\n",
      "\n",
      "Epoch: 484\n",
      "Train Loss: 0.2937656044960022\n",
      "Test Loss: 0.26124852895736694\n",
      "\n",
      "Epoch: 485\n",
      "Train Loss: 0.29769591490427655\n",
      "Test Loss: 0.2629360258579254\n",
      "\n",
      "Epoch: 486\n",
      "Train Loss: 0.30179530133803684\n",
      "Test Loss: 0.2618325352668762\n",
      "\n",
      "Epoch: 487\n",
      "Train Loss: 0.3007671708861987\n",
      "Test Loss: 0.2598477602005005\n",
      "\n",
      "Epoch: 488\n",
      "Train Loss: 0.30642269551754\n",
      "Test Loss: 0.2679211497306824\n",
      "\n",
      "Epoch: 489\n",
      "Train Loss: 0.30470799406369525\n",
      "Test Loss: 0.2632542848587036\n",
      "\n",
      "Epoch: 490\n",
      "Train Loss: 0.30038785686095554\n",
      "Test Loss: 0.27703651785850525\n",
      "\n",
      "Epoch: 491\n",
      "Train Loss: 0.30098962287108105\n",
      "Test Loss: 0.260626882314682\n",
      "\n",
      "Epoch: 492\n",
      "Train Loss: 0.3084508726994197\n",
      "Test Loss: 0.26374566555023193\n",
      "\n",
      "Epoch: 493\n",
      "Train Loss: 0.3019874518116315\n",
      "Test Loss: 0.262503057718277\n",
      "\n",
      "Epoch: 494\n",
      "Train Loss: 0.3017287030816078\n",
      "Test Loss: 0.2624858319759369\n",
      "\n",
      "Epoch: 495\n",
      "Train Loss: 0.3028521512945493\n",
      "Test Loss: 0.26096901297569275\n",
      "\n",
      "Epoch: 496\n",
      "Train Loss: 0.30100904653469723\n",
      "Test Loss: 0.2623978853225708\n",
      "\n",
      "Epoch: 497\n",
      "Train Loss: 0.2989121600985527\n",
      "Test Loss: 0.2607046961784363\n",
      "\n",
      "Epoch: 498\n",
      "Train Loss: 0.2984210476279259\n",
      "Test Loss: 0.25642651319503784\n",
      "\n",
      "Epoch: 499\n",
      "Train Loss: 0.2991580019394557\n",
      "Test Loss: 0.2595553398132324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    print(\"Epoch:\",epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_index = np.array([x for x in range(train_tiles.shape[0])])\n",
    "    np.random.shuffle(train_index)\n",
    "\n",
    "    train_index = train_index.T[:140].reshape(-1,BATCH_SIZE)\n",
    "    i=-1\n",
    "    \n",
    "    o_model.train()\n",
    "    for tr_ind in train_index:\n",
    "        i+=1\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = train_tiles[tr_ind]\n",
    "        \n",
    "        x = x.reshape(-1,3,224,224)\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        y = cluster_dists[tr_ind]\n",
    "        y = y.to(DEVICE)\n",
    "    \n",
    "        x2, y2 = model(x)\n",
    "        \n",
    "        _, _, pred, ls = o_model(x2, x)\n",
    "        del x, _\n",
    "        pl = 2*loss(pred,y)\n",
    "        pl += loss(pred,y2)\n",
    "        \n",
    "        for x,y in zip(ls, x2):\n",
    "            pl += 0.15 * loss(x, y)\n",
    "        del y, y2, pred, x2, ls\n",
    "        \n",
    "        o_optimizer.zero_grad()\n",
    "        pl.backward()\n",
    "        o_optimizer.step()\n",
    "        \n",
    "        running_loss += pl.item()\n",
    "    \n",
    "    \n",
    "    o_model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        x = test_tiles\n",
    "        y = te_cluster_dists\n",
    "        y=y.to(DEVICE)\n",
    "        \n",
    "        x = x.reshape(-1,3,224,224).to(DEVICE)\n",
    "        x2, y2 = model(x)\n",
    "        _, _, pred, ls = o_model(x2, x)\n",
    "        del x\n",
    "        tl = 2*loss(pred,y)\n",
    "        tl += loss(pred,y2)\n",
    "        \n",
    "        for x,y in zip(ls, x2):\n",
    "            tl += 0.15 * loss(x, y)\n",
    "            \n",
    "        del y, pred\n",
    "        \n",
    "            \n",
    "        del x2, ls\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(\"Train Loss:\", running_loss/i)\n",
    "    print(\"Test Loss:\", tl.item())\n",
    "    print()\n",
    "    del pl, tl\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b8120cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e7b1752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "tensor([[0.3373, 0.5332, 0.6235, 0.3188, 0.4363, 0.5275, 0.6456, 0.0000, 0.5819,\n",
      "         0.4891, 0.4102, 0.6521, 0.0904, 0.4071, 0.3883, 0.0078, 0.4868, 0.5799,\n",
      "         0.5108, 0.1243, 0.5061, 0.4710, 0.5229, 1.0000, 0.3739, 0.1781, 0.2041,\n",
      "         0.6952, 0.5949, 0.5235, 0.2035, 0.3568, 0.2636, 0.4233, 0.1525, 0.7835,\n",
      "         0.3229, 0.4309, 0.3362, 0.0597]], device='cuda:6')\n",
      "tensor(5, dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005240112077444792\n",
      "0.0010337764397263527\n",
      "tensor([0.5116, 0.3660, 0.3326, 0.2874, 0.3739, 0.2697], device='cuda:6')\n",
      "tensor([0.5237, 0.4310, 0.3490, 0.2951, 0.3807, 0.2314])\n",
      "tensor([0.5167, 0.3837, 0.3555, 0.2680, 0.3818, 0.2267], device='cuda:6')\n"
     ]
    }
   ],
   "source": [
    "n+=5\n",
    "model.eval()\n",
    "o_model.eval()\n",
    "print(n)\n",
    "with torch.no_grad():\n",
    "    in_img = train_tiles[n].reshape(3,224,224).unsqueeze(0).to(DEVICE)\n",
    "    x, y = model(in_img)\n",
    "    box, base, pred, l = o_model(x, in_img)\n",
    "\n",
    "    print(box)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "    axes[0].imshow(in_img.reshape(224,224,3).detach().cpu().numpy())\n",
    "    #axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(base.reshape(224,224,3).detach().cpu().numpy())\n",
    "    #axes[1].axis('off')\n",
    "    print(train_cluster[n])\n",
    "    plt.show()\n",
    "    print(loss(pred,y).item())\n",
    "    print(loss(pred,cluster_dists[n].to(DEVICE)).item())\n",
    "    print(pred[0])\n",
    "    print(cluster_dists[n])\n",
    "    print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "82d51ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_d_model1.pickle\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_o_model1.pickle\"\n",
    "torch.save(o_model.state_dict(), SAVE_PATH)\n",
    "\n",
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_d_model1_backup.pickle\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "SAVE_PATH = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/saved_models/whatmatters_init_o_model1_backup.pickle\"\n",
    "torch.save(o_model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "d345c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4961, 0.5171, 0.4878, 0.5181, 0.4893, 0.4994], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "o_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = o_model(conv[169].unsqueeze(0))[0]\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "2f8fd7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_tiles[169].reshape(56,56,3).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "dbacdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = torch.ones(56,56,3)\n",
    "for o in out.reshape(3,2):\n",
    "    x_min = int(o[0] * 56)\n",
    "    y_min = int(o[1] * 56)\n",
    "    x_max = x_min+14\n",
    "    y_max = y_min+14\n",
    "    base[x_min:x_max, y_min:y_max] = test_tiles[169][x_min:x_max, y_min:y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "eaa50c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.patches as patches\n",
    "plt.imshow(base.reshape(56,56,3).detach().cpu().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6185691356658936\n",
      "Test Loss: 0.013077893294394016\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 4.466363310813904\n",
      "Test Loss: 0.08383168280124664\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.9857504367828369\n",
      "Test Loss: 0.1306329220533371\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.4335505776107311\n",
      "Test Loss: 0.11838601529598236\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.1894276663661003\n",
      "Test Loss: 0.17145520448684692\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.11076735612004995\n",
      "Test Loss: 0.20163533091545105\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.06546270055696368\n",
      "Test Loss: 0.23045168817043304\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.03978912718594074\n",
      "Test Loss: 0.25447165966033936\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.030275192810222507\n",
      "Test Loss: 0.25443536043167114\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.024784686043858528\n",
      "Test Loss: 0.2580438256263733\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.02040827041491866\n",
      "Test Loss: 0.25746673345565796\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.023005058988928795\n",
      "Test Loss: 0.26734060049057007\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.017401195713318884\n",
      "Test Loss: 0.2632371783256531\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.016578912502154708\n",
      "Test Loss: 0.2604631185531616\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.015443754266016185\n",
      "Test Loss: 0.25787028670310974\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.013517873478122056\n",
      "Test Loss: 0.2509487569332123\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.012707430869340897\n",
      "Test Loss: 0.2664503753185272\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.010244129807688296\n",
      "Test Loss: 0.264952152967453\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.01376846560742706\n",
      "Test Loss: 0.2591142952442169\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.014018056215718389\n",
      "Test Loss: 0.2530756890773773\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.014790704473853111\n",
      "Test Loss: 0.24640822410583496\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.015644261497072875\n",
      "Test Loss: 0.2564164400100708\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.011659952229820192\n",
      "Test Loss: 0.2638177275657654\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.013651449466124177\n",
      "Test Loss: 0.25329136848449707\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.011883809580467641\n",
      "Test Loss: 0.2565382421016693\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.009422098228242248\n",
      "Test Loss: 0.2624457776546478\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.009408520651049912\n",
      "Test Loss: 0.2642855942249298\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.009222712134942412\n",
      "Test Loss: 0.25244584679603577\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.008430076180957258\n",
      "Test Loss: 0.2540249228477478\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.010139122372493148\n",
      "Test Loss: 0.26229336857795715\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.009419935755431652\n",
      "Test Loss: 0.2636673152446747\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.00809960108017549\n",
      "Test Loss: 0.25353023409843445\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.009064150508493185\n",
      "Test Loss: 0.25543496012687683\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.010349620482884347\n",
      "Test Loss: 0.2495541274547577\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.008806869736872613\n",
      "Test Loss: 0.2494097203016281\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.011779112741351128\n",
      "Test Loss: 0.26040464639663696\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.012417705729603767\n",
      "Test Loss: 0.2539421021938324\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.011891240486875176\n",
      "Test Loss: 0.256619930267334\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.010040451656095684\n",
      "Test Loss: 0.25648000836372375\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.010442016995511949\n",
      "Test Loss: 0.242721825838089\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.009263826708775014\n",
      "Test Loss: 0.2560822069644928\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.010113262571394444\n",
      "Test Loss: 0.25213539600372314\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.00838752428535372\n",
      "Test Loss: 0.25727030634880066\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.008899270032998174\n",
      "Test Loss: 0.25322720408439636\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.009923417121171951\n",
      "Test Loss: 0.25688278675079346\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.010956532671116292\n",
      "Test Loss: 0.25237274169921875\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.01004152896348387\n",
      "Test Loss: 0.26082274317741394\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.008749631466343999\n",
      "Test Loss: 0.26311612129211426\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.008452643523924053\n",
      "Test Loss: 0.2584606111049652\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.009560647769831121\n",
      "Test Loss: 0.2674320936203003\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.008458346943370998\n",
      "Test Loss: 0.25485238432884216\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.007539874583017081\n",
      "Test Loss: 0.25388818979263306\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.009844215237535536\n",
      "Test Loss: 0.2627265155315399\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.00863578071584925\n",
      "Test Loss: 0.2589925229549408\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.011617343639954925\n",
      "Test Loss: 0.2546611428260803\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.008776089351158589\n",
      "Test Loss: 0.2640848457813263\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.007850251975469291\n",
      "Test Loss: 0.25793296098709106\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.007010665431153029\n",
      "Test Loss: 0.25082191824913025\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.0071445347857661545\n",
      "Test Loss: 0.25478968024253845\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.009321293968241662\n",
      "Test Loss: 0.25722572207450867\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.009068682440556586\n",
      "Test Loss: 0.24556753039360046\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.007098610629327595\n",
      "Test Loss: 0.2569270133972168\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.009093654400203377\n",
      "Test Loss: 0.2521864175796509\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.009048448700923473\n",
      "Test Loss: 0.24977341294288635\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.007840666745323688\n",
      "Test Loss: 0.2611459195613861\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.0069102622219361365\n",
      "Test Loss: 0.2606334388256073\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.007082512776833028\n",
      "Test Loss: 0.2579692304134369\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.006933493714313954\n",
      "Test Loss: 0.26131173968315125\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.006016913859639317\n",
      "Test Loss: 0.2547260820865631\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.00834111764561385\n",
      "Test Loss: 0.26214486360549927\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.0080983770894818\n",
      "Test Loss: 0.2482110857963562\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.009450481971725821\n",
      "Test Loss: 0.2554300129413605\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.008239179383963346\n",
      "Test Loss: 0.2593255341053009\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.006627022929023951\n",
      "Test Loss: 0.24651959538459778\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.006590350763872266\n",
      "Test Loss: 0.25535449385643005\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.0059867805684916675\n",
      "Test Loss: 0.25477898120880127\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.006238856236450374\n",
      "Test Loss: 0.25087282061576843\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.006214148772414774\n",
      "Test Loss: 0.2536798417568207\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.006272053869906813\n",
      "Test Loss: 0.25763827562332153\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.006521261704619974\n",
      "Test Loss: 0.25514259934425354\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.007310443790629506\n",
      "Test Loss: 0.25005465745925903\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.007280357473064214\n",
      "Test Loss: 0.25859764218330383\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.006618111918214709\n",
      "Test Loss: 0.25533491373062134\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.009322126628831029\n",
      "Test Loss: 0.25129351019859314\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.006789031031075865\n",
      "Test Loss: 0.252718061208725\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.007515294419135898\n",
      "Test Loss: 0.25315287709236145\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.005690881342161447\n",
      "Test Loss: 0.25622111558914185\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.0057818416389636695\n",
      "Test Loss: 0.2512740194797516\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.005785306333564222\n",
      "Test Loss: 0.2692524492740631\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.0056966987904161215\n",
      "Test Loss: 0.2539668679237366\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.00559129478642717\n",
      "Test Loss: 0.2577890455722809\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.005265555460937321\n",
      "Test Loss: 0.24837446212768555\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.005421906360425055\n",
      "Test Loss: 0.24961645901203156\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.004974423558451235\n",
      "Test Loss: 0.25063982605934143\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.005654076172504574\n",
      "Test Loss: 0.2524847090244293\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.004472580447327346\n",
      "Test Loss: 0.25676077604293823\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.004803334129974246\n",
      "Test Loss: 0.25619637966156006\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.0047206374001689255\n",
      "Test Loss: 0.25497129559516907\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.007082578958943486\n",
      "Test Loss: 0.2501315474510193\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.006156212359201163\n",
      "Test Loss: 0.2596830725669861\n",
      "\n",
      "Epoch: 100\n",
      "Train Loss: 0.006449726119171828\n",
      "Test Loss: 0.261655330657959\n",
      "\n",
      "Epoch: 101\n",
      "Train Loss: 0.005414095532614738\n",
      "Test Loss: 0.2543416917324066\n",
      "\n",
      "Epoch: 102\n",
      "Train Loss: 0.006668562185950577\n",
      "Test Loss: 0.2578762173652649\n",
      "\n",
      "Epoch: 103\n",
      "Train Loss: 0.007771709410008043\n",
      "Test Loss: 0.25327757000923157\n",
      "\n",
      "Epoch: 104\n",
      "Train Loss: 0.006176411698106676\n",
      "Test Loss: 0.24498350918293\n",
      "\n",
      "Epoch: 105\n",
      "Train Loss: 0.006727155472617596\n",
      "Test Loss: 0.25503045320510864\n",
      "\n",
      "Epoch: 106\n",
      "Train Loss: 0.007043016550596803\n",
      "Test Loss: 0.25523459911346436\n",
      "\n",
      "Epoch: 107\n",
      "Train Loss: 0.007384247204754502\n",
      "Test Loss: 0.24202056229114532\n",
      "\n",
      "Epoch: 108\n",
      "Train Loss: 0.006946515990421176\n",
      "Test Loss: 0.2572095990180969\n",
      "\n",
      "Epoch: 109\n",
      "Train Loss: 0.006510727573186159\n",
      "Test Loss: 0.2630656659603119\n",
      "\n",
      "Epoch: 110\n",
      "Train Loss: 0.006130154535640031\n",
      "Test Loss: 0.2558428645133972\n",
      "\n",
      "Epoch: 111\n",
      "Train Loss: 0.007991276565007865\n",
      "Test Loss: 0.25433892011642456\n",
      "\n",
      "Epoch: 112\n",
      "Train Loss: 0.006690210197120905\n",
      "Test Loss: 0.2616724669933319\n",
      "\n",
      "Epoch: 113\n",
      "Train Loss: 0.0068044509389437735\n",
      "Test Loss: 0.25195401906967163\n",
      "\n",
      "Epoch: 114\n",
      "Train Loss: 0.006608703930396587\n",
      "Test Loss: 0.25273433327674866\n",
      "\n",
      "Epoch: 115\n",
      "Train Loss: 0.005942277843132615\n",
      "Test Loss: 0.25563210248947144\n",
      "\n",
      "Epoch: 116\n",
      "Train Loss: 0.006976240081712604\n",
      "Test Loss: 0.2728048264980316\n",
      "\n",
      "Epoch: 117\n",
      "Train Loss: 0.006239530455786735\n",
      "Test Loss: 0.25984886288642883\n",
      "\n",
      "Epoch: 118\n",
      "Train Loss: 0.00858033902477473\n",
      "Test Loss: 0.258298397064209\n",
      "\n",
      "Epoch: 119\n",
      "Train Loss: 0.007092522049788386\n",
      "Test Loss: 0.2526280879974365\n",
      "\n",
      "Epoch: 120\n",
      "Train Loss: 0.006342911859974265\n",
      "Test Loss: 0.2507616877555847\n",
      "\n",
      "Epoch: 121\n",
      "Train Loss: 0.006339833373203874\n",
      "Test Loss: 0.25466012954711914\n",
      "\n",
      "Epoch: 122\n",
      "Train Loss: 0.00579629885032773\n",
      "Test Loss: 0.26500287652015686\n",
      "\n",
      "Epoch: 123\n",
      "Train Loss: 0.008132131886668503\n",
      "Test Loss: 0.2539193034172058\n",
      "\n",
      "Epoch: 124\n",
      "Train Loss: 0.006211173604242504\n",
      "Test Loss: 0.24840612709522247\n",
      "\n",
      "Epoch: 125\n",
      "Train Loss: 0.007028693973552436\n",
      "Test Loss: 0.26567938923835754\n",
      "\n",
      "Epoch: 126\n",
      "Train Loss: 0.005675203807186335\n",
      "Test Loss: 0.2597346603870392\n",
      "\n",
      "Epoch: 127\n",
      "Train Loss: 0.005573511065449566\n",
      "Test Loss: 0.26221171021461487\n",
      "\n",
      "Epoch: 128\n",
      "Train Loss: 0.005138027248904109\n",
      "Test Loss: 0.25513002276420593\n",
      "\n",
      "Epoch: 129\n",
      "Train Loss: 0.005691752827260643\n",
      "Test Loss: 0.2569164037704468\n",
      "\n",
      "Epoch: 130\n",
      "Train Loss: 0.004847769334446639\n",
      "Test Loss: 0.2697068154811859\n",
      "\n",
      "Epoch: 131\n",
      "Train Loss: 0.00557390577159822\n",
      "Test Loss: 0.262681245803833\n",
      "\n",
      "Epoch: 132\n",
      "Train Loss: 0.006338816892821342\n",
      "Test Loss: 0.25787830352783203\n",
      "\n",
      "Epoch: 133\n",
      "Train Loss: 0.0059910526615567505\n",
      "Test Loss: 0.25849589705467224\n",
      "\n",
      "Epoch: 134\n",
      "Train Loss: 0.004959711688570678\n",
      "Test Loss: 0.2568293511867523\n",
      "\n",
      "Epoch: 135\n",
      "Train Loss: 0.004637342761270702\n",
      "Test Loss: 0.2584306001663208\n",
      "\n",
      "Epoch: 136\n",
      "Train Loss: 0.006339925923384726\n",
      "Test Loss: 0.2631992995738983\n",
      "\n",
      "Epoch: 137\n",
      "Train Loss: 0.0057650088565424085\n",
      "Test Loss: 0.26467105746269226\n",
      "\n",
      "Epoch: 138\n",
      "Train Loss: 0.005317436822224408\n",
      "Test Loss: 0.26255273818969727\n",
      "\n",
      "Epoch: 139\n",
      "Train Loss: 0.0050221397541463375\n",
      "Test Loss: 0.25736936926841736\n",
      "\n",
      "Epoch: 140\n",
      "Train Loss: 0.005274777067825198\n",
      "Test Loss: 0.2585274279117584\n",
      "\n",
      "Epoch: 141\n",
      "Train Loss: 0.005838293000124395\n",
      "Test Loss: 0.24193960428237915\n",
      "\n",
      "Epoch: 142\n",
      "Train Loss: 0.005373886553570628\n",
      "Test Loss: 0.2590087950229645\n",
      "\n",
      "Epoch: 143\n",
      "Train Loss: 0.004860788700170815\n",
      "Test Loss: 0.26747769117355347\n",
      "\n",
      "Epoch: 144\n",
      "Train Loss: 0.004916995181702077\n",
      "Test Loss: 0.2617873549461365\n",
      "\n",
      "Epoch: 145\n",
      "Train Loss: 0.0053630112670362\n",
      "Test Loss: 0.26419079303741455\n",
      "\n",
      "Epoch: 146\n",
      "Train Loss: 0.005474035628139973\n",
      "Test Loss: 0.2553528845310211\n",
      "\n",
      "Epoch: 147\n",
      "Train Loss: 0.0049192041042260826\n",
      "Test Loss: 0.2548011541366577\n",
      "\n",
      "Epoch: 148\n",
      "Train Loss: 0.006069914379622787\n",
      "Test Loss: 0.26069456338882446\n",
      "\n",
      "Epoch: 149\n",
      "Train Loss: 0.005423541238997132\n",
      "Test Loss: 0.2589174807071686\n",
      "\n",
      "Epoch: 150\n",
      "Train Loss: 0.006730892113409936\n",
      "Test Loss: 0.2545209527015686\n",
      "\n",
      "Epoch: 151\n",
      "Train Loss: 0.005594007729087025\n",
      "Test Loss: 0.25450605154037476\n",
      "\n",
      "Epoch: 152\n",
      "Train Loss: 0.007358807779382914\n",
      "Test Loss: 0.2617764174938202\n",
      "\n",
      "Epoch: 153\n",
      "Train Loss: 0.00628897687420249\n",
      "Test Loss: 0.25926849246025085\n",
      "\n",
      "Epoch: 154\n",
      "Train Loss: 0.005601209180895239\n",
      "Test Loss: 0.2629666030406952\n",
      "\n",
      "Epoch: 155\n",
      "Train Loss: 0.005449948366731405\n",
      "Test Loss: 0.26525041460990906\n",
      "\n",
      "Epoch: 156\n",
      "Train Loss: 0.004915219178656116\n",
      "Test Loss: 0.2587552070617676\n",
      "\n",
      "Epoch: 157\n",
      "Train Loss: 0.005838905286509544\n",
      "Test Loss: 0.2556140720844269\n",
      "\n",
      "Epoch: 158\n",
      "Train Loss: 0.005855640978552401\n",
      "Test Loss: 0.2610490024089813\n",
      "\n",
      "Epoch: 159\n",
      "Train Loss: 0.005665889009833336\n",
      "Test Loss: 0.25063517689704895\n",
      "\n",
      "Epoch: 160\n",
      "Train Loss: 0.005601711454801261\n",
      "Test Loss: 0.2528243362903595\n",
      "\n",
      "Epoch: 161\n",
      "Train Loss: 0.004006581933936104\n",
      "Test Loss: 0.2608585059642792\n",
      "\n",
      "Epoch: 162\n",
      "Train Loss: 0.004218555433908477\n",
      "Test Loss: 0.2657274007797241\n",
      "\n",
      "Epoch: 163\n",
      "Train Loss: 0.005135759420227259\n",
      "Test Loss: 0.25197747349739075\n",
      "\n",
      "Epoch: 164\n",
      "Train Loss: 0.005156963248737156\n",
      "Test Loss: 0.260580450296402\n",
      "\n",
      "Epoch: 165\n",
      "Train Loss: 0.004511915787588805\n",
      "Test Loss: 0.2565947473049164\n",
      "\n",
      "Epoch: 166\n",
      "Train Loss: 0.005031250417232513\n",
      "Test Loss: 0.2580367922782898\n",
      "\n",
      "Epoch: 167\n",
      "Train Loss: 0.005257779615931213\n",
      "Test Loss: 0.25802382826805115\n",
      "\n",
      "Epoch: 168\n",
      "Train Loss: 0.005357011745218188\n",
      "Test Loss: 0.2535325884819031\n",
      "\n",
      "Epoch: 169\n",
      "Train Loss: 0.005071908759418875\n",
      "Test Loss: 0.25502264499664307\n",
      "\n",
      "Epoch: 170\n",
      "Train Loss: 0.005328405706677586\n",
      "Test Loss: 0.2593061923980713\n",
      "\n",
      "Epoch: 171\n",
      "Train Loss: 0.004607561393640935\n",
      "Test Loss: 0.25138410925865173\n",
      "\n",
      "Epoch: 172\n",
      "Train Loss: 0.004601381195243448\n",
      "Test Loss: 0.25879234075546265\n",
      "\n",
      "Epoch: 173\n",
      "Train Loss: 0.005599008174613118\n",
      "Test Loss: 0.2644544839859009\n",
      "\n",
      "Epoch: 174\n",
      "Train Loss: 0.006064088374841958\n",
      "Test Loss: 0.26081737875938416\n",
      "\n",
      "Epoch: 175\n",
      "Train Loss: 0.004943204519804567\n",
      "Test Loss: 0.25120237469673157\n",
      "\n",
      "Epoch: 176\n",
      "Train Loss: 0.0040440550073981285\n",
      "Test Loss: 0.26441243290901184\n",
      "\n",
      "Epoch: 177\n",
      "Train Loss: 0.0047338439035229385\n",
      "Test Loss: 0.2536883056163788\n",
      "\n",
      "Epoch: 178\n",
      "Train Loss: 0.004622163658495992\n",
      "Test Loss: 0.2533856928348541\n",
      "\n",
      "Epoch: 179\n",
      "Train Loss: 0.004560190427582711\n",
      "Test Loss: 0.2546738386154175\n",
      "\n",
      "Epoch: 180\n",
      "Train Loss: 0.004768532118760049\n",
      "Test Loss: 0.25981760025024414\n",
      "\n",
      "Epoch: 181\n",
      "Train Loss: 0.005312818335369229\n",
      "Test Loss: 0.24866220355033875\n",
      "\n",
      "Epoch: 182\n",
      "Train Loss: 0.006376754376105964\n",
      "Test Loss: 0.25958383083343506\n",
      "\n",
      "Epoch: 183\n",
      "Train Loss: 0.004976904776412994\n",
      "Test Loss: 0.2647448480129242\n",
      "\n",
      "Epoch: 184\n",
      "Train Loss: 0.005627244419883937\n",
      "Test Loss: 0.2584283649921417\n",
      "\n",
      "Epoch: 185\n",
      "Train Loss: 0.0050953071331605315\n",
      "Test Loss: 0.2613508105278015\n",
      "\n",
      "Epoch: 186\n",
      "Train Loss: 0.004894105135463178\n",
      "Test Loss: 0.26345115900039673\n",
      "\n",
      "Epoch: 187\n",
      "Train Loss: 0.004794301959918812\n",
      "Test Loss: 0.2559313178062439\n",
      "\n",
      "Epoch: 188\n",
      "Train Loss: 0.006043007946573198\n",
      "Test Loss: 0.25167742371559143\n",
      "\n",
      "Epoch: 189\n",
      "Train Loss: 0.00704470940399915\n",
      "Test Loss: 0.2612594962120056\n",
      "\n",
      "Epoch: 190\n",
      "Train Loss: 0.004890247364528477\n",
      "Test Loss: 0.2725663185119629\n",
      "\n",
      "Epoch: 191\n",
      "Train Loss: 0.0052184221567586064\n",
      "Test Loss: 0.2609841227531433\n",
      "\n",
      "Epoch: 192\n",
      "Train Loss: 0.004346083733253181\n",
      "Test Loss: 0.2571460008621216\n",
      "\n",
      "Epoch: 193\n",
      "Train Loss: 0.004835619067307562\n",
      "Test Loss: 0.24530740082263947\n",
      "\n",
      "Epoch: 194\n",
      "Train Loss: 0.005971215839963406\n",
      "Test Loss: 0.2623685598373413\n",
      "\n",
      "Epoch: 195\n",
      "Train Loss: 0.005628231388982385\n",
      "Test Loss: 0.2604106664657593\n",
      "\n",
      "Epoch: 196\n",
      "Train Loss: 0.00606492831138894\n",
      "Test Loss: 0.2585051357746124\n",
      "\n",
      "Epoch: 197\n",
      "Train Loss: 0.004845447023399174\n",
      "Test Loss: 0.2662985920906067\n",
      "\n",
      "Epoch: 198\n",
      "Train Loss: 0.0050253423396497965\n",
      "Test Loss: 0.2532404959201813\n",
      "\n",
      "Epoch: 199\n",
      "Train Loss: 0.0064208898693323135\n",
      "Test Loss: 0.2560775876045227\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd752207",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = Discriminator(3, 128, 12)\n",
    "s_model = s_model.to(DEVICE)\n",
    "s_optimizer = torch.optim.Adam(s_model.parameters(), lr = LR)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "dfb925a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "50947497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1761, 0.3635, 0.7116, 0.4861, 0.2940, 0.4402, 0.5499, 0.1935, 0.6286,\n",
      "         0.5097, 0.4684, 0.3761]], device='cuda:7')\n",
      "tensor(2, device='cuda:7')\n",
      "tensor([0.1921, 0.3630, 0.7462, 0.5108, 0.3015, 0.4393, 0.5710, 0.2034, 0.6278,\n",
      "        0.5121, 0.5224, 0.3982])\n",
      "tensor(2)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "n+=1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(train_tiles[n].reshape(3,56,56).to(DEVICE).unsqueeze(0))\n",
    "    print(out)\n",
    "    print(out.argmax())\n",
    "    print(cluster_dists[n])\n",
    "    print(cluster_dists[n].argmax())\n",
    "    print(train_cluster[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "06ab80ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2593, 0.4116, 0.7252, 0.5696, 0.4040, 0.4303, 0.6239, 0.2999, 0.5534,\n",
      "         0.5677, 0.5029, 0.3411]], device='cuda:7')\n",
      "tensor(2, device='cuda:7')\n",
      "tensor([0.2681, 0.4193, 0.7577, 0.5917, 0.4077, 0.4306, 0.6630, 0.3285, 0.6058,\n",
      "        0.5836, 0.5645, 0.3457])\n",
      "tensor(2)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "n+=1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(test_tiles[n].reshape(3,56,56).to(DEVICE).unsqueeze(0))\n",
    "    print(out)\n",
    "    print(out.argmax())\n",
    "    print(te_cluster_dists[n])\n",
    "    print(te_cluster_dists[n].argmax())\n",
    "    print(test_cluster[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1068f829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 56, 56, 3])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(train_tiles[tr_ind].shape)\n",
    "print(train_cluster[tr_ind].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b09cfad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  8,  4,  3,  1,  0,  2, 11,  2,  7,  1, 11,  5,  5, 11,  7, 10,  1,\n",
       "         0,  5,  9, 10,  2,  6,  2,  2,  3,  8, 11, 10,  1,  2,  3,  0,  2,  0,\n",
       "         2,  9,  2, 11, 10,  0,  1,  1,  2,  0,  2, 10,  0, 11,  2,  2, 10, 11,\n",
       "        10,  1,  0,  9,  9,  7,  8,  3,  9,  8,  9,  6,  3,  1,  4,  5, 11, 11,\n",
       "        11,  4,  2,  1,  1, 11, 10, 11,  0,  6,  1,  5,  4, 11,  5,  6,  2,  2,\n",
       "         0,  0,  1,  7,  1, 11,  2,  1, 11,  1], dtype=torch.int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cluster[tr_ind]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_thesis_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
