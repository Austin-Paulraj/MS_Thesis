{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e51b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulraae/.conda/envs/ms_thesis_Env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "from torch_geometric.data import Dataset, download_url, Data\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from torch_geometric import nn as gnn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import crop\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a74335bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/paulraae/MS_Thesis_Data/tester_eye_data/test_202_with_tiles_processed.pk1', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "324c41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_graph_rest_free(Dataset): #For this ds I have ald done all the required pre prcessing \n",
    "    def __init__(self, root, data_dict = None):\n",
    "        self.data_dict = data_dict\n",
    "        super().__init__(root, None, None, None)\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return \"not_implementated.pt\"\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return \"not_implemented.pt\"\n",
    "    \n",
    "    def process(self):\n",
    "            # Process pre made data dictionary\n",
    "\n",
    "            centroids = torch.Tensor(self.data_dict[\"tile_centroids\"].reshape((12,56,56,3)))\n",
    "            tiles = torch.Tensor(self.data_dict[\"tiles\"].reshape((3232,56,56,3)))\n",
    "            tile_clusters = torch.Tensor(self.data_dict[\"tile_cluster\"])\n",
    "            centroid_y = torch.Tensor(torch.Tensor([i for i in range(self.data_dict[\"tile_centroids\"].shape[0])]))\n",
    "            tile_knn_edges = torch.Tensor(self.data_dict[\"tile_knn_edge_index\"])\n",
    "            \n",
    "            x_features = torch.cat((tiles, centroids))\n",
    "            y = torch.cat((tile_clusters, centroid_y)).to(torch.int)\n",
    "            \n",
    "            edge_map = []\n",
    "\n",
    "\n",
    "            i=-1\n",
    "            for cluster in y[:tiles.shape[0]]:\n",
    "                i+=1\n",
    "                edge_map.append([i, cluster+tiles.shape[0]])\n",
    "            \n",
    "\n",
    "            i+=1\n",
    "            for centroid1 in y[i:]:\n",
    "                for centroid2 in y[i+1:]:\n",
    "                    edge_map.append([centroid1+tiles.shape[0], centroid2+tiles.shape[0]])\n",
    "                i+=1\n",
    "            \n",
    "            edge_map = torch.cat((torch.Tensor(edge_map), tile_knn_edges), axis=0)\n",
    "            \n",
    "            edge_map = edge_map.to(int)\n",
    "            \n",
    "            edge_map_aux = [[],[]]\n",
    "            for x in edge_map:\n",
    "                edge_map_aux[0].append(x[0])\n",
    "                edge_map_aux[1].append(x[1]) \n",
    "                    \n",
    "            edge_map_aux = np.array(edge_map_aux)\n",
    "            \n",
    "            edge_attrs = []\n",
    "            for _ in edge_map_aux[0]:\n",
    "                edge_attrs.append(1)\n",
    "                \n",
    "            edge_attrs = torch.Tensor(edge_attrs)\n",
    "            edge_map_aux = torch.Tensor(edge_map_aux).to(int)\n",
    "            \n",
    "            data = Data(x_features, edge_map_aux, edge_attrs, y)\n",
    "\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_0.pt'))\n",
    "\n",
    "    def len(self):\n",
    "        return self.data_dict[\"features\"].shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f3b2d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "root_path = \"/home/paulraae/MS_Thesis/ViG_based_link_pred_implementation/test_graph_ds\"\n",
    "dataset = One_graph_rest_free(root_path, data_dict=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07ad3418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3244, 56, 56, 3], edge_index=[2, 16541], edge_attr=[16541], y=[3244])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5193ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/home/paulraae/')\n",
    "from Vision_GNN.vig import *\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 56\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    'gnn_patch16_224': _cfg(\n",
    "        crop_pct=0.9, input_size=(3, 224, 224),\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "}\n",
    "\n",
    "@register_model\n",
    "def vig_ti_224_gelu(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, drop_rate=0.0, num_knn=9, **kwargs):\n",
    "            self.k = num_knn # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.n_blocks = 12 # number of basic blocks in the backbone\n",
    "            self.n_filters = 192 # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.dropout = drop_rate # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['gnn_patch16_224']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14757f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vig_block(4096,4096,4096,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35dfe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = model.forward(dataset[0].x[\"graph\"], dataset[0].edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f9be7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([132, 4096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_thesis_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
